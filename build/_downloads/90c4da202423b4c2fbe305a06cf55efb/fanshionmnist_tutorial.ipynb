{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5728FASHION-MNIST\u4e0a\u8bad\u7ec3CNN\n==============================\n\n**\u4f5c\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`__\n\n\u6211\u4eec\u5c06\u91c7\u53d6\u4ee5\u4e0b\u6b65\u9aa4:\n\n1. \u4f7f\u7528 ``torchvision`` \u52a0\u8f7d\u548c\u89c4\u8303\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\n2. \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\n3. \u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoardX\u67e5\u770b\n4. \u5b9a\u4e49\u635f\u5931\u51fd\u6570\n5. \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8bad\u7ec3\u7f51\u7edc\n6. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\n\nplt.ion()\n# \u5ffd\u7565 warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FASHION-MNIST\u6570\u636e\u96c6\u7684\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\n-------------------------------------------\n\nFashion-MNIST\u662f\u4e00\u4e2a\u66ff\u4ee3MNIST\u624b\u5199\u6570\u5b57\u96c6\u7684\u56fe\u50cf\u6570\u636e\u96c6\u3002 \u5b83\u662f\u7531Zalando\uff08\u4e00\u5bb6\u5fb7\u56fd\u7684\u65f6\u5c1a\u79d1\u6280\u516c\u53f8\uff09\n\u65d7\u4e0b\u7684\u7814\u7a76\u90e8\u95e8\u63d0\u4f9b\u3002\u5176\u6db5\u76d6\u4e86\u6765\u81ea10\u79cd\u7c7b\u522b\u7684\u51717\u4e07\u4e2a\u4e0d\u540c\u5546\u54c1\u7684\u6b63\u9762\u56fe\u7247\u3002\nFashion-MNIST\u7684\u5927\u5c0f\u3001\u683c\u5f0f\u548c\u8bad\u7ec3\u96c6/\u6d4b\u8bd5\u96c6\u5212\u5206\u4e0e\u539f\u59cb\u7684MNIST\u5b8c\u5168\u4e00\u81f4\u3002\n60000/10000\u7684\u8bad\u7ec3\u6d4b\u8bd5\u6570\u636e\u5212\u5206\uff0c28x28\u7684\u7070\u5ea6\u56fe\u7247\u3002\u4f60\u53ef\u4ee5\u76f4\u63a5\u7528\u5b83\u6765\u6d4b\u8bd5\u4f60\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u6027\u80fd\uff0c\n\u4e14\u4e0d\u9700\u8981\u6539\u52a8\u4efb\u4f55\u7684\u4ee3\u7801\u3002\nhttps://github.com/zalandoresearch/fashion-mnist/blob/master/README.zh-CN.md\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5), (0.5, 0.5))])\n\ntrainset = torchvision.datasets.FashionMNIST(root='./data/fashiomnist', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.FashionMNIST(root='./data/fashiomnist', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n\n\nprint(\"\u8bad\u7ec3\u96c6\u5927\u5c0f\uff1a\",len(trainloader)*batch_size)\nprint(\"\u6d4b\u8bd5\u96c6\u5927\u5c0f\uff1a\",len(testloader)*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u8bad\u7ec3\u96c6\u4e2d\u7684\u56fe\u7247\n-----------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u7528\u4e8e\u663e\u793a\u4e00\u5f20\u56fe\u50cf\u7684\u51fd\u6570\ndef imshow(img, title=None):\n    img = img / 2 + 0.5     # \u53bb\u5f52\u4e00\u5316\n    npimg = img.numpy()\n    plt.figure(figsize=[6.5, 2.5])\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n\n\n# \u83b7\u53d6\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\uff0c\u4e00\u6b21\u8fed\u4ee3\u53d6\u51fabatch_size\u5f20\u56fe\u7247\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# \u663e\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\nimshow(torchvision.utils.make_grid(images),\"Tne original image batches\")\n# \u8f93\u51fa \u5bf9\u5e94\u6279\u6b21\u56fe\u50cf\u7684\u6807\u7b7e\nprint(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u7f51\u7edc\u6a21\u578b\n---------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net1(nn.Module):\n    def __init__(self):\n        super(Net1, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        self.bn = nn.BatchNorm2d(20)\n\n    def forward(self, x):\n        x = F.max_pool2d(self.conv1(x), 2)\n        x = F.relu(x) + F.relu(-x)\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = self.bn(x)\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nclass Net2(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\n\nclass Net3(nn.Module):\n\n    \"\"\" Simple network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1,32, kernel_size=3, padding=1), # 28\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 14\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2) # 7\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 64 * 7 * 7)\n        x = self.classifier(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoard\u67e5\u770b\n-----------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorboardX import SummaryWriter\n\n# \u65e0\u610f\u4e49\u8f93\u5165\uff0c\u4e0eMNIST\u7684\u4e00\u4e2abatch\u6570\u636e\u7684shape\u76f8\u540c\ndummy_input = torch.autograd.Variable(torch.rand(batch_size, 1, 28, 28))\n\nmodel1 = Net1()\nprint(model1)\nwith SummaryWriter(comment='_fashionmnist_net1') as w:\n    w.add_graph(model1, (dummy_input, ))\n\nmodel2 = Net2()\nprint(model2)\nwith SummaryWriter(comment='_fashionmnist_net2') as w:\n    w.add_graph(model2, (dummy_input, ))\n    \nmodel3 = Net3()\nprint(model3)\nwith SummaryWriter(comment='_fashionmnist_net3') as w:\n    w.add_graph(model3, (dummy_input, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n----------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# \u9009\u62e9\u4e0a\u9762\u5b9a\u4e49\u7684\u4efb\u610f\u4e00\u4e2a\u6a21\u578b model1\uff0cmodel2\uff0cmodel3\uff0c...\nnet = model3.to(device)  #or = model2\n\nloss = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\nwriter = SummaryWriter(comment='_fashionmnist_logs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8ba1\u7b97\u521d\u59cb\u7f51\u7edc\u7684\u51c6\u786e\u7387\n--------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u8bad\u7ec3\n----------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\nnum_batches = len(trainloader)\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for step, data in enumerate(trainloader):\n        n_iter = epoch * num_batches + step\n        images, labels = data[0].to(device), data[1].to(device)\n        # \u5c06\u68af\u5ea6\u6e05\u96f6\n        optimizer.zero_grad()\n        # \u5411\u524d\u4f20\u9012\n        out = net(images)\n        # \u8ba1\u7b97\u635f\u5931\n        loss_value = loss(out, labels)\n        # \u5411\u540e\u4f20\u9012\n        loss_value.backward()\n        # \u4f18\u5316\n        optimizer.step()\n        # \u8bb0\u5f55\u65e5\u5fd7\n        writer.add_scalar('loss', loss_value.item(), n_iter)\n        running_loss += loss_value.item()\n        \n        if step % 500 == 499:    # \u6bcf 500 \u4e2a mini-batches \u5c31\u8f93\u51fa\u4e00\u6b21\u8bad\u7ec3\u4fe1\u606f\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, step + 1, running_loss / 500))\n            running_loss = 0.0\n            \nwriter.close()\nprint('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u6d4b\u8bd5\n----------------\n\n\u5728\u6574\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7edf\u8ba1\u6bcf\u4e2a\u7c7b\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_classes = len(classes)\nclass_correct = list(0. for i in range(num_classes))\nclass_total = list(0. for i in range(num_classes))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(batch_size):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(num_classes):\n    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u6d4b\u8bd5\u6837\u672c\uff0c\u771f\u5b9e\u6807\u7b7e\u548c\u9884\u6d4b\u6807\u7b7e\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images),\"Images of One Test Batch\")\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n\noutputs = net(images.to(device))\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}