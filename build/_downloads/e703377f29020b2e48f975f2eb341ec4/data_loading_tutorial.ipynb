{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b\n====================================\n**\u7ffb\u8bd1\u8005**: `Antares <http://www.studyai.com/antares>`_\n\n\u5728\u89e3\u51b3\u4efb\u4f55\u673a\u5668\u5b66\u4e60\u95ee\u9898\u65f6\u6211\u4eec\u901a\u5e38\u4ed8\u51fa\u4e86\u5f88\u5927\u7684\u52aa\u529b\u6765\u51c6\u5907\u6570\u636e\u3002PyTorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\uff0c\n\u5e76\u5e0c\u671b\u80fd\u591f\u4f7f\u60a8\u7684\u4ee3\u7801\u66f4\u5177\u53ef\u8bfb\u6027\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4e86\u89e3\u5982\u4f55\u4ece\u975e\u5e73\u51e1\u7684\u6570\u636e\u96c6\u4e2d\u52a0\u8f7d\u548c\u9884\u5904\u7406/\u589e\u5f3a\u6570\u636e\u3002\n\n\u8981\u8fd0\u884c\u672c\u6559\u7a0b, \u8bf7\u786e\u4fdd\u5b89\u88c5\u4e86\u8fd9\u4e9b packages :\n\n-  ``scikit-image``: \u7528\u4e8e\u56fe\u50cf\u7684\u8f93\u5165\u8f93\u51fa(IO)\u548c\u53d8\u6362(transforms)\n-  ``pandas``: \u7528\u4e8e\u66f4\u52a0\u7b80\u5355\u7684\u89e3\u6790 csv \u6587\u4ef6\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # \u4ea4\u4e92\u5f0f\u6a21\u5f0f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6211\u4eec\u8981\u5904\u7406\u7684\u6570\u636e\u96c6\u662f\u9762\u90e8\u59ff\u6001(facial pose).\n\u8fd9\u610f\u5473\u7740\u4e00\u5f20\u4eba\u8138\u5c06\u88ab\u5982\u4e0b\u6807\u6ce8:\n\n.. figure:: /_static/img/landmarked_face2.png\n   :width: 400\n\n\u603b\u4f53\u4e0a, \u6bcf\u5f20\u8138\u4e0a\u6807\u6ce8\u4e86 68 \u4e2a\u4e0d\u540c\u7684landmark points\u3002\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\u4ece `\u8fd9\u91cc <https://download.pytorch.org/tutorial/faces.zip>`_ \u4e0b\u8f7d\u6570\u636e\u96c6\uff0c \n    \u4ee5\u4fbf \u56fe\u50cf\u6570\u636e\u7684\u5b58\u653e\u76ee\u5f55\u7ed3\u6784\u662f\u8fd9\u6837\u7684\uff1a'data/faces/' \u3002\n    \u8fd9\u4e2a\u6570\u636e\u96c6\u4e8b\u5b9e\u4e0a\u662f\u4f7f\u7528 `dlib \u7684\u59ff\u6001\u4f30\u8ba1 <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__\n    \u6765\u4ea7\u751f\u7684\uff0c\u6240\u7528\u7684\u56fe\u50cf\u6765\u81ea\u4e8e imagenet \u4e2d\u6807\u8bb0\u4e3a 'face' \u7684\u82e5\u5e72\u5f20\u56fe\u50cf\u3002</p></div>\n\n\u6570\u636e\u96c6\u81ea\u5e26\u4e00\u4e2a csv \u6587\u4ef6\uff0c\u91cc\u9762\u662f\u5b58\u653e\u7740 \u6807\u6ce8(annotations)\uff0c\u5c31\u50cf\u8fd9\u6837\u54d2:\n\n::\n\n    image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n    0805personali01.jpg,27,83,27,98, ... 84,134\n    1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n\n\u8ba9\u6211\u4eec\u5feb\u901f\u8bfb\u53d6 CSV \u6587\u4ef6 \u7136\u540e\u83b7\u5f97\u6807\u6ce8\u4fe1\u606f\uff0c\u5e76\u4fdd\u5b58\u5230\u4e00\u4e2a (N, 2) \u7684\u6570\u7ec4\u4e2d\u53bb\u5427\uff0c\u5176\u4e2d N \u662f\nlandmarks \u7684\u6570\u91cf\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "landmarks_frame = pd.read_csv('./data/faces/face_landmarks.csv')\n\nn = 65\nimg_name = landmarks_frame.iloc[n, 0]\nlandmarks = landmarks_frame.iloc[n, 1:].as_matrix()\nlandmarks = landmarks.astype('float').reshape(-1, 2)\n\nprint('Image name: {}'.format(img_name))\nprint('Landmarks shape: {}'.format(landmarks.shape))\nprint('First 4 Landmarks: {}'.format(landmarks[:4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u7b80\u5355\u7684\u8f85\u52a9\u51fd\u6570\u6765\u663e\u793a\u4e00\u4e2a\u56fe\u50cf\u53ca\u5176\u6807\u6ce8\uff0c\u5e76\u4f7f\u7528\u5b83\u6765\u663e\u793a\u4e00\u4e2a\u793a\u4f8b\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('./data/faces/', img_name)),\n               landmarks)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset \u7c7b\n-------------\n\n``torch.utils.data.Dataset`` \u662f\u8868\u793a\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b\u3002\n\u60a8\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f ``Dataset`` \u5e76\u8986\u76d6\u4ee5\u4e0b\u65b9\u6cd5\uff1a\n\n-  ``__len__`` \u4ee5\u4fbf ``len(dataset)`` \u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u7684size\u3002\n-  ``__getitem__`` \u7528\u4e8e\u652f\u6301\u7c7b\u4f3c ``dataset[i]`` \u8fd9\u6837\u7684\u7d22\u5f15\uff0c\u7528\u6765\u53d6\u5f97\u7b2c $i$ \u4e2a\u6837\u672c\u3002\n\n\u8ba9\u6211\u4eec\u4e3a\u6211\u4eec\u7684\u8138\u5730\u6807\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2aDataSet\u7c7b\u3002\u6211\u4eec\u5c06\u5728 ``__init__`` \u4e2d\u8bfb\u53d6CSV\uff0c\n\u4f46\u5c06\u56fe\u50cf\u7684\u8bfb\u53d6\u7559\u7ed9 ``__getitem__`` \u3002\n\u8fd9\u662f\u5185\u5b58\u6709\u6548\u7684\uff0c\u56e0\u4e3a\u6240\u6709\u7684\u56fe\u50cf\u4e0d\u662f\u4e00\u6b21\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\uff0c\u800c\u662f\u6839\u636e\u9700\u8981\u8bfb\u53d6\u3002\n\n\u6211\u4eec\u6570\u636e\u96c6\u7684\u6837\u672c\u5c06\u4f1a\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u50cf\u8fd9\u6837 \n``{'image': image, 'landmarks': landmarks}``\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u4f1a\u63a5\u53d7\u4e00\u4e2a\u53ef\u9009\u53c2\u6570 \n``transform`` \u4ee5\u4fbf\u4efb\u4f55\u9700\u8981\u7684\u6570\u636e\u9884\u5904\u7406\u53ef\u4ee5\u65bd\u52a0\u5230\u6837\u672c\u4e0a\u3002 \n\u6211\u4eec\u5c06\u4f1a\u5728\u4e0b\u4e00\u4e2a\u5c0f\u8282\u770b\u5230 ``transform`` \u7684\u7528\u5904\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8ba9\u6211\u4eec\u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u8fed\u4ee3\u6570\u636e\u6837\u672c\u3002\u6211\u4eec\u5c06\u6253\u5370\u524d4\u4e2a\u6837\u672c\u7684\u5927\u5c0f\u5e76\u663e\u793a\u5b83\u4eec\u7684landmarks\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "face_dataset = FaceLandmarksDataset(csv_file='./data/faces/face_landmarks.csv',\n                                    root_dir='./data/faces/')\n\nfig = plt.figure(figsize=[6.5,2.5])\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u53d8\u6362(Transforms)\n---------------------\n\n\u4ece\u4e0a\u9762\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u6837\u672c\u7684\u5c3a\u5bf8\u4e0d\u4e00\u6837\u3002\u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u671f\u671b\u5f97\u5230\u56fa\u5b9a\n\u5927\u5c0f\u7684\u56fe\u50cf\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u7f16\u5199\u4e00\u4e9b\u9884\u5904\u7406\u4ee3\u7801\u3002\u8ba9\u6211\u4eec\u521b\u5efa\u4e09\u4e2a\u8f6c\u6362\uff1a\n\n-  ``Rescale``: \u7f29\u653e\u56fe\u50cf\n-  ``RandomCrop``: \u968f\u673a\u88c1\u526a\u56fe\u50cf. \u7528\u4e8e\u6570\u636e\u589e\u5e7f(data augmentation).\n-  ``ToTensor``: \u628a numpy images \u8f6c\u6362\u4e3a torch images (\u6211\u4eec\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74).\n\n\u6211\u4eec\u5c06\u628a\u5b83\u4eec\u5199\u6210\u53ef\u8c03\u7528\u7684\u7c7b(callable classes)\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u51fd\u6570\uff0c\n\u8fd9\u6837\u6bcf\u6b21\u8c03\u7528transform\u65f6\u90fd\u4e0d\u9700\u8981\u4f20\u9012\u8f6c\u6362\u7684\u53c2\u6570\u3002\n\u4e3a\u6b64\uff0c\u6211\u4eec\u53ea\u9700\u5b9e\u73b0 ``__call__`` \u65b9\u6cd5\uff0c\u5982\u679c\u9700\u8981\uff0c\u5219\u5b9e\u73b0 ``__init__`` \u65b9\u6cd5\u3002\n\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8fd9\u6837\u7684\u8f6c\u6362\uff1a\n\n::\n\n    tsfm = Transform(params)\n    transformed_sample = tsfm(sample)\n\n\u4e0b\u9762\u89c2\u5bdf\u5982\u4f55\u5c06\u8fd9\u4e9b\u8f6c\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\u548clandmarks\u3002 \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n    \"\"\"\u628a\u56fe\u50cf\u7f29\u653e\u5230\u4e00\u4e2a\u7ed9\u5b9a\u7684\u5c3a\u5bf8\u3002\n\n    Args:\n        output_size (tuple or int): \u60f3\u8981\u7684\u8f93\u51fa\u5c3a\u5bf8. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        # h \u548c w are swapped for landmarks because for images,\n        # x \u548c y axes are axis 1 and 0 respectively\n        landmarks = landmarks * [new_w / w, new_h / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\n\nclass RandomCrop(object):\n    \"\"\"\u5728\u4e00\u4e2a\u56fe\u50cf\u6837\u672c\u4e0a\u968f\u673a\u88c1\u5207\u56fe\u50cf.\n\n    Args:\n        output_size (tuple or int): \u60f3\u8981\u7684\u8f93\u51fa\u5c3a\u5bf8. If int, square crop is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left, top]\n\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass ToTensor(object):\n    \"\"\"\u628a\u6837\u672c\u7684 ndarrays \u8f6c\u6362\u4e3a Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'landmarks': torch.from_numpy(landmarks)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u590d\u5408\u5f0f\u53d8\u6362\u5668\n~~~~~~~~~~~~~~~~~~\n\n\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u8f6c\u6362\u5e94\u7528\u4e8e\u4e00\u4e2a\u793a\u4f8b\u3002\n\n\u6bd4\u65b9\u8bf4\uff0c\u6211\u4eec\u60f3\u8981\u5c06\u56fe\u50cf\u7684\u8f83\u77ed\u90e8\u5206\u6062\u590d\u5230256\uff0c\n\u7136\u540e\u4ece\u56fe\u50cf\u4e2d\u968f\u673a\u88c1\u526a\u51fa\u5927\u5c0f\u4e3a224\u7684\u65b9\u5f62\u56fe\u50cf\u3002\n\u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u60f3\u8981\u5408\u6210 ``Rescale`` \u548c ``RandomCrop`` \u53d8\u6362\u3002\n``torchvision.transforms.Compose`` \u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53ef\u8c03\u7528\u7c7b\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u8fd9\u6837\u505a\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scale = Rescale(256)\ncrop = RandomCrop(128)\ncomposed = transforms.Compose([Rescale(256),\n                               RandomCrop(224)])\n\n# \u5c06\u4e0a\u8ff0\u6bcf\u4e2a\u8f6c\u6362\u5e94\u7528\u4e8e\u793a\u4f8b\nfig = plt.figure(figsize=[6.5,2.5])\nsample = face_dataset[65]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tsfrm).__name__)\n    show_landmarks(**transformed_sample)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\u904d\u5386\n-----------------------------\n\n\u8ba9\u6211\u4eec\u5c06\u6240\u6709\u8fd9\u4e9b\u653e\u5728\u4e00\u8d77\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u7ec4\u5408\u8f6c\u6362\u7684\u6570\u636e\u96c6\u3002\u603b\u4e4b\uff0c\u6bcf\u6b21\u91c7\u6837\u8be5\u6570\u636e\u96c6\u65f6\n\n-  \u4ece\u6587\u4ef6\u4e2d\u52a8\u6001\u8bfb\u53d6\u4e00\u5f20\u56fe\u50cf\u3002\n-  Transforms \u88ab\u5e94\u7528\u4e8e\u8bfb\u53d6\u51fa\u7684\u56fe\u50cf\u3002\n-  \u7531\u4e8e\u5176\u4e2d\u4e00\u4e2a\u53d8\u6362\u662f\u968f\u673a\u7684\uff0c\u6240\u4ee5\u5728\u91c7\u6837\u65f6\u4f1a\u589e\u52a0\u6570\u636e\u3002 \n\n\u6211\u4eec\u53ef\u4ee5\u50cf\u4ee5\u524d\u4e00\u6837\u4f7f\u7528 ``for i in range`` \u5faa\u73af\u8fed\u4ee3\u521b\u5efa\u7684\u6570\u636e\u96c6\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='./data/faces/face_landmarks.csv',\n                                           root_dir='./data/faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4f46\u662f\uff0c\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684for\u5faa\u73af\u6765\u8fed\u4ee3\u6570\u636e\uff0c\u6211\u4eec\u5931\u53bb\u4e86\u5f88\u591a\u7279\u6027\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u9519\u8fc7\u4e86:\n\n-  \u6279\u91cf\u5316\u6570\u636e\n-  \u968f\u673a\u6253\u4e71\u6570\u636e\n-  \u4f7f\u7528 ``multiprocessing`` workers \u5e76\u884c\u52a0\u8f7d\u6570\u636e.\n\n``torch.utils.data.DataLoader`` \u662f\u4e00\u4e2a\u63d0\u4f9b\u4e86\u4e0a\u8ff0\u7279\u6027\u7684\u8fed\u4ee3\u5668\u3002\n\u6211\u4eec\u5e94\u8be5\u5bf9\u4e0b\u9762\u6240\u7528\u7684\u53c2\u6570\u6709\u6240\u660e\u4e86\u3002 \u5176\u4e2d\u4e00\u4e2a\u6709\u610f\u601d\u7684\u53c2\u6570\u662f ``collate_fn`` \u3002\n\u4f60\u53ef\u4ee5\u901a\u8fc7 ``collate_fn`` \u6765\u660e\u786e\u7684\u6307\u5b9a\u6837\u672c\u5982\u4f55\u88ab\u6279\u91cf\u5316\u3002\n\u7136\u800c, \u9ed8\u8ba4\u7684\u8bbe\u7f6e\u5df2\u7ecf\u8db3\u4ee5\u5e94\u4ed8\u5927\u591a\u6570\u60c5\u51b5\u5566\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# \u663e\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u8f85\u52a9\u51fd\u6570\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure(figsize=[6.5,2.5])\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u540e\u8bb0: torchvision\n----------------------\n\n\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u7f16\u5199\u548c\u4f7f\u7528\u6570\u636e\u96c6\u7c7b\u3001\u53d8\u6362\u5668\u7c7b\u548c\u6570\u636e\u52a0\u8f7d\u5668\u7c7b\u3002\n``torchvision`` \u5305\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5e38\u89c1\u7684\u6570\u636e\u96c6\u7c7b\u548c\u53d8\u6362\u5668\u7c7b\u3002 \u60a8\u751a\u81f3\u53ef\u80fd\u4e0d\u5fc5\u7f16\u5199\u81ea\u5b9a\u4e49\u7c7b\u3002\n``torchvision`` \u4e2d\u53ef\u7528\u7684\u66f4\u901a\u7528\u7684\u6570\u636e\u96c6\u4e4b\u4e00\u662f ``ImageFolder`` \u3002\n\u5b83\u5047\u5b9a\u56fe\u50cf\u7684\u7ec4\u7ec7\u65b9\u5f0f\u5982\u4e0b::\n\n    root/ants/xxx.png\n    root/ants/xxy.jpeg\n    root/ants/xxz.png\n    .\n    .\n    .\n    root/bees/123.jpg\n    root/bees/nsdf3.png\n    root/bees/asd932_.png\n\n\u5176\u4e2d 'ants', 'bees' etc. \u662f\u7c7b\u6807\u7b7e\u3002 \u7c7b\u4f3c\u7684\uff0c\u64cd\u4f5c ``PIL.Image`` \u7c7b\u578b\u7684\u56fe\u50cf\u7684\u901a\u7528\u53d8\u6362\uff0c\u6bd4\u5982\n``RandomHorizontalFlip``, ``Scale`` \u7b49\u4e5f\u662f\u53ef\u7528\u7684\u3002\n\u4f60\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u6765\u5199\u4e00\u4e2a dataloader\uff0c\u5c31\u50cf\u8fd9\u6837::\n\n  import torch\n  from torchvision import transforms, datasets\n\n  data_transform = transforms.Compose([\n          transforms.RandomSizedCrop(224),\n          transforms.RandomHorizontalFlip(),\n          transforms.ToTensor(),\n          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n      ])\n  hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n                                             transform=data_transform)\n  dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n                                               batch_size=4, shuffle=True,\n                                               num_workers=4)\n\n\u8981\u627e\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u53d8\u6362\u5668\u548c\u8bad\u7ec3\u8fc7\u7a0b\u7ed3\u5408\u8d77\u6765\u7684\u4f8b\u5b50, \u8bf7\u770b\n:doc:`transfer_learning_tutorial`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}