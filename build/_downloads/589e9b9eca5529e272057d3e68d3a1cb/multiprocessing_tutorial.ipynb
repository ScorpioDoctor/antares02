{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch\u591a\u8fdb\u7a0b\u5171\u4eab\u5185\u5b58\n=======================\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n\u8fd9\u662f\u4e00\u4e2a\u7528PyTorch\u7684\u591a\u8fdb\u7a0b ``torch.multiprocessing`` \u5e76\u884c\u8bad\u7ec3\u6a21\u578b\u7684\u6848\u4f8b:\n\u6211\u4eec\u7528\u591a\u4e2a\u8fdb\u7a0b\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2aMNIST\u7f51\u7edc\u6a21\u578b\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a \u201c\u75af\u72c2MNIST(mnist_hogwild)\u201d\u3002\n\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u591a\u4e2a\u8fdb\u7a0b\u5171\u4eab\u5185\u5b58\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5bfc\u5165\u4f9d\u8d56\u5305\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\nimport argparse\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\nimport torch.optim as optim\nfrom torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49CNN\u6a21\u578b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# \u6253\u5370\u8f93\u51fa\u7f51\u7edc\u7ed3\u6784\nprint(Net())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_epoch(epoch, args, model, data_loader, optimizer):\n    model.train()\n    pid = os.getpid()\n    for batch_idx, (data, target) in enumerate(data_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n                100. * batch_idx / len(data_loader), loss.item()))\n\n\ndef train(rank, args, model):\n    torch.manual_seed(args.seed + rank)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                    transform=transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.1307,), (0.3081,))\n                    ])),\n        batch_size=args.batch_size, shuffle=True, num_workers=1)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, train_loader, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u6d4b\u8bd5\u8fc7\u7a0b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in data_loader:\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1)[1] # get the index of the max log-probability\n            correct += pred.eq(target).sum().item()\n\n    test_loss /= len(data_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))\n\n\ndef test(args, model):\n    torch.manual_seed(args.seed)\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=args.batch_size, shuffle=True, num_workers=1)\n\n    test_epoch(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u591a\u8fdb\u7a0b\u8bad\u7ec3\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u8bad\u7ec3\u914d\u7f6e\u9879\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--num-processes', type=int, default=2, metavar='N',\n                    help='how many training processes to use (default: 2)')\n\n\nif __name__ == \"__main__\":\n    # \u53c2\u6570\u89e3\u6790\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    # \u5b9e\u4f8b\u5316\u6a21\u578b\n    model = Net()\n    model.share_memory() # gradients are allocated lazily, so they are not shared here\n\n    # \u5f00\u542f\u591a\u8fdb\u7a0b\u8bad\u7ec3\n    processes = []\n    for rank in range(args.num_processes):\n        p = mp.Process(target=train, args=(rank, args, model))\n        # We first train the model across `num_processes` processes\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n    # Once training is complete, we can test the model\n    test(args, model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}