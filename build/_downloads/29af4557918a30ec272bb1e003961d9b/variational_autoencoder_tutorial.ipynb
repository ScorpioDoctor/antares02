{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\n=======================\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n\u8fd9\u662fKingma\u548cWelling\u5bf9\u8bba\u6587(`[\u968f\u673a\u68af\u5ea6vb\u548c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668] <http://arxiv.org/abs/1312.6114>`__)\u7684\u6539\u8fdb\u5b9e\u73b0\u3002\n\u5b83\u4f7f\u7528ReLUs\u6fc0\u6d3b\u51fd\u6570\u548cAdam\u4f18\u5316\u5668\uff0c\u800c\u4e0d\u662fsigmoids\u6fc0\u6d3b\u51fd\u6570\u548cAdagrad\u4f18\u5316\u5668\u3002\u8fd9\u4e9b\u6539\u52a8\u4f7f\u7f51\u7edc\u6536\u655b\u5f97\u66f4\u5feb\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5bfc\u5165\u4f9d\u8d56\u5305\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\nimport argparse\nimport torch\nimport torch.utils.data\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5168\u5c40\u53c2\u6570\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='VAE MNIST Example')\nparser.add_argument('--batch-size', type=int, default=128, metavar='N',\n                    help='\u7528\u4e8e\u8bad\u7ec3\u7684\u8f93\u5165\u6279\u6b21\u5927\u5c0f (default: 128)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='\u7528\u4e8e\u8bad\u7ec3\u7684\u56de\u5408\u6570 (default: 10)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='\u542f\u7528 CUDA \u8bad\u7ec3')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='\u968f\u673a\u6570\u79cd\u5b50 (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='\u6bcf\u9694\u591a\u5c11\u4e2a\u6279\u6b21\u8bb0\u5f55\u4e00\u6b21\u8bad\u7ec3\u72b6\u6001')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u51c6\u5907\u6570\u636e\u96c6\u548c\u6570\u636e\u52a0\u8f7d\u5668\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u6307\u5b9a\u8ba1\u7b97\u8bbe\u5907\ntorch.manual_seed(12321236)\ndevice = torch.device(\"cuda\" if args.cuda else \"cpu\")\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data/mnist', train=True, download=True,\n                   transform=transforms.ToTensor()),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data/mnist', train=False, transform=transforms.ToTensor()),\n    batch_size=args.batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u663e\u793a\u56fe\u50cf\u6570\u636e\u96c6\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n# \u6253\u5f00\u4ea4\u4e92\u6a21\u5f0f\nplt.ion()\n# \u5ffd\u7565 warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# \u7528\u4e8e\u663e\u793a\u4e00\u5f20\u56fe\u50cf\u7684\u51fd\u6570\ndef imshow(img,title=None):\n    npimg = img.numpy()\n    plt.figure(figsize=[6.5,2.5])\n    plt.tight_layout()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n    plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n\n# \u83b7\u53d6\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\uff0c\u4e00\u6b21\u8fed\u4ee3\u53d6\u51fabatch_size\u5f20\u56fe\u7247\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(images[:16].size())\n# \u663e\u793a16\u5f20\u56fe\u50cf\nimshow(make_grid(images[:16]), \"The Original Digits Images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u578b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n    def encode(self, x):\n        h1 = F.relu(self.fc1(x))\n        return self.fc21(h1), self.fc22(h1)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5*logvar)\n        eps = torch.randn_like(std)\n        return eps.mul(std).add_(mu)\n\n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h3))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x.view(-1, 784))\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\nmodel = VAE().to(device)\n\n# \u6253\u5370\u8f93\u51fa\u7f51\u7edc\u7ed3\u6784\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u635f\u5931\u51fd\u6570: Reconstruction + KL divergence losses summed over all elements and batch\ndef loss_function(recon_x, x, mu, logvar):\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n\n    # see Appendix B from VAE paper:\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n    # https://arxiv.org/abs/1312.6114\n    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    return BCE + KLD\n\n# \u4f18\u5316\u5668\noptimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u8bad\u7ec3\u8fc7\u7a0b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(data)))\n\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, train_loss / len(train_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u6d4b\u8bd5\u8fc7\u7a0b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n    model.eval()\n    test_loss = 0\n    with torch.no_grad():\n        for i, (data, _) in enumerate(test_loader):\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n            if i == 0:\n                n = min(data.size(0), 8)\n                comparison = torch.cat([data[:n],\n                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n                # \u4e0b\u9762\u8fd9\u4e2a\u51fd\u6570\u4fdd\u5b58\u56fe\u50cf\u7684\u65f6\u5019\u6709\u95ee\u9898\uff0c\u4e0d\u77e5\u5565\u539f\u56e0\n                # save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n                if epoch % args.epochs == 0:\n                    # \u663e\u793a\u6700\u540e\u7684\u4e00\u6b21epoch\u540e\u89e3\u7801\u56fe\u50cf\u4e0e\u539f\u56fe\u50cf\u4f5c\u5bf9\u6bd4\n                    imshow(make_grid(comparison.cpu()), \"First Row:Original Images,Sencond Row:Decoded Images\")\n\n    test_loss /= len(test_loader.dataset)\n    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u8bad\u7ec3\u548c\u6d4b\u8bd5\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u968f\u673a\u6570\u636e\u89e3\u7801\n---------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    sample = torch.randn(32, 20).to(device)\n    sample = model.decode(sample).cpu()\n    # \u4e0b\u9762\u8fd9\u4e2a\u51fd\u6570\u4fdd\u5b58\u56fe\u50cf\u7684\u65f6\u5019\u6709\u95ee\u9898\uff0c\u4e0d\u77e5\u5565\u539f\u56e0\n    # save_image(sample.view(32, 1, 28, 28), 'results/sample_' + '.png')\n    if epoch % args.epochs == 0:\n        imshow(make_grid(sample.view(32, 1, 28, 28).cpu()), \"Decoded Images From Gaussian Data\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}