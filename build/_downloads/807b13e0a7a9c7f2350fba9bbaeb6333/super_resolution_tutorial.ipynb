{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u4f7f\u7528\u4e9a\u50cf\u7d20CNN\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\n=====================================\n**\u6559\u7a0b\u4f5c\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n\u8d85\u5206\u8fa8\u7387\u662f\u63d0\u9ad8\u56fe\u50cf\u3001\u89c6\u9891\u5206\u8fa8\u7387\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6216\u89c6\u9891\u7f16\u8f91\u3002\n\n\u672c\u6559\u7a0b\u6559\u4f60\u5982\u4f55\u5728\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u4f7f\u7528\u9ad8\u6548\u7684\u4e9a\u50cf\u7d20\u5377\u79ef\u5c42\u63d0\u9ad8\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1\u7684\u56fe\u50cf\u7a7a\u95f4\u5206\u8fa8\u7387\u3002\n\u672c\u6559\u7a0b\u6240\u5b9e\u73b0\u7684\u7b97\u6cd5\u51fa\u81ea\u6587\u7ae0: `[\"Real-Time Single Image and Video Super-Resolution Using an \nEfficient Sub-Pixel Convolutional Neural Network\" - Shi et al.] <https://arxiv.org/abs/1609.05158>`__\n\n\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5728\u6570\u636e\u96c6 `[BSD300 dataset] <https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/>`__  \n\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\u6a21\u578b\u3002\u603b\u5171\u4f7f\u7528\u4e86200\u5f20\u8bad\u7ec3\u56fe\u50cf\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u88c1\u526a(crops)\uff0c\u5728100\u5f20\u7ecf\u8fc7\u88c1\u526a\u7684\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002 \n\u6bcf\u4e2a\u56de\u5408(epoch)\u8bad\u7ec3\u5b8c\u6bd5,\u7a0b\u5e8f\u90fd\u4f1a\u4fdd\u5b58\u4e00\u4e2a\u68c0\u67e5\u70b9\u6587\u4ef6 ``model_epoch_<epoch_number>.pth`` \u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u6570\u636e\u96c6\u7c7b\u7684\u5b50\u7c7b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.utils.data as data\n\nfrom os import listdir\nfrom os.path import join\nfrom PIL import Image\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n\n\ndef load_img(filepath):\n    img = Image.open(filepath).convert('YCbCr')\n    y, _, _ = img.split()\n    return y\n\n\nclass DatasetFromFolder(data.Dataset):\n    def __init__(self, image_dir, input_transform=None, target_transform=None):\n        super(DatasetFromFolder, self).__init__()\n        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        input = load_img(self.image_filenames[index])\n        target = input.copy()\n        if self.input_transform:\n            input = self.input_transform(input)\n        if self.target_transform:\n            target = self.target_transform(target)\n\n        return input, target\n\n    def __len__(self):\n        return len(self.image_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6570\u636e\u96c6\u7684\u4e0b\u8f7d,\u53d8\u6362\uff0c\u83b7\u53d6\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from math import log10\nfrom os.path import exists, join, basename\nfrom os import makedirs, remove\nfrom six.moves import urllib\nimport tarfile\nfrom torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n\ndef download_bsd300(dest=\"dataset\"):\n    output_image_dir = join(dest, \"BSDS300/images\")\n\n    if not exists(output_image_dir):\n        makedirs(dest)\n        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n        print(\"downloading url \", url)\n\n        data = urllib.request.urlopen(url)\n\n        file_path = join(dest, basename(url))\n        with open(file_path, 'wb') as f:\n            f.write(data.read())\n\n        print(\"Extracting data\")\n        with tarfile.open(file_path) as tar:\n            for item in tar:\n                tar.extract(item, dest)\n\n        remove(file_path)\n\n    return output_image_dir\n\n\ndef calculate_valid_crop_size(crop_size, upscale_factor):\n    return crop_size - (crop_size % upscale_factor)\n\n\ndef input_transform(crop_size, upscale_factor):\n    return Compose([\n        CenterCrop(crop_size),\n        Resize(crop_size // upscale_factor),\n        ToTensor(),\n    ])\n\n\ndef target_transform(crop_size):\n    return Compose([\n        CenterCrop(crop_size),\n        ToTensor(),\n    ])\n\n\ndef get_training_set(upscale_factor):\n    root_dir = download_bsd300()\n    train_dir = join(root_dir, \"train\")\n    crop_size = calculate_valid_crop_size(256, upscale_factor)\n\n    return DatasetFromFolder(train_dir,\n                             input_transform=input_transform(crop_size, upscale_factor),\n                             target_transform=target_transform(crop_size))\n\n\ndef get_test_set(upscale_factor):\n    root_dir = download_bsd300()\n    test_dir = join(root_dir, \"test\")\n    crop_size = calculate_valid_crop_size(256, upscale_factor)\n\n    return DatasetFromFolder(test_dir,\n                             input_transform=input_transform(crop_size, upscale_factor),\n                             target_transform=target_transform(crop_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u53c2\u6570\u89e3\u6790\u5668\u914d\u7f6e\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\n\n# \u8bad\u7ec3\u53c2\u6570\u914d\u7f6e\nparser = argparse.ArgumentParser(description='PyTorch Super Res Example')\nparser.add_argument('--upscale_factor', type=int, required=True, help=\"super resolution upscale factor\")\nparser.add_argument('--batchSize', type=int, default=64, help='training batch size')\nparser.add_argument('--testBatchSize', type=int, default=10, help='testing batch size')\nparser.add_argument('--nEpochs', type=int, default=2, help='number of epochs to train for')\nparser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')\nparser.add_argument('--cuda', action='store_true', help='use cuda?')\nparser.add_argument('--threads', type=int, default=4, help='number of threads for data loader to use')\nparser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\nopt = parser.parse_args()\n\nprint(opt)\n\nif opt.cuda and not torch.cuda.is_available():\n    raise Exception(\"No GPU found, please run without --cuda\")\n\ntorch.manual_seed(opt.seed)\n\ndevice = torch.device(\"cuda\" if opt.cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7f51\u7edc\u6a21\u578b\u7684\u8bbe\u8ba1\u4e0e\u6784\u5efa\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.init as init\n\nclass Net(nn.Module):\n    def __init__(self, upscale_factor):\n        super(Net, self).__init__()\n\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv4.weight)\n\n# \u5b9e\u4f8b\u5316\u7f51\u7edc\u6a21\u578b\nprint('===> Building model')\nmodel = Net(upscale_factor=opt.upscale_factor).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u635f\u5931\u51fd\u6570\u4e0e\u4f18\u5316\u5668\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n\ncriterion = nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=opt.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d\u6570\u636e\u96c6\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n\nprint('===> Loading datasets')\ntrain_set = get_training_set(opt.upscale_factor)\ntest_set = get_test_set(opt.upscale_factor)\ntraining_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\ntesting_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bad\u7ec3\u51fd\u6570\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n    epoch_loss = 0\n    for iteration, batch in enumerate(training_data_loader, 1):\n        input, target = batch[0].to(device), batch[1].to(device)\n\n        optimizer.zero_grad()\n        loss = criterion(model(input), target)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n\n    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6d4b\u8bd5\u51fd\u6570\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test():\n    avg_psnr = 0\n    with torch.no_grad():\n        for batch in testing_data_loader:\n            input, target = batch[0].to(device), batch[1].to(device)\n\n            prediction = model(input)\n            mse = criterion(prediction, target)\n            psnr = 10 * log10(1 / mse.item())\n            avg_psnr += psnr\n    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u8bad\u7ec3\u6d4b\u8bd5\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def checkpoint(epoch):\n    model_out_path = \"model_epoch_{}.pth\".format(epoch)\n    torch.save(model, model_out_path)\n    print(\"Checkpoint saved to {}\".format(model_out_path))\n\nfor epoch in range(1, opt.nEpochs + 1):\n    train(epoch)\n    test()\n    checkpoint(epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}