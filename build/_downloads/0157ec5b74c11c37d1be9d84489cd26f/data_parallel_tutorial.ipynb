{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u53ef\u9009\u9879: \u6570\u636e\u5e76\u884c\n==========================\n**Authors**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang <https://github.com/jennykang>`_\n\n\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u591a\u4e2aGPU\uff1a ``DataParallel`` \u7684\u7528\u6cd5.\n\n\u4e0ePyTorch\u4e00\u8d77\u4f7f\u7528GPU\u975e\u5e38\u5bb9\u6613\u3002\u60a8\u53ef\u4ee5\u5c06\u6a21\u578b\u653e\u5728GPU\u4e0a:\n\n.. code:: python\n\n    device = torch.device(\"cuda:0\")\n    model.to(device)\n\nThen, you can copy all your tensors to the GPU:\n\n.. code:: python\n\n    mytensor = my_tensor.to(device)\n\n\u8bf7\u6ce8\u610f\uff0c\u53ea\u8981\u8c03\u7528  ``my_tensor.to(device)`` \uff0c\u5c31\u4f1a\u5728GPU\u4e0a\u8fd4\u56de ``my_tensor`` \u7684\u65b0\u526f\u672c\uff0c\u800c\u4e0d\u662f\u91cd\u5199 ``my_tensor`` \u3002\n\u60a8\u9700\u8981\u5c06\u5b83\u5206\u914d\u7ed9\u4e00\u4e2a\u65b0\u7684tensor\uff0c\u5e76\u5728GPU\u4e0a\u4f7f\u7528\u8be5tensor\u3002\n\n\u5728\u591a\u4e2aGPU\u4e0a\u6267\u884c\u524d\u5411\u3001\u540e\u5411\u4f20\u64ad\u662f\u5f88\u81ea\u7136\u7684\u3002\u4f46\u662f\uff0cPYTORCH\u9ed8\u8ba4\u53ea\u4f7f\u7528\u4e00\u4e2aGPU\u3002\n\u4f60\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u591a\u4e2aGPU\u4e0a\u8fd0\u884c\u60a8\u7684\u64cd\u4f5c\uff0c\u65b9\u6cd5\u662f\u8ba9\u4f60\u7684\u6a21\u578b\u4f7f\u7528 ``DataParallel`` \u5e76\u884c\u8fd0\u884c:\n\n.. code:: python\n\n    model = nn.DataParallel(model)\n\n\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u66f4\u8be6\u7ec6\u5730\u63a2\u8ba8\u5b83\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5bfc\u5165 \u4e0e \u53c2\u6570 \n----------------------\n\n\u5bfc\u5165 PyTorch \u6a21\u5757\u548c\u5b9a\u4e49\u53c2\u6570\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# \u53c2\u6570 \u548c \u6570\u636e\u52a0\u8f7d\u5668 DataLoaders\ninput_size = 5\noutput_size = 2\n\nbatch_size = 30\ndata_size = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bbe\u5907\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u865a\u62df\u6570\u636e\u96c6\n-------------\n\n\u5236\u9020\u4e00\u4e2a \u865a\u62df\u7684(\u968f\u673a\u4ea7\u751f) \u6570\u636e\u96c6\u3002\u4f60\u53ea\u9700\u8981\u5b9e\u73b0 Python \u7684 \u9b54\u6cd5\u51fd\u6570 ``getitem`` :\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RandomDataset(Dataset):\n\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n                         batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7b80\u5355\u6a21\u578b\n------------\n\n\u5bf9\u4e8e\u6f14\u793a\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u83b7\u5f97\u4e00\u4e2a\u8f93\u5165\uff0c\u6267\u884c\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u8f93\u51fa\u3002\n\u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u5728\u4efb\u4f55\u6a21\u578b(CNN\u3001RNN\u3001Capsule Net\u7b49)\u4e0a\u4f7f\u7528 ``DataParallel`` \u3002\n\n\u6211\u4eec\u5728\u6a21\u578b\u4e2d\u653e\u7f6e\u4e86\u4e00\u4e2aprint\u8bed\u53e5\u6765\u76d1\u89c6\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002\n\u8bf7\u6ce8\u610f\u6279\u6b210\u7684\u6253\u5370\u5185\u5bb9\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n    # \u6211\u4eec\u7684\u6a21\u578b\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c\n-----------------------------\n\n\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\uff0c\u5e76\u68c0\u67e5\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\n\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 ``nn.DataParallel`` \u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002\n\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b ``model.to(device)`` \u5c06\u6211\u4eec\u7684\u6a21\u578b\u653e\u5728GPU\u4e0a\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model(input_size, output_size)\nif torch.cuda.device_count() > 1:\n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u6a21\u578b\n-------------\n\n\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in rand_loader:\n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7ed3\u679c\n-------\n\n\u5982\u679c\u60a8\u6ca1\u6709GPU\u6216\u4e00\u4e2aGPU\uff0c\u5f53\u6211\u4eec\u6279\u5904\u740630\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\u65f6\uff0c\u6a21\u578b\u5f97\u523030\uff0c\u8f93\u51fa\u4e0e\u9884\u671f\u76f8\u540c\u3002\n\u4f46\u662f\u5982\u679c\u4f60\u6709\u591a\u4e2aGPU\uff0c\u90a3\u4e48\u4f60\u53ef\u4ee5\u5f97\u5230\u8fd9\u6837\u7684\u7ed3\u679c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 2 GPUs\n# ~~~~~~\n#\n# If you have 2, you will see:\n#\n# .. code:: bash\n#\n#     # on 2 GPUs\n#     Let's use 2 GPUs!\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n#\n# 3 GPUs\n# ~~~~~~\n#\n# If you have 3 GPUs, you will see:\n#\n# .. code:: bash\n#\n#     Let's use 3 GPUs!\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n#\n# 8 GPUs\n# ~~~~~~~~~~~~~~\n#\n# If you have 8, you will see:\n#\n# .. code:: bash\n#\n#     Let's use 8 GPUs!\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u603b\u7ed3\n-------\n\nDataParallel \u4f1a\u81ea\u52a8\u62c6\u5206\u6570\u636e\uff0c\u5e76\u5c06\u4f5c\u4e1a\u8ba2\u5355\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002\n\u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u5b83\u4eec\u7684\u5de5\u4f5c\u4e4b\u540e\uff0cDataParallel \u5728\u5c06\u7ed3\u679c\u8fd4\u56de\u7ed9\u4f60\u4e4b\u524d\u6536\u96c6\u548c\u5408\u5e76\u7ed3\u679c\u3002\n\n\u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u770b \nhttps://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}