{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u4f7f\u7528 PyTorch \u8fdb\u884c\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\n=============================\n\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n\n\u4ecb\u7ecd\n------------\n\n\u672c\u6559\u7a0b\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b9e\u73b0\u7531Leon A.Gatys\u5f00\u53d1\u7684 `Neural-Style algorithm <https://arxiv.org/abs/1508.06576>`__ \u3002\nNeural-Style, \u6216 Neural-Transfer, \u5141\u8bb8\u4f60\u5bf9\u4e00\u5e45\u56fe\u50cf\u91c7\u53d6\u4e00\u79cd\u65b0\u7684\u827a\u672f\u98ce\u683c\u7684\u5f62\u8c61\u548c\u518d\u73b0\u3002\n\u8be5\u7b97\u6cd5\u63a5\u53d7\u8f93\u5165\u56fe\u50cf(input image)\u3001 \u5185\u5bb9\u56fe\u50cf(content-image)\u548c\u98ce\u683c\u56fe\u50cf(style-image)\u4e09\u79cd\u56fe\u50cf\uff0c\u5e76\u5bf9\u8f93\u5165\u8fdb\u884c\u4fee\u6539\uff0c\n\u4f7f\u4e4b\u4e0e\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u548c\u98ce\u683c\u56fe\u50cf\u7684\u827a\u672f\u98ce\u683c\u76f8\u4f3c\u3002\n \n.. figure:: /_static/img/neural-style/neuralstyle.png\n   :alt: content1\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5e95\u5c42\u539f\u7406\n--------------------\n\n\u539f\u7406\u5f88\u7b80\u5355\uff1a\u6211\u4eec\u5b9a\u4e49\u4e86\u4e24\u4e2a\u8ddd\u79bb\uff0c\u4e00\u4e2a\u7528\u4e8e\u5185\u5bb9($D_C$) \uff0c\u4e00\u4e2a\u7528\u4e8e\u6837\u5f0f($D_S$)\u3002\n$D_C$ \u6d4b\u91cf\u4e24\u4e2a\u56fe\u50cf\u4e4b\u95f4\u7684\u5185\u5bb9\u6709\u591a\u4e0d\u540c\uff0c\u800c $D_S$ \u6d4b\u91cf\u4e24\u4e2a\u56fe\u50cf\u4e4b\u95f4\u7684\u98ce\u683c\u6709\u591a\u4e0d\u540c\u3002\n\u7136\u540e\uff0c\u6211\u4eec\u63a5\u53d7\u7b2c\u4e09\u4e2a\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8f6c\u6362\u5b83\uff0c\u4ee5\u6700\u5c0f\u5316\u5b83\u4e0e\u5185\u5bb9\u56fe\u50cf\u7684\u5185\u5bb9\u8ddd\u79bb\u548c\n\u4e0e\u6837\u5f0f\u56fe\u50cf\u7684\u98ce\u683c\u8ddd\u79bb\u3002\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u5e76\u5f00\u59cb neural transfer\u3002\n\n\u5bfc\u5165\u5305\u548c\u9009\u62e9\u8bbe\u5907\n-----------------------------------------\n\u4e0b\u9762\u6240\u5217\u51fa\u7684\u5305\u90fd\u662f\u5b9e\u73b0 neural transfer \u65f6\u6240\u7528\u5230\u7684\u5305\u3002\n\n-  ``torch``, ``torch.nn``, ``numpy`` (\u7528PyTorch\u795e\u7ecf\u7f51\u7edc\u4e0d\u53ef\u7f3a\u5c11\u7684\u8f6f\u4ef6\u5305)\n-  ``torch.optim`` (\u9ad8\u6548\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\u5305)\n-  ``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (\u52a0\u8f7d\u548c\u5c55\u793a\u56fe\u50cf\u7684\u5305)\n-  ``torchvision.transforms`` (\u628a PIL \u56fe\u50cf\u8f6c\u6362\u4e3atensors)\n-  ``torchvision.models`` (\u8bad\u7ec3 \u548c \u52a0\u8f7d \u9884\u8bad\u7ec3\u7684\u6a21\u578b)\n-  ``copy`` (\u6df1\u5ea6\u62f7\u8d1d\u6a21\u578b; system package)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u9009\u62e9\u5728\u54ea\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\u7f51\u7edc\uff0c\u5e76\u5bfc\u5165\u5185\u5bb9\u548c\u6837\u5f0f\u56fe\u50cf\u3002\n\u5728\u5927\u578b\u56fe\u50cf\u4e0a\u8fd0\u884cneural transfer\u7b97\u6cd5\u9700\u8981\u82b1\u8d39\u66f4\u957f\u7684\u65f6\u95f4\uff0c\u5e76\u4e14\u5728GPU\u4e0a\u8fd0\u884c\u7684\u901f\u5ea6\u8981\u5feb\u5f97\u591a\u3002\n\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 ``torch.cuda.is_available()`` \u6765\u68c0\u6d4b\u662f\u5426\u6709\u53ef\u7528\u7684GPU\u3002\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06 ``torch.device`` \u8bbe\u7f6e\u4e3a\u5728\u6574\u4e2a\u6559\u7a0b\u4e2d\u4f7f\u7528\u3002\n\u6b64\u5916\uff0c``.to(device)`` \u65b9\u6cd5\u7528\u4e8e\u5c06\u5f20\u91cf\u6216\u6a21\u5757\u79fb\u52a8\u5230\u6240\u9700\u7684\u8bbe\u5907\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d\u56fe\u50cf\n------------------\n\n\u73b0\u5728\u6211\u4eec\u5c06\u5bfc\u5165\u6837\u5f0f\u56fe\u50cf\u548c\u5185\u5bb9\u56fe\u50cf\u3002\u539f\u59cbPIL\u56fe\u50cf\u7684\u503c\u4ecb\u4e8e0\u5230255\u4e4b\u95f4\uff0c\u4f46\u662f\u5f53\u8f6c\u6362\u4e3atorch tensors\u65f6\uff0c\n\u5b83\u4eec\u7684\u503c\u88ab\u8f6c\u6362\u4e3a0\u548c1\u4e4b\u95f4\u3002\u56fe\u50cf\u4e5f\u9700\u8981\u8c03\u6574\u5927\u5c0f\u4ee5\u5177\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\u3002\u9700\u8981\u6ce8\u610f\u7684\u4e00\u4e2a\u91cd\u8981\u7ec6\u8282\u662f\uff0c\ntorch library\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u5f20\u91cf\u503c\u4ece0\u52301\u53d8\u5316\u3002\u5982\u679c\u60a8\u8bd5\u56fe\u5411\u7f51\u7edc\u63d0\u4f9b\u53d6\u503c\u4e3a0\u5230255\u7684\u5f20\u91cf\u56fe\u50cf\uff0c\n\u90a3\u4e48\u6fc0\u6d3b\u7684\u7279\u5f81\u6620\u5c04\u5c06\u65e0\u6cd5\u611f\u89c9\u5230\u9884\u671f\u7684\u5185\u5bb9\u548c\u6837\u5f0f\u3002 \u7136\u800c\uff0c\u6765\u81eaCaffe\u5e93\u7684\u9884\u8bad\u7ec3\u7f51\u7edc\u88ab\u8bad\u7ec3\u62100\u5230255\u7684\u5f20\u91cf\u56fe\u50cf.\n\n.. Note::\n    \u8fd9\u91cc\u662f\u672c\u6559\u7a0b\u7528\u5230\u7684\u4e24\u5f20\u56fe\u7247\u7684\u4e0b\u8f7d\u5730\u5740:\n    `picasso.jpg <https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg>`__ \u548c\n    `dancing.jpg <https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg>`__.\n    \u4e0b\u8f7d\u8fd9\u4e24\u5f20\u56fe\u7247\u7136\u540e\u5c06\u5b83\u4eec\u653e\u5230\u4f60\u5f53\u524d\u5de5\u4f5c\u76ee\u5f55\u4e2d\u540d\u79f0\u4e3a ``images`` \u7684\u6587\u4ef6\u5939\u4e2d\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u8f93\u51fa\u56fe\u50cf\u7684\u671f\u671b\u5c3a\u5bf8\nimsize = 512 if torch.cuda.is_available() else 128  # \u5982\u679c\u6ca1\u6709GPU\u7684\u8bdd\uff0c\u5c31\u628a\u5c3a\u5bf8\u641e\u5c0f\u70b9\u513f\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),  # \u7f29\u653e\u5bfc\u5165\u7684\u56fe\u50cf\n    transforms.ToTensor()])  # \u628a\u5b83\u8f6c\u6362\u6210 torch tensor\n\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # \u865a\u62df\u7684 batch \u7ef4 \uff0c\u4e3a\u4e86\u6ee1\u8db3\u7f51\u7edc\u8f93\u5165\u5bf9\u7eac\u5ea6\u7684\u8981\u6c42\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n\nstyle_img = image_loader(\"./data/images/neural-style/sky.jpg\")\ncontent_img = image_loader(\"./data/images/neural-style/builds.jpg\")\ninput_img = image_loader(\"./data/images/neural-style/me.jpg\")\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u901a\u8fc7\u5c06\u56fe\u50cf\u7684\u526f\u672c\u8f6c\u6362\u4e3aPIL\u683c\u5f0f\u5e76\u4f7f\u7528 ``plt.imshow`` \u663e\u793a\u526f\u672c\n\u6765\u521b\u5efa\u4e00\u4e2a\u663e\u793a\u56fe\u50cf\u7684\u51fd\u6570\u3002\u6211\u4eec\u5c06\u5c1d\u8bd5\u663e\u793a\u5185\u5bb9\u56fe\u50cf\u548c\u6837\u5f0f\u56fe\u50cf\uff0c\n\u4ee5\u786e\u4fdd\u5b83\u4eec\u88ab\u6b63\u786e\u5bfc\u5165\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()  # \u518d\u6b21\u8f6c\u6362\u4e3a PIL image\n\nplt.ion()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n    image = image.squeeze(0)      # remove the fake batch dimension\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # pause a bit so that plots are updated\n\n\nplt.figure(figsize=[4.0, 4.0])\nimshow(style_img, title='Style Image')\n\nplt.figure(figsize=[4.0, 4.0])\nimshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u635f\u5931\u51fd\u6570\n--------------\n\u5185\u5bb9\u635f\u5931(Content Loss)\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u5185\u5bb9\u635f\u5931\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u5b83\u8868\u793a\u4e86\u4e00\u4e2a\u5355\u72ec\u5c42\u7684\u52a0\u6743\u5185\u5bb9\u8ddd\u79bb\u3002\u8be5\u51fd\u6570\u63a5\u6536\u5904\u7406\u8f93\u5165 $X$ \u7684\u7f51\u7edc\u7684\n\u5c42 $L$ \u7684\u7279\u5f81\u56fe $F_{XL}$ \uff0c\u8fd4\u56de\u8f93\u5165\u56fe\u50cf $X$ \u548c \u5185\u5bb9\u56fe\u50cf $C$\u4e4b\u95f4\u7684\n\u52a0\u6743\u5185\u5bb9\u8ddd\u79bb $w_{CL}.D_C^L(X,C)$ \u3002 \u5185\u5bb9\u56fe\u50cf\u7684\u7279\u5f81\u56fe($F_{CL}$)\u5fc5\u987b\u5df2\u77e5\u4ee5\u4fbf\u80fd\u591f\u8ba1\u7b97\n\u5185\u5bb9\u8ddd\u79bb\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u51fd\u6570\u5b9e\u73b0\u4e3a\u4e00\u4e2a torch module \uff0c\u5b83\u6709\u4e00\u4e2a\u6784\u9020\u5668 \u63a5\u53d7 $F_{CL}$ \u4f5c\u4e3a\u8f93\u5165\u3002\n\u8be5\u8ddd\u79bb $\\|F_{XL} - F_{CL}\\|^2$ \u662f\u4e24\u4e2a\u7279\u5f81\u56fe\u96c6\u5408\u4e4b\u95f4\u7684\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\uff0c\u53ef\u4ee5\u4f7f\u7528 ``nn.MSELoss`` \n\u6765\u8ba1\u7b97\u3002\n\n\u6211\u4eec\u5c06\u628a\u8fd9\u4e2a\u5185\u5bb9\u635f\u5931module\u76f4\u63a5\u52a0\u5230\u8ba1\u7b97\u5185\u5bb9\u8ddd\u79bb\u7684\u5377\u79ef\u5c42\u540e\u9762\u3002\u5728\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u6bcf\u6b21\u7f51\u7edc\u63a5\u5230\u4e00\u5f20\u8f93\u5165\u56fe\u50cf\uff0c\n\u5185\u5bb9\u635f\u5931\u5c06\u4f1a\u5728\u9700\u8981\u7684\u5c42\u88ab\u8ba1\u7b97\u51fa\u6765\uff0c\u5e76\u4e14\u56e0\u4e3a auto grad, \u6240\u6709\u7684\u68af\u5ea6\u5c06\u4f1a\u88ab\u8ba1\u7b97\u3002\n\u73b0\u5728, \u4e3a\u4e86\u4f7f\u5f97\u5185\u5bb9\u635f\u5931\u5c42\u53d8\u5f97\u900f\u660e\uff0c\u6211\u4eec\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a ``forward`` \u65b9\u6cd5 \u6765\u8ba1\u7b97\u5185\u5bb9\u635f\u5931\u7136\u540e\u8fd4\u56de\u8be5\u5c42\u7684\u8f93\u5165\u3002\n\u8ba1\u7b97\u51fa\u7684\u635f\u5931\u88ab\u4fdd\u5b58\u4e3amodule\u7684\u53c2\u6570\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # we 'detach' the target content from the tree used\n        # to dynamically compute the gradient: this is a stated value,\n        # not a variable. Otherwise the forward method of the criterion\n        # will throw an error.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n   **\u91cd\u8981\u7ec6\u8282**: \u867d\u7136\u8fd9\u4e2a\u6a21\u5757\u540d\u4e3a ``ContentLoss``\uff0c\u4f46\u5b83\u4e0d\u662f\u4e00\u4e2a\u771f\u6b63\u7684PyTorch\u635f\u5931\u51fd\u6570\u3002\n   \u5982\u679c\u8981\u5c06\u5185\u5bb9\u635f\u5931\u5b9a\u4e49\u4e3aPyTorch\u635f\u5931\u51fd\u6570\uff0c\u5219\u5fc5\u987b\u521b\u5efaPyTorch\u81ea\u52a8\u68af\u5ea6\u51fd\u6570\uff0c\u4ee5\u4fbf\u5728 \n   ``backward`` \u65b9\u6cd5\u4e2d\u624b\u52a8\u8ba1\u7b97/\u5b9e\u73b0\u68af\u5ea6\u3002\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u98ce\u683c\u635f\u5931(Style Loss)\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe style loss module \u7684\u5b9e\u73b0\u4e0e content loss module \u7684\u5b9e\u73b0\u7c7b\u4f3c\u3002\n\u5b83\u5728\u7f51\u7edc\u4e2d\u4f5c\u4e3a\u4e00\u4e2a\u900f\u660e\u7684\u5c42\u53bb\u8ba1\u7b97\u8be5\u5c42\u7684\u98ce\u683c\u635f\u5931\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97 gram \u77e9\u9635 $G_{XL}$ \u3002\ngram \u77e9\u9635\u662f\u7ed9\u5b9a\u77e9\u9635\u548c\u8be5\u77e9\u9635\u7684\u8f6c\u7f6e\u76f8\u4e58\u7684\u7ed3\u679c\u3002\u5728\u8fd9\u4e2a\u5e94\u7528\u4e2d\uff0c\u7ed9\u5b9a\u7684\u77e9\u9635\u662f\n\u5c42 $L$ \u7684\u7279\u5f81\u56fe $F_{XL}$ \u7684 reshaped \u7248\u672c\u3002\n$F_{XL}$ \u88ab reshape \u6765\u5f62\u6210 $\\hat{F}_{XL}$, \u4e00\u4e2a $K$\\ x\\ $N$\n\u77e9\u9635, \u5176\u4e2d $K$ \u662f\u5c42 $L$ \u7684\u7279\u5f81\u56fe\u7684\u6570\u91cf\uff0c\u800c $N$ \u662f\u4efb\u610f\u5411\u91cf\u5316\u7684\u7279\u5f81\u56fe \n$F_{XL}^k$ \u7684\u957f\u5ea6\u3002\u6bd4\u5982\u8bf4\uff0c$\\hat{F}_{XL}$ \u7684\u7b2c\u4e00\u884c\u5bf9\u5e94\u4e8e\u7b2c\u4e00\u4e2a\n\u5411\u91cf\u5316\u7684\u7279\u5f81\u56fe $F_{XL}^1$ \u3002\n\n\u6700\u7ec8, gram \u77e9\u9635\u5fc5\u987b\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5143\u7d20\u9664\u4ee5\u77e9\u9635\u4e2d\u7684\u5143\u7d20\u603b\u6570\u6765\u6807\u51c6\u5316\u3002\u8fd9\u79cd\u5f52\u4e00\u5316\u662f\u4e3a\u4e86\u62b5\u6d88\n\u5177\u6709\u5927 $N$ \u7ef4\u6570\u7684 $\\hat{F}_{XL}$ \u77e9\u9635\u5728gram\u77e9\u9635\u4e2d\u4ea7\u751f\u8f83\u5927\u503c\u7684\u4e8b\u5b9e\u3002\n\u8fd9\u79cd\u7279\u522b\u5927\u7684\u503c\u5c06\u4f1a\u5f15\u8d77\u524d\u9762\u7684\u5c42(\u5728\u6c60\u5316\u5c42\u4e4b\u524d\u7684\u5c42)\u5728\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u91cd\u8981\u7684\u5f71\u54cd\u3002\nStyle features \u503e\u5411\u4e8e\u5728\u7f51\u7edc\u4e2d\u66f4\u6df1\u7684\u5c42\uff0c\u6240\u4ee5\u8fd9\u4e2a\u5f52\u4e00\u5316\u6b65\u9aa4\u6781\u5176\u91cd\u8981\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    # b=number of feature maps\n    # (c,d)=dimensions of a f. map (N=c*d)\n\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\n    G = torch.mm(features, features.t())  # compute the gram product\n\n    # we 'normalize' the values of the gram matrix\n    # by dividing by the number of element in each feature maps.\n    return G.div(a * b * c * d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728\uff0cstyle loss module \u770b\u8d77\u6765\u4e0econtent loss module \u51e0\u4e4e\u5b8c\u5168\u4e00\u6837\u3002\n\u4f7f\u7528 $G_{XL}$ \u548c $G_{SL}$ \u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u8ba1\u7b97style distance\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5bfc\u5165\u6a21\u578b\n-------------------\n\n\u73b0\u5728\u6211\u4eec\u9700\u8981\u5f15\u8fdb\u4e00\u4e2a\u9884\u5148\u8bad\u7ec3\u8fc7\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u5c06\u4f7f\u752819\u5c42VGG\u7f51\u7edc\uff0c\u5c31\u50cf\u8bba\u6587\u4e2d\u4f7f\u7528\u7684\u90a3\u6837\u3002\n\nPyTorch\u5b9e\u73b0\u7684VGG\u662f\u4e00\u4e2a\u6a21\u5757(module)\uff0c\u5206\u4e3a\u4e24\u4e2a\u5b50 ``Sequential`` \u6a21\u5757\uff1a\n``features`` (\u5305\u542b\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42)\u548c ``classifier`` (\u5305\u542b\u5b8c\u5168\u8fde\u63a5\u7684\u5c42)\u3002\n\u6211\u4eec\u5c06\u4f7f\u7528``features`` module\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981 \u4e2a\u522b\u5377\u79ef\u5c42\u7684\u8f93\u51fa \u4ee5\u6d4b\u91cf\u5185\u5bb9\u635f\u5931\u548c\u98ce\u683c\u635f\u5931\u3002\n\u6709\u4e9b\u5c42\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u4e0e\u8bc4\u4f30\u4e0d\u540c\uff0c\u56e0\u6b64\u6211\u4eec\u5fc5\u987b\u4f7f\u7528 ``.eval()`` \u5c06\u7f51\u7edc\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u53e6\u5916, VGG \u7f51\u7edc \u662f\u5728\u6bcf\u4e2a\u901a\u9053\u88ab\u5747\u503c\u4e3a mean=[0.485, 0.456, 0.406] \u548c \nstd=[0.229, 0.224, 0.225] \u6240\u89c4\u8303\u5316\u7684\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u3002\n\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u4eec\u5f52\u4e00\u5316\u56fe\u50cf\uff0c\u7136\u540e\u628a\u5f52\u4e00\u5316\u56fe\u50cf\u9001\u7ed9\u7f51\u7edc\u5904\u7406\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# \u521b\u5efa\u4e00\u4e2amodule\u53bb\u5f52\u4e00\u5316\u8f93\u5165\u56fe\u50cf\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u6ef4\u5c06\u5b83\u4eec\u9001\u7ed9 nn.Sequential \u3002\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view the mean and std to make them [C x 1 x 1] so that they can\n        # directly work with image Tensor of shape [B x C x H x W].\n        # B is batch size. C is number of channels. H is height and W is width.\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # normalize img\n        return (img - self.mean) / self.std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``Sequential`` module \u5305\u542b\u4e00\u4e2a\u7531child modules\u6784\u6210\u7684\u6709\u5e8f\u7684list\u3002\n\u6bd4\u5982, ``vgg19.features`` \u5305\u542b\u4e00\u4e2a\u5e8f\u5217 (Conv2d, ReLU, MaxPool2d,\nConv2d, ReLU\u2026) aligned in the right order of depth. \n\u6211\u4eec\u9700\u8981\u5728\u4ed6\u4eec\u68c0\u6d4b\u5230\u7684\u5377\u79ef\u5c42\u4e4b\u540e\u7acb\u5373\u6dfb\u52a0\u5185\u5bb9\u635f\u5931\u5c42\u548c\u98ce\u683c\u635f\u5931\u5c42\u3002\n\u4e3a\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u5185\u5bb9\u635f\u5931\u6a21\u5757\u548c\u98ce\u683c\u635f\u5931\u6a21\u5757\u88ab\u6b63\u786e\u63d2\u5165\u7684\n\u65b0 ``Sequential`` \u6a21\u5757\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u8ba1\u7b97 style/content losses \u6240\u9700\u8981\u7684\u6df1\u5ea6\u7684\u5c42:\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    cnn = copy.deepcopy(cnn)\n\n    # normalization module\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # just in order to have an iterable access to or list of content/syle\n    # losses\n    content_losses = []\n    style_losses = []\n\n    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n    # to put in modules that are supposed to be activated sequentially\n    model = nn.Sequential(normalization)\n\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # The in-place version doesn't play very nicely with the ContentLoss\n            # and StyleLoss we insert below. So we replace with out-of-place\n            # ones here.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # add content loss:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # add style loss:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # now we trim off the layers after the last content and style losses\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u63a5\u4e0b\u6765\u6211\u4eec\u9009\u62e9\u8f93\u5165\u56fe\u50cf\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528\u5185\u5bb9\u56fe\u50cf\u7684\u526f\u672c\u6216\u8005\u4e00\u526f\u767d\u566a\u58f0\u56fe\u50cf \u4f5c\u4e3a\u8f93\u5165\u56fe\u50cf\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# input_img = content_img.clone()\n# \u5982\u679c\u4f60\u60f3\u4f7f\u7528\u767d\u566a\u58f0\uff0c\u5c31\u53bb\u6389\u4e0b\u9762\u8fd9\u884c\u4ee3\u7801\u7684\u6ce8\u91ca:\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# \u628a\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u52a0\u5165\u5230 figure \u4e2d:\nplt.figure()\nimshow(input_img, title='Input Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u68af\u5ea6\u4e0b\u964d\n----------------\n\n\u8be5\u7b97\u6cd5\u7684\u4f5c\u8005 `\u5efa\u8bae <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__, \n\u6211\u4eec\u4f7f\u7528 L-BFGS \u7b97\u6cd5\u6765\u8fd0\u884c\u68af\u5ea6\u4e0b\u964d\u3002\u4e0d\u50cf\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\uff0c\u6211\u4eec\u60f3\u8981\u8bad\u7ec3\u7684\u662f\u8f93\u5165\u56fe\u50cf\u4ee5\u4fbf\u6700\u5c0f\u5316 content/style losses\u3002\n\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a PyTorch L-BFGS \u4f18\u5316\u5668 ``optim.LBFGS`` \u5e76\u628a\u6211\u4eec\u7684\u56fe\u50cf\u4f20\u9012\u7ed9\u5b83\u4f5c\u4e3a\u8981\u88ab\u4f18\u5316\u7684\u5f20\u91cf\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_input_optimizer(input_img):\n    # this line to show that input is a parameter that requires a gradient\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6700\u540e\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u4e00\u4e2a\u6267\u884cneural transfer\u7684\u51fd\u6570\u3002\u5bf9\u4e8e\u7f51\u7edc\u7684\u6bcf\u4e00\u6b21\u8fed\u4ee3\uff0c\n\u5b83\u90fd\u5f97\u5230\u4e00\u4e2a\u66f4\u65b0\u7684\u8f93\u5165\uff0c\u5e76\u8ba1\u7b97\u65b0\u7684\u635f\u5931\u3002\n\u6211\u4eec\u5c06\u8fd0\u884c\u6bcf\u4e2aloss module\u7684 ``backward`` \u65b9\u6cd5\u6765\u52a8\u6001\u5730\u8ba1\u7b97\u5b83\u4eec\u7684\u68af\u5ea6\u3002\n\u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a \u201cclosure\u201d \u51fd\u6570\uff0c\u5b83\u91cd\u65b0\u8bc4\u4f30\u6a21\u5757\u5e76\u8fd4\u56de\u635f\u5931\u3002\n\n\u6211\u4eec\u8fd8\u6709\u6700\u540e\u4e00\u4e2a\u5236\u7ea6\u56e0\u7d20\u8981\u89e3\u51b3\u3002\u7f51\u7edc\u53ef\u4ee5\u5c1d\u8bd5\u4f18\u5316\u8f93\u5165\u503c\uff0c\u5176\u503c\u8d85\u8fc7\u56fe\u50cf\u76840\u52301\u5f20\u91cf\u8303\u56f4\u3002\n\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6bcf\u6b21\u7f51\u7edc\u8fd0\u884c\u65f6\u5c06\u8f93\u5165\u503c\u4fee\u6b63\u4e3a0\u52301\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=600,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # correct the values of updated input image\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6700\u540e, \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u7b97\u6cd5\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nimshow(output, title='Output Image')\n\n# sphinx_gallery_thumbnail_number = 4\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}