{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5355\u8bcd\u7ea7\u8bed\u8a00\u6a21\u578bRNN\n=====================================\n**\u6559\u7a0b\u4f5c\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n\u672c\u6559\u7a0b\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8bad\u7ec3\u591a\u5c42RNN(Elman\u3001GRU\u6216LSTM)\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u811a\u672c\u4f7f\u7528\u63d0\u4f9b\u7684Wikitext-2\u6570\u636e\u96c6\u3002\n\u7136\u540e\uff0c\u751f\u6210\u811a\u672c(generate script)\u53ef\u4ee5\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u751f\u6210\u65b0\u6587\u672c\u3002\n\n\u8be5\u6a21\u578b\u4f7f\u7528 `nn.RNN` \u6a21\u5757(\u53ca\u5176\u59ca\u59b9\u6a21\u5757 `nn.GRU` \u548c `nn.LSTM` )\uff0c\u5982\u679c\u5728\u5b89\u88c5\u4e86cuDNN\u7684CUDA\u4e0a\u8fd0\u884c\uff0c\n\u8be5\u6a21\u5757\u5c06\u81ea\u52a8\u4f7f\u7528cuDNN\u540e\u7aef\u3002\n\n\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u5982\u679c\u63a5\u6536\u5230\u952e\u76d8\u4e2d\u65ad(Ctrl-C)\uff0c\u5219\u505c\u6b62\u8bad\u7ec3\uff0c\u5e76\u6839\u636e\u6d4b\u8bd5\u6570\u636e\u96c6\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u6570\u636e\u96c6\uff1a\u8bed\u6599\u5e93\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom io import open\nimport torch\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        \"\"\"Tokenizes a text file.\"\"\"\n        print(path)\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u7f51\u7edc\u6a21\u578b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\nclass RNNModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if rnn_type in ['LSTM', 'GRU']:\n            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n        else:\n            try:\n                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n            except KeyError:\n                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        if self.rnn_type == 'LSTM':\n            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n        else:\n            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u53c2\u6570\u89e3\u6790\u5668\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport time\nimport math\n\nparser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\nparser.add_argument('--data', type=str, default='./data/wikitext-2',\n                    help='location of the data corpus')\nparser.add_argument('--model', type=str, default='LSTM',\n                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\nparser.add_argument('--emsize', type=int, default=200,\n                    help='size of word embeddings')\nparser.add_argument('--nhid', type=int, default=200,\n                    help='number of hidden units per layer')\nparser.add_argument('--nlayers', type=int, default=2,\n                    help='number of layers')\nparser.add_argument('--lr', type=float, default=20,\n                    help='initial learning rate')\nparser.add_argument('--clip', type=float, default=0.25,\n                    help='gradient clipping')\nparser.add_argument('--epochs', type=int, default=40,\n                    help='upper epoch limit')\nparser.add_argument('--batch_size', type=int, default=20, metavar='N',\n                    help='batch size')\nparser.add_argument('--bptt', type=int, default=35,\n                    help='sequence length')\nparser.add_argument('--dropout', type=float, default=0.2,\n                    help='dropout applied to layers (0 = no dropout)')\nparser.add_argument('--tied', action='store_true',\n                    help='tie the word embedding and softmax weights')\nparser.add_argument('--seed', type=int, default=1111,\n                    help='random seed')\nparser.add_argument('--cuda', action='store_true',\n                    help='use CUDA', default=True)\nparser.add_argument('--log-interval', type=int, default=200, metavar='N',\n                    help='report interval')\nparser.add_argument('--save', type=str, default='model.pt',\n                    help='path to save the final model')\nparser.add_argument('--onnx-export', type=str, default='',\n                    help='path to export the final model in onnx format')\n\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d\u6570\u636e\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corpus = Corpus(args.data)\n\n# Starting from sequential data, batchify arranges the dataset into columns.\n# For instance, with the alphabet as the sequence and batch size 4, we'd get\n# \u250c a g m s \u2510\n# \u2502 b h n t \u2502\n# \u2502 c i o u \u2502\n# \u2502 d j p v \u2502\n# \u2502 e k q w \u2502\n# \u2514 f l r x \u2518.\n# These columns are treated as independent by the model, which means that the\n# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n# batch processing.\n\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\neval_batch_size = 10\ntrain_data = batchify(corpus.train, args.batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6784\u5efa\u6a21\u578b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = len(corpus.dictionary)\nmodel = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n\ncriterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u6a21\u578b\n-----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\n# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n# \u250c a g m s \u2510 \u250c b h n t \u2510\n# \u2514 b h n t \u2518 \u2514 c i o u \u2518\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\n\ndef get_batch(source, i):\n    seq_len = min(args.bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len].to(device)\n    target = source[i+1:i+1+seq_len].view(-1).to(device)\n    return data, target\n\n\ndef evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(eval_batch_size)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, args.bptt):\n            data, targets = get_batch(data_source, i)\n            output, hidden = model(data, hidden)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n            hidden = repackage_hidden(hidden)\n    return total_loss / (len(data_source) - 1)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(args.batch_size)\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n        data, targets = get_batch(train_data, i)\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        hidden = repackage_hidden(hidden)\n        model.zero_grad()\n        output, hidden = model(data, hidden)\n        loss = criterion(output.view(-1, ntokens), targets)\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        for p in model.parameters():\n            p.data.add_(-lr, p.grad.data)\n\n        total_loss += loss.item()\n\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss / args.log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n                    'loss {:5.2f} | ppl {:8.2f}'.format(\n                epoch, batch, len(train_data) // args.bptt, lr,\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\n\ndef export_onnx(path, batch_size, seq_len):\n    print('The model is also exported in ONNX format at {}'.\n          format(os.path.realpath(args.onnx_export)))\n    model.eval()\n    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n    hidden = model.init_hidden(batch_size)\n    torch.onnx.export(model, (dummy_input, hidden), path)\n\nimport copy\n\n# Loop over epochs.\nlr = args.lr\nbest_val_loss = None\nbest_model = None\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    for epoch in range(1, args.epochs+1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n        print('-' * 89)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                           val_loss, math.exp(val_loss)))\n        print('-' * 89)\n        # Save the model if the validation loss is the best we've seen so far.\n        if not best_val_loss or val_loss < best_val_loss:\n            best_model = copy.deepcopy(model.cpu())\n            best_val_loss = val_loss\n            # with open(args.save, 'wb') as f:\n                # torch.save(model, f)\n        else:\n            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n            lr /= 4.0\nexcept KeyboardInterrupt:\n    print('-' * 89)\n    print('Exiting from training early')\n\n# Load the best saved model.\nwith open(args.save, 'rb') as f:\n    # model = torch.load(f)\n    model = best_model.to(device)\n    # after load the rnn params are not a continuous chunk of memory\n    # this makes them a continuous chunk, and will speed up forward pass\n    model.rnn.flatten_parameters()\n\n# Run on test data.\ntest_loss = evaluate(test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)\n\nif len(args.onnx_export) > 0:\n    # Export the model in ONNX format.\n    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}