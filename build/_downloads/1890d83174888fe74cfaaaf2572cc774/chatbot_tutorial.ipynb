{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b\n================\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`_\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u4e00\u4e2a\u6709\u8da3\u800c\u6709\u610f\u601d\u7684\u7528\u4f8b-\u9012\u5f52\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b(recurrent sequence-to-sequence models)\u7684\u7528\u4f8b\u3002\n\u6211\u4eec\u5c06\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u804a\u5929\u673a\u5668\u4eba\u4f7f\u7528\u5eb7\u5948\u5c14\u7535\u5f71\u5267\u672c-\u5bf9\u8bdd\u8bed\u6599\u5e93\n(`Cornell Movie-Dialogs Corpus <https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__)\u3002\n\n\u4f1a\u8bdd\u6a21\u578b(Conversational models)\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u4e00\u4e2a\u70ed\u70b9\u3002\u804a\u5929\u673a\u5668\u4eba(Chatbots)\u53ef\u4ee5\u5728\u591a\u79cd\u573a\u666f\u4e2d\u627e\u5230\uff0c\u5305\u62ec\u5ba2\u6237\u670d\u52a1\u5e94\u7528\u7a0b\u5e8f\u548c\u5728\u7ebf\u5e2e\u52a9\u53f0\u3002\n\u8fd9\u4e9b\u673a\u5668\u4eba\u901a\u5e38\u7531\u57fa\u4e8e\u68c0\u7d22\u7684\u6a21\u578b(retrieval-based models)\u8fdb\u884c\u9a71\u52a8\uff0c\u8fd9\u4e9b\u6a21\u578b\u8f93\u51fa\u5bf9\u67d0\u4e9b\u5f62\u5f0f\u7684\u95ee\u9898\u7684\u9884\u5b9a\u4e49\u7684\u56de\u7b54\u3002\n\u5728\u4e00\u4e2a\u9ad8\u5ea6\u53d7\u9650\u7684\u9886\u57df\uff0c\u6bd4\u5982\u4e00\u4e2a\u516c\u53f8\u7684IT\u670d\u52a1\u53f0\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5df2\u7ecf\u8db3\u591f\u4e86\uff0c\u4f46\u662f\u5bf9\u4e8e\u66f4\u4e00\u822c\u7684\u7528\u4f8b\u6765\u8bf4\uff0c\u5b83\u4eec\u8fd8\u4e0d\u591f\u5065\u58ee\u3002\n\u6559\u4e00\u53f0\u673a\u5668\u5728\u591a\u4e2a\u9886\u57df\u4e0e\u4eba\u8fdb\u884c\u6709\u610f\u4e49\u7684\u5bf9\u8bdd\u662f\u4e00\u4e2a\u8fdc\u672a\u89e3\u51b3\u7684\u7814\u7a76\u95ee\u9898\u3002\u6700\u8fd1\uff0c\u6df1\u5ea6\u5b66\u4e60\u7684\u70ed\u6f6e\u5df2\u7ecf\u5141\u8bb8\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\n\u5982\u8c37\u6b4c\u7684\u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b(`Neural Conversational Model <https://arxiv.org/abs/1506.05869>`__)\uff0c\n\u8fd9\u6807\u5fd7\u7740\u5411\u591a\u57df\u751f\u6210\u4f1a\u8bdd\u6a21\u578b(multi-domain generative conversational models)\u8fc8\u51fa\u4e86\u4e00\u5927\u6b65\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5728PyTorch\u4e2d\u5b9e\u73b0\u8fd9\u79cd\u6a21\u578b\u3002\n\n.. figure:: /_static/img/chatbot/bot.png\n   :align: center\n   :alt: bot\n\n.. code:: python\n\n  > hello?\n  Bot: hello .\n  > where am I?\n  Bot: you re in a hospital .\n  > who are you?\n  Bot: i m a lawyer .\n  > how are you doing?\n  Bot: i m fine .\n  > are you my friend?\n  Bot: no .\n  > you're under arrest\n  Bot: i m trying to help you !\n  > i'm just kidding\n  Bot: i m sorry .\n  > where are you from?\n  Bot: san francisco .\n  > it's time for me to leave\n  Bot: i know .\n  > goodbye\n  Bot: goodbye .\n\n**\u672c\u6559\u7a0b\u4eae\u70b9**\n\n-  \u52a0\u8f7d\u548c\u9884\u5904\u7406 `Cornell Movie-Dialogs Corpus <https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__\n   \u6570\u636e\u96c6\n-  \u5b9e\u73b0\u4e00\u4e2a\u5e26\u6709 `Luong \u6ce8\u610f\u529b\u673a\u5236(s) <https://arxiv.org/abs/1508.04025>`__ \u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b(sequence-to-sequence model)\n-  \u4f7f\u7528 mini-batches \u8054\u5408\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b(encoder and decoder models)\n-  \u5b9e\u73b0\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u6a21\u5757\n-  \u4e0e\u8bad\u7ec3\u597d\u7684chatbot\u8fdb\u884c\u4ea4\u4e92\n\n**\u9e23\u8c22**\n\n\u672c\u6559\u7a0b\u501f\u7528\u4e0b\u5217\u6765\u6e90\u7684\u4ee3\u7801:\n\n1) Yuan-Kuei Wu\u2019s pytorch-chatbot implementation:\n   https://github.com/ywk991112/pytorch-chatbot\n\n2) Sean Robertson\u2019s practical-pytorch seq2seq-translation example:\n   https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n\n3) FloydHub\u2019s Cornell Movie Corpus preprocessing code:\n   https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u9884\u5907\u5de5\u4f5c\n------------\n\n\u9996\u5148\uff0c\u4ece `\u8fd9\u91cc <https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__ \u4e0b\u8f7d\u6570\u636e\u7684ZIP\u6587\u4ef6\uff0c\n\u5e76\u5c06\u5176\u653e\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u7684 ``data/`` \u76ee\u5f55\u4e2d\u3002\n\n\u4e4b\u540e\uff0c\u6211\u4eec\u8981\u5bfc\u5165\u4e00\u4e9b\u5fc5\u8981\u7684\u5305\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\n\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d & \u9884\u5904\u7406\u6570\u636e\n----------------------\n\n\u4e0b\u4e00\u6b65\u662f\u91cd\u65b0\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6\uff0c\u5e76\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7684\u7ed3\u6784\u4e2d\u3002\n\n`Cornell \u7535\u5f71\u5bf9\u8bdd\u8bed\u6599\u5e93 <https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__\n\u662f\u4e00\u4e2a\u7535\u5f71\u4eba\u7269\u5bf9\u8bdd\u7684\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3002\n\n-  10,292 \u5bf9\u7535\u5f71\u89d2\u8272\u4e4b\u95f4\u7684220\uff0c579\u6b21\u4f1a\u8bdd\u4ea4\u6d41\n-  \u6765\u81ea 617 \u90e8\u7535\u5f71\u76849,035\u4e2a\u4eba\u7269\u89d2\u8272\n-  304,713 total utterances(\u8bdd\u8bed)\n\n\u8be5\u6570\u636e\u96c6\u5e9e\u5927\u591a\u6837\uff0c\u8bed\u8a00\u5f62\u5f0f\u3001\u65f6\u95f4\u5468\u671f\u3001\u60c5\u611f\u7b49\u90fd\u6709\u5f88\u5927\u7684\u53d8\u5316\u3002\n\u6211\u4eec\u5e0c\u671b\u8fd9\u79cd\u591a\u6837\u6027\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u5bf9\u591a\u79cd\u5f62\u5f0f\u7684\u8f93\u5165\u548c\u67e5\u8be2\u90fd\u6709\u5f88\u5f3a\u7684\u62b5\u6297\u529b\u3002\n\n\u9996\u5148\uff0c\u6211\u4eec\u5c06\u67e5\u770b\u6570\u636e\u6587\u4ef6\u7684\u4e00\u4e9b\u884c\uff0c\u4ee5\u67e5\u770b\u539f\u59cb\u683c\u5f0f\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corpus_name = \"cornell movie-dialogs corpus\"\ncorpus = os.path.join(\"./data\", corpus_name)\n\ndef printLines(file, n=10):\n    with open(file, 'rb') as datafile:\n        lines = datafile.readlines()\n    for line in lines[:n]:\n        print(line)\n\nprintLines(os.path.join(corpus, \"movie_lines.txt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u683c\u5f0f\u826f\u597d\u7684\u6570\u636e\u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u4e00\u4e2atab\u5206\u9694\n\u7684\u67e5\u8be2\u8bed\u53e5(*query sentence*)\u548c\u4e00\u4e2a\u54cd\u5e94\u8bed\u53e5\u5bf9(*response sentence*)\u3002\n\n\u4ee5\u4e0b\u51fd\u6570\u6709\u52a9\u4e8e\u89e3\u6790\u539f\u59cb\u7684 *movie_lines.txt* \u6570\u636e\u6587\u4ef6\u3002\n\n-  ``loadLines`` \uff1a\u5c06\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u62c6\u5206\u4e3a\u5b57\u6bb5\u5b57\u5178(lineID, characterID, movieID, character, text)\u3002\n-  ``loadConversations`` \uff1a\u57fa\u4e8e *movie_conversations.txt* \u628a\u4ece ``loadLines`` \u5f97\u5230\u7684lines\u7684\u5b57\u6bb5\u5206\u7ec4\u4e3a\u56de\u8bdd\n-  ``extractSentencePairs`` \u4ece\u4f1a\u8bdd\u4e2d\u62bd\u53d6\u53e5\u5b50\u5bf9\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u5c06\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u62c6\u5206\u4e3a\u5b57\u6bb5\u5b57\u5178\u3002\ndef loadLines(fileName, fields):\n    lines = {}\n    with open(fileName, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            values = line.split(\" +++$+++ \")\n            # Extract fields\n            lineObj = {}\n            for i, field in enumerate(fields):\n                lineObj[field] = values[i]\n            lines[lineObj['lineID']] = lineObj\n    return lines\n\n\n# \u57fa\u4e8e *movie_conversations.txt* \u628a\u4ece ``loadLines`` \u5f97\u5230\u7684lines\u7684\u5b57\u6bb5\u5206\u7ec4\u4e3a\u56de\u8bdd\ndef loadConversations(fileName, lines, fields):\n    conversations = []\n    with open(fileName, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            values = line.split(\" +++$+++ \")\n            # Extract fields\n            convObj = {}\n            for i, field in enumerate(fields):\n                convObj[field] = values[i]\n            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n            lineIds = eval(convObj[\"utteranceIDs\"])\n            # Reassemble lines\n            convObj[\"lines\"] = []\n            for lineId in lineIds:\n                convObj[\"lines\"].append(lines[lineId])\n            conversations.append(convObj)\n    return conversations\n\n\n# \u4ece\u4f1a\u8bdd\u4e2d\u62bd\u53d6\u53e5\u5b50\u5bf9\ndef extractSentencePairs(conversations):\n    qa_pairs = []\n    for conversation in conversations:\n        # Iterate over all the lines of the conversation\n        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n            # Filter wrong samples (if one of the lists is empty)\n            if inputLine and targetLine:\n                qa_pairs.append([inputLine, targetLine])\n    return qa_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728\u6211\u4eec\u5c06\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\u5e76\u521b\u5efa\u6587\u4ef6\u3002\u6211\u4eec\u5c06\u5176\u547d\u540d\u4e3a *formatted_movie_lines.txt* \u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u5b9a\u4e49\u6307\u5411\u65b0\u6587\u4ef6\u7684\u8def\u5f84\ndatafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n\ndelimiter = '\\t'\n# Unescape the delimiter\ndelimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n\n# \u521d\u59cb\u5316 lines dict, conversations list, and field ids\nlines = {}\nconversations = []\nMOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\nMOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n\n# \u52a0\u8f7d lines \u5e76\u5904\u7406 conversations\nprint(\"\\nProcessing corpus...\")\nlines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\nprint(\"\\nLoading conversations...\")\nconversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n\n# \u5199\u5165\u5230\u65b0\u7684 csv \u6587\u4ef6\nprint(\"\\nWriting newly formatted file...\")\nwith open(datafile, 'w', encoding='utf-8') as outputfile:\n    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n    for pair in extractSentencePairs(conversations):\n        writer.writerow(pair)\n\n# \u6253\u5370\u8f93\u51fa lines \u7684\u6837\u672c\nprint(\"\\nSample lines from file:\")\nprintLines(datafile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d \u5e76 \u88c1\u526a \u6570\u636e\n~~~~~~~~~~~~~~~~~~\n\n\u6211\u4eec\u4e0b\u4e00\u6b65\u7684\u5de5\u4f5c\u662f\u521b\u5efa\u4e00\u4e2a\u8bcd\u6c47\u8868\uff0c\u5e76\u5c06 \u67e5\u8be2/\u54cd\u5e94\u8bed\u53e5\u5bf9 \u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002\n\n\u6ce8\u610f\uff0c\u6211\u4eec\u5904\u7406\u7684\u662f **words** \u5e8f\u5217\uff0c\u5b83\u4eec\u6ca1\u6709\u9690\u5f0f\u6620\u5c04\u5230\u79bb\u6563\u7684\u6570\u503c\u7a7a\u95f4\u3002\n\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u901a\u8fc7\u5c06\u6211\u4eec\u5728DataSet\u4e2d\u9047\u5230\u7684\u6bcf\u4e2a\u552f\u4e00\u5355\u8bcd\u6620\u5c04\u5230\u4e00\u4e2a\u7d22\u5f15\u503c\u6765\u521b\u5efa\u4e00\u4e2a\u7d22\u5f15\u3002\n\n\u4e3a\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a ``Voc`` \u7c7b\uff0c\u5b83\u4fdd\u6301\u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u3001\u7d22\u5f15\u5230\u5355\u8bcd\u7684\u53cd\u5411\u6620\u5c04\u3001\u6bcf\u4e2a\u5355\u8bcd\u7684\u8ba1\u6570\u548c\u603b\u5355\u8bcd\u8ba1\u6570\u3002\n\u8be5\u7c7b\u63d0\u4f9b\u4e86\u5c06\u5355\u8bcd\u6dfb\u52a0\u5230\u8bcd\u6c47\u8868(``addWord``)\u3001\u5728\u53e5\u5b50\u4e2d\u6dfb\u52a0\u6240\u6709\u5355\u8bcd(``addSentence``)\n\u548c\u4fee\u526a\u5c11\u89c1\u5355\u8bcd(``trim``)\u7684\u65b9\u6cd5\u3002\u7a0d\u540e\u66f4\u591a\u5173\u4e8e\u4fee\u526a\u7684\u5185\u5bb9\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u9ed8\u8ba4\u7684\u5355\u8bcd\u6807\u8bb0(word tokens)\nPAD_token = 0  # Used for padding short sentences\nSOS_token = 1  # \u53e5\u5b50\u8d77\u59cb\u7684 token\nEOS_token = 2  # \u53e5\u5b50\u7ed3\u675f\u7684 token\n\nclass Voc:\n    def __init__(self, name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3  # Count SOS, EOS, PAD\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            self.word2count[word] += 1\n\n    # \u79fb\u9664\u90a3\u4e9b\u51fa\u73b0\u6b21\u6570\u4f4e\u4e8e\u67d0\u4e2a\u9608\u503c\u7684\u5355\u8bcd\n    def trim(self, min_count):\n        if self.trimmed:\n            return\n        self.trimmed = True\n\n        keep_words = []\n\n        for k, v in self.word2count.items():\n            if v >= min_count:\n                keep_words.append(k)\n\n        print('keep_words {} / {} = {:.4f}'.format(\n            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n        ))\n\n        # Reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count default tokens\n\n        for word in keep_words:\n            self.addWord(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u7ec4\u88c5\u6211\u4eec\u7684\u8bcd\u6c47\u8868\u548c\u67e5\u8be2/\u54cd\u5e94\u53e5\u5b50\u5bf9\u3002\u5728\u51c6\u5907\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u4e4b\u524d\uff0c\n\u6211\u4eec\u5fc5\u987b\u6267\u884c\u4e00\u4e9b\u9884\u5904\u7406\u3002\n\n\u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u4f7f\u7528 ``unicodeToAscii`` \u5c06Unicode\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aASCII\u3002\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5e94\u8be5\u5c06\u6240\u6709\u5b57\u6bcd\u8f6c\u6362\u4e3a\u5c0f\u5199\uff0c\u5e76\u4fee\u526a\u9664\u57fa\u672c\u6807\u70b9\u7b26\u53f7(``normalizeString``)\u4ee5\u5916\u7684\u6240\u6709\u975e\u5b57\u6bcd\u5b57\u7b26\u3002\n\u6700\u540e\uff0c\u4e3a\u4e86\u5e2e\u52a9\u8bad\u7ec3\u6536\u655b\uff0c\u6211\u4eec\u5c06\u8fc7\u6ee4\u51fa\u957f\u5ea6\u5927\u4e8e ``MAX_LENGTH`` \u9608\u503c\u7684\u53e5\u5b50(``filterPairs``)\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n\n# Turn a Unicode string to plain ASCII, thanks to\n# http://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s\n\n# Read query/response pairs and return a voc object\ndef readVocs(datafile, corpus_name):\n    print(\"Reading lines...\")\n    # Read the file and split into lines\n    lines = open(datafile, encoding='utf-8').\\\n        read().strip().split('\\n')\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n    voc = Voc(corpus_name)\n    return voc, pairs\n\n# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\ndef filterPair(p):\n    # Input sequences need to preserve the last word for EOS token\n    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n\n# Filter pairs using filterPair condition\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\n# Using the functions defined above, return a populated voc object and pairs list\ndef loadPrepareData(corpus, corpus_name, datafile, save_dir):\n    print(\"Start preparing training data ...\")\n    voc, pairs = readVocs(datafile, corpus_name)\n    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n    print(\"Counting words...\")\n    for pair in pairs:\n        voc.addSentence(pair[0])\n        voc.addSentence(pair[1])\n    print(\"Counted words:\", voc.num_words)\n    return voc, pairs\n\n\n# Load/Assemble voc and pairs\nsave_dir = os.path.join(\"data\", \"save\")\nvoc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n# Print some pairs to validate\nprint(\"\\npairs:\")\nfor pair in pairs[:10]:\n    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u53e6\u4e00\u79cd\u6709\u5229\u4e8e\u5728\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u7684\u7b56\u7565\u662f\u5c06\u5f88\u5c11\u4f7f\u7528\u7684\u5355\u8bcd\u4ece\u6211\u4eec\u7684\u8bcd\u6c47\u8868\u4e2d\u5220\u6389\u3002\n\u51cf\u5c11\u7279\u5f81\u6570\u91cf\u4e5f\u4f1a\u964d\u4f4e\u6a21\u578b\u5fc5\u987b\u5b66\u4e60\u8fd1\u4f3c\u7684\u51fd\u6570\u7684\u96be\u5ea6\u3002\u6211\u4eec\u5c06\u91c7\u53d6\u4e24\u6b65\u884c\u52a8\uff1a\n\n1) \u4f7f\u7528\u51fd\u6570 ``voc.trim`` \u88c1\u526a\u51fa\u73b0\u6b21\u6570\u5c11\u4e8e ``MIN_COUNT`` \u9608\u503c\u7684\u5355\u8bcd\u3002\n\n2) Filter out pairs with trimmed words.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MIN_COUNT = 3    # \u7528\u4e8e\u4fee\u526a\u7684\u6700\u5c0f\u5355\u8bcd\u91cf\u9608\u503c\n\ndef trimRareWords(voc, pairs, MIN_COUNT):\n    # Trim words used under the MIN_COUNT from the voc\n    voc.trim(MIN_COUNT)\n    # Filter out pairs with trimmed words\n    keep_pairs = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        output_sentence = pair[1]\n        keep_input = True\n        keep_output = True\n        # Check input sentence\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_input = False\n                break\n        # Check output sentence\n        for word in output_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_output = False\n                break\n\n        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n        if keep_input and keep_output:\n            keep_pairs.append(pair)\n\n    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n    return keep_pairs\n\n\n# Trim voc and pairs\npairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e\n-----------------------\n\n\u867d\u7136\u6211\u4eec\u5df2\u7ecf\u4ed8\u51fa\u4e86\u5f88\u5927\u7684\u52aa\u529b\u6765\u51c6\u5907\u548c\u88c5\u9970\u6211\u4eec\u7684\u6570\u636e\u5230\u4e00\u4e2a\u5f88\u597d\u7684\u8bcd\u6c47\u8868\u5bf9\u8c61\u548c\u53e5\u5b50\u5bf9\u5217\u8868\uff0c\n\u6211\u4eec\u7684\u6a21\u578b\u6700\u7ec8\u671f\u671b\u7684\u662f\u6570\u503c\u578b\u7684torch tensors\u4f5c\u4e3a\u8f93\u5165\u3002\n\u4e00\u79cd\u4e3a\u6a21\u578b\u51c6\u5907\u5904\u7406\u6570\u636e\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728(`seq2seq translation tutorial \n<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__)\u4e2d\u627e\u5230\u3002\n\u5728\u8be5\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u7684 batch size\u662f1\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f\u5c06\u53e5\u5b50\u5bf9\u4e2d\u7684\u5355\u8bcd\n\u4ece\u8bcd\u6c47\u8868\u4e2d\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u7d22\u5f15\uff0c\u5e76\u5c06\u5176\u63d0\u4f9b\u7ed9\u6a21\u578b\u3002\n\n\u7136\u800c\uff0c\u5982\u679c\u60a8\u6709\u5174\u8da3\u52a0\u5feb\u8bad\u7ec3\u548c/\u6216\u5e0c\u671b\u5229\u7528GPU\u5e76\u884c\u5316\u529f\u80fd\uff0c\u60a8\u5c06\u9700\u8981\u8bad\u7ec3 mini-batches \u3002\n\n\u4f7f\u7528mini-batches\u4e5f\u610f\u5473\u7740\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u6279\u6b21\u4e2d\u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u3002\u4e3a\u4e86\u5728\u540c\u4e00\u6279\u6b21\u4e2d\u5bb9\u7eb3\u4e0d\u540c\u5927\u5c0f\u7684\u53e5\u5b50\uff0c\n\u6211\u4eec\u5c06\u628a\u6279\u91cf\u8f93\u5165\u7684\u5f20\u91cf\u7684shape\u90fd\u53d8\u6210 *(max_length, batch_size)*\uff0c\n\u90a3\u4e9b\u5c0f\u4e8e *max_length* \u7684\u53e5\u5b50\u5c06\u5728 *EOS_Token* \u4e4b\u540e\u8fdb\u884c\u96f6\u586b\u5145\u8865\u5168\u3002\n\n\u5982\u679c\u6211\u4eec\u7b80\u5355\u5730 \u901a\u8fc7\u628a\u5355\u8bcd\u8f6c\u6362\u4e3a\u7d22\u5f15(\\ ``indexesFromSentence``) \u548czero-pad \u5c06\u6211\u4eec\u7684\u82f1\u8bed\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\n\u90a3\u4e48\u6211\u4eec\u7684\u5f20\u91cf\u7684shape\u4e3a *(batch_size, max_length)* \uff0c\u7d22\u5f15\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5c06\u8fd4\u56de\u5728\u6240\u6709\u65f6\u95f4\u6b65\u4e2d\u7684\u4e00\u4e2a\u5b8c\u6574\u5e8f\u5217\u3002\n\u7136\u800c\uff0c\u6211\u4eec\u9700\u8981\u80fd\u591f\u6cbf\u7740\u65f6\u95f4\u7d22\u5f15\u6211\u4eec\u7684batch\uff0c\u5e76\u8de8\u8d8abatch\u4e2d\u7684\u6240\u6709\u5e8f\u5217\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u8f93\u5165batch\u7684shape\u8f6c\u6362\n\u4e3a *(max_length, batch_size)* \uff0c\u4ee5\u4fbf\u8de8\u7b2c\u4e00\u7ef4\u7d22\u5f15\u8fd4\u56debatch\u4e2d\u6240\u6709\u53e5\u5b50\u7684\u65f6\u95f4\u6b65\u957f\u3002\n\u6211\u4eec\u5728 ``zeroPadding`` \u51fd\u6570\u4e2d\u9690\u5f0f\u5730\u5904\u7406\u8fd9\u4e2a\u8f6c\u7f6e\u3002\n\n.. figure:: /_static/img/chatbot/seq2seq_batches.png\n   :align: center\n   :alt: batches\n\n``inputVar`` \u51fd\u6570\u5904\u7406\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\u7684\u8fc7\u7a0b\uff0c\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a\u5f62\u72b6\u6b63\u786e\u7684\u96f6\u586b\u5145\u5f20\u91cf\u3002\n\u5b83\u8fd8\u8fd4\u56debatch\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684 ``lengths`` \u7684\u5f20\u91cf\uff0c\u7a0d\u540e\u5c06\u4f20\u9012\u7ed9\u6211\u4eec\u7684\u89e3\u7801\u5668\u3002\n\n``outputVar`` \u51fd\u6570\u4e0e ``inputVar`` \u51fd\u6570\u6267\u884c\u76f8\u4f3c\u7684\u529f\u80fd, \u4f46\u662f\u5b83\u8fd4\u56de\u7684\u4e0d\u662f ``lengths`` \u5f20\u91cf\uff0c\u800c\u662f\n\u4e8c\u503c\u63a9\u6a21\u5f20\u91cf(binary mask tensor) \u548c \u6700\u5927\u76ee\u6807\u53e5\u5b50\u957f\u5ea6(maximum target sentence length)\u3002\n\u4e8c\u503c\u63a9\u6a21\u5f20\u91cf\u7684shape\u4e0e\u8f93\u51fa\u76ee\u6807\u5f20\u91cf\u7684shape\u662f\u76f8\u540c\u7684\uff0c\u4f46\u91cc\u9762\u6bcf\u4e00\u4e2a\u5143\u7d20\u9664\u4e86 *PAD_token* \u662f0\u4e4b\u5916\uff0c\u5176\u4f59\u5730\u65b9\u90fd\u662f1\u3002\n\n``batch2TrainData`` \u7b80\u5355\u7684\u63a5\u6536 a bunch of pairs\uff0c\u5e76\u4f7f\u7528\u4e0a\u8ff0\u51fd\u6570\u8fd4\u56de\u8f93\u5165\u5f20\u91cf\u548c\u76ee\u6807\u5f20\u91cf\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n\n\ndef zeroPadding(l, fillvalue=PAD_token):\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n\ndef binaryMatrix(l, value=PAD_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\n# Returns padded input sequence tensor and lengths\ndef inputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n\n# Returns padded target sequence tensor, padding mask, and max target length\ndef outputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    max_target_len = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.ByteTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar, mask, max_target_len\n\n# Returns all items for a given batch of pairs\ndef batch2TrainData(voc, pair_batch):\n    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n    input_batch, output_batch = [], []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp, lengths = inputVar(input_batch, voc)\n    output, mask, max_target_len = outputVar(output_batch, voc)\n    return inp, lengths, output, mask, max_target_len\n\n\n# Example for validation\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable)\nprint(\"lengths:\", lengths)\nprint(\"target_variable:\", target_variable)\nprint(\"mask:\", mask)\nprint(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u6a21\u578b\n-------------\n\nSeq2Seq Model\n~~~~~~~~~~~~~\n\n\u6211\u4eec\u7684\u804a\u5929\u673a\u5668\u4eba\u7684\u5927\u8111\u662f\u4e00\u4e2a\u5e8f\u5217\u5230\u5e8f\u5217(Seq2seq)\u6a21\u578b\u3002seq2seq\u6a21\u578b\u7684\u76ee\u6807\u662f\u4ee5\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\n\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u6a21\u578b(fixed-sized model)\u8fd4\u56de\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u51fa\u3002\n\n`Sutskever \u7b49\u4eba <https://arxiv.org/abs/1409.3215>`__ \u53d1\u73b0\u5229\u7528\u4e24\u4e2a\u5206\u5f00\u7684\n\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(recurrent neural nets)\u53ef\u4ee5\u5b8c\u6210\u8fd9\u4e00\u4efb\u52a1\u3002\n\u4e00\u4e2aRNN\u5145\u5f53\u7f16\u7801\u5668(**encoder**)\uff0c\u5c06\u53ef\u53d8\u957f\u5ea6\u7684\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u6210\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u3002\n\u7406\u8bba\u4e0a\uff0c\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u5411\u91cf(RNN\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42)\u5c06\u5305\u542b \u8f93\u5165\u5230bot\u7684\u67e5\u8be2\u8bed\u53e5 \u7684\u8bed\u4e49\u4fe1\u606f\u3002\n\u7b2c\u4e8c\u4e2aRNN\u662f\u89e3\u7801\u5668(**decoder**)\uff0c\u5b83\u63a5\u53d7\u8f93\u5165\u5355\u8bcd\u548c\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u5e76\u8fd4\u56de\u5e8f\u5217\u4e2d\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u731c\u6d4b\u548c\n\u4e0b\u4e00\u6b21\u8fed\u4ee3\u4f7f\u7528\u7684\u9690\u85cf\u72b6\u6001\u3002\n\n.. figure:: /_static/img/chatbot/seq2seq_ts.png\n   :align: center\n   :alt: model\n\n\u56fe\u7247\u6765\u6e90:\nhttps://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7f16\u7801\u5668(Encoder)\n~~~~~~~~~~~~~~~~~\n\n\u7f16\u7801\u5668RNN\u4e00\u6b21\u8fed\u4ee3\u8f93\u5165\u53e5\u5b50\u7684\u4e00\u4e2a\u6807\u8bb0(e.g.\u00a0word)\uff0c\u5728\u6bcf\u4e00\u65f6\u95f4\u6b65\u8f93\u51fa\u4e00\u4e2a \u201coutput\u201d vector\n\u548c \u201chidden state\u201d vector\u3002\u7136\u540e\u5c06\u9690\u85cf\u72b6\u6001\u5411\u91cf\u4f20\u9012\u5230\u4e0b\u4e00\u65f6\u95f4\u6b65\uff0c\u540c\u65f6\u8bb0\u5f55\u8f93\u51fa\u5411\u91cf\u3002\n\u7f16\u7801\u5668\u5c06\u5728\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u70b9\u4e0a\u770b\u5230\u7684\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u7ec4\u70b9\uff0c\n\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e9b\u70b9\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002\n\n\u6211\u4eec\u7f16\u7801\u5668\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u7531 `CHO \u7b49\u4eba <https://arxiv.org/pdf/1406.1078v3.pdf>`__ \n2014\u5e74\u53d1\u660e\u7684\u591a\u5c42\u7684\u95e8\u63a7\u9012\u5f52\u5355\u5143(multi-layered Gated Recurrent Unit)\u3002\n\u6211\u4eec\u5c06\u4f7f\u7528GRU\u7684\u53cc\u5411\u53d8\u4f53(bidirectional variant)\uff0c\n\u8fd9\u610f\u5473\u7740\u672c\u8d28\u4e0a\u6709\u4e24\u4e2a\u72ec\u7acb\u7684RNN\uff1a\u4e00\u4e2a\u4ee5\u6b63\u5e38\u987a\u5e8f\u8f93\u5165\u5e8f\u5217\uff0c\u53e6\u4e00\u4e2a\u4ee5\u53cd\u5411\u987a\u5e8f\u8f93\u5165\u5e8f\u5217\u3002\n\u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u88ab\u6c42\u548c\u3002\u4f7f\u7528\u53cc\u5411GRU\u5c06\u4e3a\u6211\u4eec\u63d0\u4f9b\u7f16\u7801\u8fc7\u53bb\u4e0a\u4e0b\u6587\u548c\u672a\u6765\u4e0a\u4e0b\u6587\u7684\u4f18\u52bf\u3002\n\nBidirectional RNN:\n\n.. figure:: /_static/img/chatbot/RNN-bidirectional.png\n   :width: 70%\n   :align: center\n   :alt: rnn_bidir\n\n\u56fe\u7247\u6765\u6e90: http://colah.github.io/posts/2015-09-NN-Types-FP/\n\n\u6ce8\u610f\uff0c\u5d4c\u5165\u5c42(``embedding`` layer)\u7528\u4e8e\u5728\u4efb\u610f\u5927\u5c0f\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u7f16\u7801\u6211\u4eec\u7684\u5355\u8bcd\u7d22\u5f15\u3002\u5bf9\u4e8e\u6211\u4eec\u7684\u6a21\u578b\uff0c\n\u8fd9\u4e2a\u5c42\u5c06\u628a\u6bcf\u4e2a\u5355\u8bcd\u6620\u5c04\u5230\u4e00\u4e2a\u5927\u5c0f\u4e3a *hidden_size* \u7684\u7279\u5f81\u7a7a\u95f4\u3002\n\u5728\u8bad\u7ec3\u65f6\uff0c\u8fd9\u4e9b\u503c\u5e94\u8be5\u7f16\u7801\u540c\u4e49\u8bcd\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027(semantic similarity)\u3002\n\n\u6700\u540e\uff0c\u5982\u679c\u5c06\u4e00\u6279\u586b\u5145\u597d\u7684\u5e8f\u5217\u4f20\u9012\u7ed9RNN module\uff0c\u5219\u5fc5\u987b\u4f7f\u7528 \n``torch.nn.utils.rnn.pack_padded_sequence`` \u548c ``torch.nn.utils.rnn.pad_packed_sequence`` \n\u5206\u522b\u5bf9RNN\u4f20\u9012\u5468\u56f4\u7684\u586b\u5145\u8fdb\u884c\u6253\u5305(pack)\u548c\u89e3\u538b(unpack)\u3002\n\n**\u8ba1\u7b97\u56fe:**\n\n   1) \u5c06\u5355\u8bcd\u7d22\u5f15\u8f6c\u6362\u4e3a\u5d4c\u5165(Convert word indexes to embeddings)\u3002\n   2) \u4e3a RNN module \u6253\u5305\u586b\u5145\u597d\u7684\u6279\u91cf\u5e8f\u5217\n   3) \u524d\u5411\u4f20\u9012\u4f7f\u5176\u901a\u8fc7GRU.\n   4) \u53bb\u9664\u586b\u5145\u503c(Unpack padding).\n   5) \u628a\u53cc\u5411GRU\u7684\u8f93\u51fa\u52a0\u8d77\u6765\n   6) \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u540e\u7684\u9690\u85cf\u5c42\n\n**\u8f93\u5165:**\n\n-  ``input_seq``: \u8f93\u5165\u5e8f\u5217\u7684batch; shape=\\ *(max_length, batch_size)*\n-  ``input_lengths``: \u5bf9\u5e94\u4e8ebatch\u4e2d\u7684\u6bcf\u4e00\u4e2a\u53e5\u5b50\u7684\u5e8f\u5217\u957f\u5ea6\u7684\u5217\u8868; shape=\\ *(batch_size)*\n-  ``hidden``: \u9690\u85cf\u72b6\u6001; shape=\\ *(n_layers x num_directions, batch_size, hidden_size)*\n\n**\u8f93\u51fa:**\n\n-  ``outputs``: GRU\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u7279\u5f81(\u53cc\u5411\u8f93\u51fa\u4e4b\u548c); shape=\\ *(max_length, batch_size, hidden_size)*\n-  ``hidden``: \u4eceGRU\u66f4\u65b0\u7684\u9690\u85cf\u72b6\u6001; shape=\\ *(n_layers x num_directions, batch_size, hidden_size)*\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n        #   because our input size is a word embedding with number of features == hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Convert word indexes to embeddings\n        embedded = self.embedding(input_seq)\n        # Pack padded batch of sequences for RNN module\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        # Forward pass through GRU\n        outputs, hidden = self.gru(packed, hidden)\n        # Unpack padding\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        # Sum bidirectional GRU outputs\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        # Return output and final hidden state\n        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u89e3\u7801\u5668(Decoder)\n~~~~~~~~~~~~~~~~~~~~\n\n\u89e3\u7801\u5668RNN\u4ee5\u4ee4\u724c\u63a5\u4ee4\u724c(token-by-token)\u7684\u65b9\u5f0f\u751f\u6210\u54cd\u5e94\u8bed\u53e5(reponse sentance)\u3002\n\u5b83\u4f7f\u7528\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6765\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\n\u5b83\u8fde\u7eed\u751f\u6210\u5355\u8bcd\uff0c\u76f4\u5230\u8f93\u51fa *EOS_token* \uff0c\u8868\u793a\u53e5\u5b50\u7684\u7ed3\u5c3e\u3002\n\u4e00\u4e2a\u666e\u901a\u7684seq2seq\u89e3\u7801\u5668\u7684\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u662f\uff1a\n\u5982\u679c\u6211\u4eec\u4f9d\u9760\u4e0a\u4e0b\u6587\u5411\u91cf\u6765\u7f16\u7801\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u7684\u610f\u4e49\uff0c\u6211\u4eec\u5f88\u53ef\u80fd\u4f1a\u6709\u4fe1\u606f\u4e22\u5931\u3002\n\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u8f93\u5165\u5e8f\u5217\u65f6\uff0c\u5927\u5927\u9650\u5236\u4e86\u89e3\u7801\u5668\u7684\u6027\u80fd\u3002\n\n\u4e0e\u6b64\u4f5c\u6597\u4e89\uff0c`Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__ \n\u521b\u5efa\u4e86\u4e00\u4e2a\u201c\u6ce8\u610f\u673a\u5236(attention mechanism)\u201d\uff0c\n\u5141\u8bb8\u89e3\u7801\u5668\u6ce8\u610f\u8f93\u5165\u5e8f\u5217\u7684\u67d0\u4e9b\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u6574\u4e2a\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u3002\n\n\u5728\u8f83\u9ad8\u7684\u5c42\u6b21\u4e0a\uff0c\u6ce8\u610f\u529b\u662f\u5229\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u9690\u85cf\u72b6\u6001\u548c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u6765\u8ba1\u7b97\u7684\u3002\n\u8f93\u51fa\u6ce8\u610f\u6743\u91cd(output attention weights)\u4e0e\u8f93\u5165\u5e8f\u5217\u5177\u6709\u76f8\u540c\u7684shape\uff0c\n\u53ef\u4ee5\u5c06\u5b83\u4eec\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u76f8\u4e58\uff0c\u7ed9\u51fa\u4e00\u4e2a\u52a0\u6743\u548c\uff0c\u8868\u793a\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u90e8\u5206\u3002\n`Sean Robertson\u2019s <https://github.com/spro>`__  \u7684\u56fe\u7247\u5f88\u597d\u5730\u63cf\u8ff0\u4e86\u8fd9\u4e00\u70b9:\n\n.. figure:: /_static/img/chatbot/attn2.png\n   :align: center\n   :alt: attn2\n\n`Luong et al. <https://arxiv.org/abs/1508.04025>`__ \u901a\u8fc7\u521b\u9020\u201c\u5168\u5c40\u6ce8\u610f(Global attention)\u201d \u5728Bahdanau\u7b49\u4eba\u7684\u57fa\u7840\u4e0a\u52a0\u4ee5\u6539\u8fdb\u3002\n\u5173\u952e\u7684\u533a\u522b\u5728\u4e8e\uff0c\u5bf9\u4e8e\u201c\u5168\u5c40\u6ce8\u610f\u201d\uff0c\u6211\u4eec\u8003\u8651\u7f16\u7801\u5668\u7684\u6240\u6709\u9690\u85cf\u72b6\u6001\uff0c\n\u800c\u4e0d\u662fBahdanau\u7b49\u4eba\u7684\u201c\u5c40\u90e8\u6ce8\u610f\u201d\uff0c\u540e\u8005\u53ea\u8003\u8651\u7f16\u7801\u5668\u7684\u5f53\u524d\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u72b6\u6001\u3002\n\u53e6\u4e00\u4e2a\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\uff0c\u5bf9\u4e8e\u201c\u5168\u5c40\u6ce8\u610f\u201d\uff0c\u6211\u4eec\u53ea\u4ece\u5f53\u524d\u7684\u65f6\u95f4\u6b65\u4e2d\u4f7f\u7528\u89e3\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u6216\u80fd\u91cf\u3002\nBahdanau\u7b49\u4eba\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u9700\u8981\u4ece\u524d\u4e00\u65f6\u95f4\u6b65\u4e86\u89e3\u89e3\u7801\u5668\u7684\u72b6\u6001\u3002\u53e6\u5916\uff0cLuong\u7b49\u4eba\u63d0\u4f9b\u5404\u79cd\u65b9\u6cd5\u6765\u8ba1\u7b97\u7f16\u7801\u5668\u8f93\u51fa\n\u548c\u89e3\u7801\u5668\u8f93\u51fa\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u80fd\u91cf\uff0c\u79f0\u4e3a \u201cscore functions\u201d :\n\n.. figure:: /_static/img/chatbot/scores.png\n   :width: 60%\n   :align: center\n   :alt: scores\n\n\u5176\u4e2d $h_t$ = current target decoder state \u548c $\\bar{h}_s$ = all encoder states.\n\n\u603b\u4f53\u800c\u8a00, \u5168\u5c40\u6ce8\u610f\u673a\u5236\u53ef\u4ee5\u7531\u4e0b\u9762\u7684\u8fd9\u4e2a\u56fe\u6765\u8bf4\u660e\u3002\u6ce8\u610f\u5230\u6211\u4eec\u5c06\u5b9e\u73b0 \u201cAttention Layer\u201d \n\u4f5c\u4e3a\u5355\u72ec\u5206\u5f00\u7684 ``nn.Module`` \uff0c\u79f0\u4e4b\u4e3a ``Attn`` \u3002\u8be5module\u7684\u8f93\u51fa\u662f\u4e00\u4e2ashape\u4e3a \n*(batch_size, 1, max_length)* \u7684 softmax normalized weights tensor\u3002\n\n.. figure:: /_static/img/chatbot/global_attn.png\n   :align: center\n   :width: 60%\n   :alt: global_attn\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Luong attention layer\nclass Attn(torch.nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n        self.method = method\n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, \"is not an appropriate attention method.\")\n        self.hidden_size = hidden_size\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n\n    def dot_score(self, hidden, encoder_output):\n        return torch.sum(hidden * encoder_output, dim=2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n\n    def forward(self, hidden, encoder_outputs):\n        # Calculate the attention weights (energies) based on the given method\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden, encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden, encoder_outputs)\n\n        # Transpose max_length and batch_size dimensions\n        attn_energies = attn_energies.t()\n\n        # Return the softmax normalized probability scores (with added dimension)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u6211\u4eec\u7684\u6ce8\u610f\u5b50\u6a21\u5757(attention submodule)\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u9645\u7684\u89e3\u7801\u5668\u6a21\u578b\u3002\n\u5bf9\u4e8e\u89e3\u7801\u5668\uff0c\u6211\u4eec\u5c06\u4e00\u6b21\u4e00\u4e2a\u65f6\u95f4\u6b65\u624b\u52a8\u8f93\u5165\u6211\u4eec\u7684batch\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u5d4c\u5165\u7684\u5355\u8bcd\u5f20\u91cf\u548cGRU\u8f93\u51fa\u90fd\u5c06\u5177\u6709\nshape \u4e3a *(1, batch_size, hidden_size)* \u3002\n\n**\u8ba1\u7b97\u56fe:**\n\n   1) \u83b7\u53d6\u5f53\u524d\u8f93\u5165\u5355\u8bcd\u7684\u5d4c\u5165(embedding)\n   2) \u524d\u5411\u4f20\u9012\u901a\u8fc7\u53cc\u5411GRU.\n   3) \u4ece\u4e0a\u9762\u7b2c(2)\u6b65\u7684\u5f53\u524dGRU\u7684\u8f93\u51fa \u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd.\n   4) \u628a\u6ce8\u610f\u529b\u6743\u91cd\u4e58\u5230\u7f16\u7801\u5668\u8f93\u51fa\u4e0a\u6765\u83b7\u5f97\u65b0\u7684\"\u52a0\u6743\u548c\"\u4e0a\u4e0b\u6587\u5411\u91cf.\n   5) \u4f7f\u7528 Luong eq. 5 \u4e32\u63a5(Concatenate)\u52a0\u6743\u4e0a\u4e0b\u6587\u5411\u91cf\u548cGRU\u8f93\u51fa\u3002\n   6) \u4f7f\u7528 Luong eq. 6 \u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd(\u6ca1\u6709 softmax).\n   7) \u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u7684\u9690\u85cf\u72b6\u6001.\n\n**\u8f93\u5165:**\n\n-  ``input_step``: one time step (one word) of input sequence batch; shape=\\ *(1, batch_size)*\n-  ``last_hidden``: GRU\u7684\u6700\u7ec8\u9690\u85cf\u5c42; shape=\\ *(n_layers x num_directions, batch_size, hidden_size)*\n-  ``encoder_outputs``: \u7f16\u7801\u5668\u7684\u8f93\u51fa; shape=\\ *(max_length, batch_size, hidden_size)*\n\n**\u8f93\u51fa:**\n\n-  ``output``: \u7ed9\u5b9a \u89e3\u7801\u5f97\u5230\u7684\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u662f\u6b63\u786e\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u6982\u7387 \u4e0b \u7684 softmax normalized tensor ;\n   shape=\\ *(batch_size, voc.num_words)*\n-  ``hidden``: GRU\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001; shape=\\ *(n_layers x num_directions, batch_size, hidden_size)*\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        # Note: we run this one step (word) at a time\n        # Get embedding of current input word\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # Forward through unidirectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        # Calculate attention weights from the current GRU output\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n        # Concatenate weighted context vector and GRU output using Luong eq. 5\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # Predict next word using Luong eq. 6\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        # Return output and final hidden state\n        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u8bad\u7ec3\u6b65\u9aa4\n-------------------------\n\n\u63a9\u6a21\u635f\u5931(Masked loss)\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u7531\u4e8e\u6211\u4eec\u5904\u7406\u7684\u662f\u4e00\u4e2a\u6279\u6b21\u7684\u586b\u5145\u5e8f\u5217(padded sequences)\uff0c\u6240\u4ee5\u5728\u8ba1\u7b97\u635f\u5931\u65f6\u4e0d\u80fd\u7b80\u5355\u5730\u8003\u8651\u5f20\u91cf\u7684\u6240\u6709\u5143\u7d20\u3002\n\u6839\u636e\u89e3\u7801\u5668\u8f93\u51fa\u5f20\u91cf\u3001\u76ee\u6807\u5f20\u91cf\u548c\u63cf\u8ff0\u76ee\u6807\u5f20\u91cf\u586b\u5145\u7684\u4e8c\u503c\u63a9\u6a21\u5f20\u91cf(binary mask tensor)\uff0c\n\u5b9a\u4e49\u4e86 ``maskNLLLoss`` \u6765\u8ba1\u7b97\u6211\u4eec\u7684\u635f\u5931\u3002\u6b64\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u4e0e\u63a9\u6a21\u5f20\u91cf\u4e2d\u7684 *1* \u5bf9\u5e94\u7684\u5143\u7d20\u7684\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136(average negative log likelihood)\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask):\n    nTotal = mask.sum()\n    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n    loss = crossEntropy.masked_select(mask).mean()\n    loss = loss.to(device)\n    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5355\u4e2a\u8bad\u7ec3\u8fed\u4ee3\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``train`` \u51fd\u6570\u5305\u542b\u5355\u4e2a\u8bad\u7ec3\u8fed\u4ee3(\u5355\u4e2a\u8f93\u5165\u6279\u6b21)\u7684\u7b97\u6cd5\u3002\n\n\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e9b\u5de7\u5999\u7684\u6280\u5de7\u6765\u5e2e\u52a9\u6536\u655b:\n\n-  \u7b2c\u4e00\u4e2a\u8bc0\u7a8d\u662f\u5229\u7528\u8001\u5e08\u7684\u5f3a\u8feb(**teacher forcing**)\u3002\u8fd9\u610f\u5473\u7740\uff0c\u5728\u4e00\u5b9a\u7684\u6982\u7387\u4e0b\uff0c\u6839\u636e ``teacher_forcing_ratio`` \u8bbe\u7f6e\uff0c\n   \u6211\u4eec\u4f7f\u7528\u5f53\u524d\u76ee\u6807\u8bcd\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u4e0b\u4e00\u4e2a\u8f93\u5165\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u89e3\u7801\u5668\u5f53\u524d\u7684\u731c\u6d4b\u3002\n   \u8fd9\u79cd\u6280\u672f\u4f5c\u4e3a\u89e3\u7801\u5668\u7684 training wheels\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u3002\n   \u7136\u800c\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6559\u5e08\u7684\u5f3a\u8feb\u4f1a\u5bfc\u81f4\u6a21\u578b\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\n   \u56e0\u4e3a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u89e3\u7801\u5668\u53ef\u80fd\u6ca1\u6709\u8db3\u591f\u7684\u673a\u4f1a\u771f\u6b63\u5730\u5b8c\u6210\u81ea\u5df1\u7684\u8f93\u51fa\u5e8f\u5217\u3002\n   \u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u5982\u4f55\u8bbe\u7f6e ``teacher_forcing_ratio`` \uff0c\u800c\u4e0d\u662f\u88ab\u5feb\u901f\u6536\u655b\u6240\u611a\u5f04\u3002\n\n-  \u6211\u4eec\u5b9e\u73b0\u7684\u7b2c\u4e8c\u4e2a\u6280\u5de7\u662f\u68af\u5ea6\u88c1\u526a **(gradient clipping)** \u3002\u8fd9\u662f\u5bf9\u4ed8\u201c\u7206\u70b8\u68af\u5ea6(exploding gradient)\u201d\u95ee\u9898\u7684\u4e00\u79cd\u5e38\u7528\u6280\u672f\u3002\n   \u4ece\u672c\u8d28\u4e0a\u8bf4\uff0c\u901a\u8fc7\u5c06\u68af\u5ea6\u88c1\u526a\u6216\u9608\u503c\u5316\u5230\u4e00\u4e2a\u6700\u5927\u503c\uff0c\u6211\u4eec\u53ef\u4ee5\u9632\u6b62\u68af\u5ea6\u6307\u6570\u589e\u957f\uff0c \u6216\u8005\u5728\u4ee3\u4ef7\u51fd\u6570\u4e2d\u6ea2\u51fa(NaN)\u6216\u8fc7\u9661\u7684\u60ac\u5d16\u3002\n\n.. figure:: /_static/img/chatbot/grad_clip.png\n   :align: center\n   :width: 60%\n   :alt: grad_clip\n\n\u56fe\u7247\u6765\u6e90: Goodfellow et al. *Deep Learning*. 2016. http://www.deeplearningbook.org/\n\n**Sequence of Operations:**\n\n   1) \u628a\u6574\u4e2a\u8f93\u5165\u6279\u6b21\u524d\u5411\u4f20\u9012\u4f7f\u5176\u901a\u8fc7\u7f16\u7801\u5668.\n   2) \u4ee5 SOS_token \u521d\u59cb\u5316\u89e3\u7801\u5668\u8f93\u5165, \u548c \u4ee5\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u521d\u59cb\u5316 hidden state .\n   3) \u4e00\u6b21\u4e00\u4e2a\u65f6\u95f4\u6b65 \u524d\u5411\u4f20\u9012\u8f93\u5165\u6279\u6b21\u5e8f\u5217 \u4f7f\u5176\u901a\u8fc7\u89e3\u7801\u5668.\n   4) \u5982\u679c\u4f7f\u7528\u6559\u5e08\u5f3a\u8feb: \u628a\u5f53\u524d\u72b6\u6001\u8bbe\u7f6e\u4e3a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165; \u5982\u679c\u4e0d\u7528: \u628a\u5f53\u524d\u89e3\u7801\u5668\u7684\u8f93\u51fa\u8bbe\u7f6e\u4e3a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165.\n   5) \u8ba1\u7b97\u5e76\u79ef\u7d2f\u635f\u5931.\n   6) \u6267\u884c\u5411\u540e\u4f20\u64ad.\n   7) \u88c1\u526a\u68af\u5ea6.\n   8) \u66f4\u65b0\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6a21\u578b\u53c2\u6570.\n\n\n.. Note ::\n\n  PyTorch\u7684RNN modules(``RNN``, ``LSTM``, ``GRU``)\u53ef\u4ee5\u50cf\u5176\u4ed6\u975e\u9012\u5f52\u5c42(non-recurrent layers)\u4e00\u6837\u4f7f\u7528\uff0c\n  \u53ea\u9700\u5c06\u6574\u4e2a\u8f93\u5165\u5e8f\u5217(\u6216\u6279\u5904\u7406\u5e8f\u5217)\u4f20\u9012\u7ed9\u5b83\u4eec\u3002\n  \u6211\u4eec\u5728 ``encoder`` \u4e2d\u5c31\u662f\u8fd9\u6837\u4f7f\u7528 ``GRU`` \u5c42\u7684\u3002\u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u5728\u5e95\u5c42\u6709\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4e0a\u5faa\u73af\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u3002\n  \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u4e00\u6b21\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8fd0\u884c\u8fd9\u4e9bmodules\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u624b\u52a8\u5faa\u73af\u5e8f\u5217\uff0c\u5c31\u50cf\u6211\u4eec\u5fc5\u987b\u4e3a ``decoder`` \u6a21\u578b\u6240\u505a\u7684\u90a3\u6837\u3002\n  \u53ea\u8981\u60a8\u7ef4\u62a4\u8fd9\u4e9bmodules\u7684\u6b63\u786e\u6982\u5ff5\u6a21\u578b\uff0c\u5b9e\u73b0\u987a\u5e8f\u6a21\u578b\u5c31\u4f1a\u975e\u5e38\u7b80\u5355\u3002\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n\n    # Zero gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Set device options\n    input_variable = input_variable.to(device)\n    lengths = lengths.to(device)\n    target_variable = target_variable.to(device)\n    mask = mask.to(device)\n\n    # Initialize variables\n    loss = 0\n    print_losses = []\n    n_totals = 0\n\n    # Forward pass through encoder\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n\n    # Create initial decoder input (start with SOS tokens for each sentence)\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n\n    # Set initial decoder hidden state to the encoder's final hidden state\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    # Determine if we are using teacher forcing this iteration\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Forward batch of sequences through decoder one time step at a time\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # Teacher forcing: next input is current target\n            decoder_input = target_variable[t].view(1, -1)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    else:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # No teacher forcing: next input is decoder's own current output\n            _, topi = decoder_output.topk(1)\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n\n    # Perform backpropatation\n    loss.backward()\n\n    # Clip gradients: gradients are modified in place\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n    # Adjust model weights\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u591a\u4e2a\u8bad\u7ec3\u8fed\u4ee3\n~~~~~~~~~~~~~~~~~~~\n\n\u6700\u540e\uff0c\u662f\u65f6\u5019\u5c06\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e0e\u6570\u636e\u7ed3\u5408\u8d77\u6765\u4e86\u3002\u7ed9\u5b9a\u4f20\u9012\u7684\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001\u6570\u636e\u7b49\uff0c\n``trainIters`` \u51fd\u6570\u8d1f\u8d23\u8bad\u7ec3\u7684 ``n_iterations`` \u6b21\u8fed\u4ee3\u3002\n\u8fd9\u4e2a\u51fd\u6570\u662f\u76f8\u5f53\u4e0d\u8a00\u81ea\u660e\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u5df2\u7ecf\u7528 ``train`` \u51fd\u6570\u505a\u4e86\u7e41\u91cd\u7684\u5de5\u4f5c\u3002\n\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u6211\u4eec\u4fdd\u5b58\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u4fdd\u5b58\u4e86\u4e00\u4e2atarball\uff0c\u5b83\u5305\u542b\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684 state_dicts (\u53c2\u6570)\u3001\n\u4f18\u5316\u5668\u7684 state_dicts\u3001\u635f\u5931\u3001\u8fed\u4ee3\u7b49\u7b49\u3002\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u4fdd\u5b58\u6a21\u578b\u5c06\u4f7f\u6211\u4eec\u5bf9\u68c0\u67e5\u70b9(checkpoint)\u5177\u6709\u6700\u7ec8\u7684\u7075\u6d3b\u6027\u3002\n\u5728\u52a0\u8f7d\u68c0\u67e5\u70b9\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u80fd\u591f\u4f7f\u7528\u6a21\u578b\u53c2\u6570\u6765\u8fd0\u884c\u63a8\u7406\uff0c\u6216\u8005\u6211\u4eec\u53ef\u4ee5\u7ee7\u7eed\u5728\u6211\u4eec\u505c\u6b62\u7684\u5730\u65b9\u8fdb\u884c\u8bad\u7ec3\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n\n    # Load batches for each iteration\n    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n                      for _ in range(n_iteration)]\n\n    # Initializations\n    print('Initializing ...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n\n    # Training loop\n    print(\"Training...\")\n    for iteration in range(start_iteration, n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        # Extract fields from batch\n        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n\n        # Run a training iteration with batch\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n        print_loss += loss\n\n        # Print progress\n        if iteration % print_every == 0:\n            print_loss_avg = print_loss / print_every\n            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n            print_loss = 0\n\n        # Save checkpoint\n        if (iteration % save_every == 0):\n            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                'iteration': iteration,\n                'en': encoder.state_dict(),\n                'de': decoder.state_dict(),\n                'en_opt': encoder_optimizer.state_dict(),\n                'de_opt': decoder_optimizer.state_dict(),\n                'loss': loss,\n                'voc_dict': voc.__dict__,\n                'embedding': embedding.state_dict()\n            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u8bc4\u4f30\n-----------------\n\n\u5728\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6a21\u578b\u4e4b\u540e\uff0c\u6211\u4eec\u5e0c\u671b\u81ea\u5df1\u80fd\u591f\u548c\u673a\u5668\u4eba\u4ea4\u8c08\u3002\u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u5982\u4f55\u89e3\u7801\u7f16\u7801\u7684\u8f93\u5165\u3002\n\n\u8d2a\u5a6a\u89e3\u7801\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u8d2a\u5a6a\u89e3\u7801(Greedy decoding)\u662f\u6211\u4eec\u5728 **\u4e0d** \u4f7f\u7528\u6559\u5e08\u5f3a\u8feb(teacher forcing)\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u4e00\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\n\u6362\u53e5\u8bdd\u8bf4\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u65f6\u95f4\u6b65\uff0c\u6211\u4eec\u53ea\u9700\u9009\u62e9\u5177\u6709\u6700\u9ad8softmax\u503c\u7684 ``decoder_output`` \u4e2d\u7684\u5355\u8bcd\u3002\n\u8fd9\u79cd\u89e3\u7801\u65b9\u6cd5\u5728\u5355\u4e2a\u65f6\u95f4\u6b65\u6c34\u5e73(single time-step level)\u4e0a\u662f\u6700\u4f18\u7684. \n\n\u4e3a\u4e86\u65b9\u4fbf\u8d2a\u5a6a\u89e3\u7801\u64cd\u4f5c\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86 ``GreedySearchDecoder`` \u7c7b\u3002\u5f53\u8fd0\u884c\u65f6\uff0c\n\u8be5\u7c7b\u7684\u5bf9\u8c61\u63a5\u53d7shape\u4e3a *(input_seq length, 1)* \u7684\u8f93\u5165\u5e8f\u5217(``input_seq``)\u3001\u6807\u91cf\u8f93\u5165\u957f\u5ea6(``input_length``)\u5f20\u91cf\uff0c\n\u4ee5\u53ca ``max_length`` \u6765\u9650\u5236\u54cd\u5e94\u8bed\u53e5\u7684\u957f\u5ea6\u3002\u8f93\u5165\u53e5\u5b50\u4f7f\u7528\u4ee5\u4e0b\u8ba1\u7b97\u56fe\u8fdb\u884c\u8ba1\u7b97\uff1a\n\n**\u8ba1\u7b97\u56fe:**\n\n   1) \u5411\u524d\u4f20\u9012\u8f93\u5165\u4f7f\u5176\u901a\u8fc7\u7f16\u7801\u5668\u6a21\u578b.\n   2) \u51c6\u5907\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u5c42\u4f5c\u4e3a\u89e3\u7801\u5668\u7b2c\u4e00\u9690\u85cf\u5c42\u7684\u8f93\u5165.\n   3) \u521d\u59cb\u5316\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u4e3a SOS_token.\n   4) \u521d\u59cb\u5316\u8981\u8ffd\u52a0\u5230\u89e3\u7801\u51fa\u7684\u5355\u8bcd\u4e0a\u7684\u5f20\u91cf.\n   5) \u4e00\u6b21\u8fed\u4ee3\u89e3\u7801\u4e00\u4e2a\u5355\u8bcd\u6807\u8bb0(word token):\n       a) \u5411\u524d\u4f20\u9012\u901a\u8fc7\u89e3\u7801\u5668.\n       b) \u83b7\u5f97\u6700\u6709\u53ef\u80fd\u7684\u5355\u8bcd\u6807\u8bb0(word token)\u548c\u5bf9\u5e94\u7684(softmax score).\n       c) \u8bb0\u5f55 token \u548c score.\n       d) \u51c6\u5907\u628a\u5f53\u524d token \u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165.\n   6) \u8fd4\u56de\u672c\u6b21\u8fed\u4ee3\u8fc7\u7a0b\u5f97\u5230\u7684 word tokens \u548c scores \u7684\u96c6\u5408(collections).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(GreedySearchDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input_seq, input_length, max_length):\n        # Forward input through encoder model\n        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n        decoder_hidden = encoder_hidden[:decoder.n_layers]\n        # Initialize decoder input with SOS_token\n        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n        # Initialize tensors to append decoded words to\n        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n        all_scores = torch.zeros([0], device=device)\n        # Iteratively decode one word token at a time\n        for _ in range(max_length):\n            # Forward pass through decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            # Obtain most likely word token and its softmax score\n            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n            # Record token and score\n            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n            # Prepare current token to be next decoder input (add a dimension)\n            decoder_input = torch.unsqueeze(decoder_input, 0)\n        # Return collections of word tokens and scores\n        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bc4\u4f30\u6211\u7684\u6587\u672c\n~~~~~~~~~~~~~~~~\n\n\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u89e3\u7801\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u7f16\u5199\u7528\u4e8e\u8bc4\u4f30(evaluate)\u5b57\u7b26\u4e32\u8f93\u5165\u8bed\u53e5\u7684\u51fd\u6570\u3002\n``evaluate`` \u51fd\u6570\u7ba1\u7406\u5904\u7406\u8f93\u5165\u8bed\u53e5\u7684\u4f4e\u7ea7\u8fc7\u7a0b\u3002\u6211\u4eec\u9996\u5148\u5c06\u53e5\u5b50\u683c\u5f0f\u5316\u4e3a\u5355\u8bcd\u7d22\u5f15\u7684\u8f93\u5165batch(\u5176\u4e2d *batch_size==1* )\u3002\n\u6211\u4eec\u5c06\u53e5\u5b50\u4e2d\u7684\u5355\u8bcd\u8f6c\u6362\u6210\u76f8\u5e94\u7684\u7d22\u5f15\uff0c\u5e76\u8c03\u6362\u7ef4\u5ea6\u987a\u5e8f\u6765\u4e3a\u6211\u4eec\u7684\u6a21\u578b\u51c6\u5907\u5f20\u91cf\u3002\u6211\u4eec\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a ``lengths`` \u5f20\u91cf\uff0c\n\u5b83\u5305\u542b\u6211\u4eec\u8f93\u5165\u53e5\u5b50\u7684\u957f\u5ea6\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c``lengths`` \u662f\u6807\u91cf\uff0c\u56e0\u4e3a\u6211\u4eec\u4e00\u6b21\u53ea\u8ba1\u7b97\u4e00\u4e2a\u53e5\u5b50(batch_size==1)\u3002\n\u7136\u540e\uff0c\u5229\u7528 ``GreedySearchDecoder`` \u5bf9\u8c61(``searcher``)\u5f97\u5230\u89e3\u7801\u540e\u7684\u54cd\u5e94\u8bed\u53e5\u5f20\u91cf\u3002\n\u6700\u540e\uff0c\u6211\u4eec\u5c06\u54cd\u5e94\u7684\u7d22\u5f15\u8f6c\u6362\u4e3a\u5355\u8bcd\uff0c\u5e76\u8fd4\u56de\u89e3\u7801\u5f97\u5230\u7684\u5355\u8bcd\u5217\u8868\u3002\n\n``evaluateInput`` \u5145\u5f53\u804a\u5929\u673a\u5668\u4eba\u7684\u7528\u6237\u754c\u9762\u3002\u8c03\u7528\u65f6\u5c06\u663e\u793a\u8f93\u5165\u6587\u672c\u6846\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u5176\u4e2d\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u3002\n\u8f93\u5165\u53e5\u5b50\u5e76\u6309 *Enter* \u952e\u540e\uff0c\u6211\u4eec\u7684\u6587\u672c\u4ee5\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u540c\u7684\u65b9\u5f0f\u5f52\u4e00\u5316\uff0c\n\u5e76\u6700\u7ec8\u88ab\u8f93\u5165\u5230 ``evaluate`` \u51fd\u6570\u4e2d\uff0c\u5f97\u5230\u89e3\u7801\u540e\u7684\u8f93\u51fa\u8bed\u53e5\u3002\u6211\u4eec\u5faa\u73af\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\n\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u4e00\u76f4\u4e0e\u6211\u4eec\u7684\u673a\u5668\u4eba\u804a\u5929\uff0c\u76f4\u5230\u6211\u4eec\u8f93\u5165\u201cq\u201d \u6216 \u201cquit\u201d\u3002\n\n\u6700\u540e\uff0c\u5982\u679c\u8f93\u5165\u7684\u53e5\u5b50\u4e2d\u5305\u542b\u4e00\u4e2a\u4e0d\u5728\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\uff0c\u6211\u4eec\u5c06\u901a\u8fc7\u6253\u5370\u9519\u8bef\u6d88\u606f\u5e76\u63d0\u793a\n\u7528\u6237\u8f93\u5165\u53e6\u4e00\u4e2a\u53e5\u5b50\u6765\u4f18\u96c5\u5730\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n    ### Format input sentence as a batch\n    # words -> indexes\n    indexes_batch = [indexesFromSentence(voc, sentence)]\n    # Create lengths tensor\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    # Transpose dimensions of batch to match models' expectations\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n    # Use appropriate device\n    input_batch = input_batch.to(device)\n    lengths = lengths.to(device)\n    # Decode sentence with searcher\n    tokens, scores = searcher(input_batch, lengths, max_length)\n    # indexes -> words\n    decoded_words = [voc.index2word[token.item()] for token in tokens]\n    return decoded_words\n\n\ndef evaluateInput(encoder, decoder, searcher, voc):\n    input_sentence = ''\n    while(1):\n        try:\n            # Get input sentence\n            input_sentence = input('> ')\n            # Check if it is quit case\n            if input_sentence == 'q' or input_sentence == 'quit': break\n            # Normalize sentence\n            input_sentence = normalizeString(input_sentence)\n            # Evaluate sentence\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n            # Format and print response sentence\n            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n            print('Bot:', ' '.join(output_words))\n\n        except KeyError:\n            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u6a21\u578b\n---------\n\n\u6700\u540e\uff0c\u662f\u65f6\u5019\u8fd0\u884c\u6211\u4eec\u7684\u6a21\u578b\u4e86!\n\n\u65e0\u8bba\u6211\u4eec\u662f\u8981\u8bad\u7ec3\u8fd8\u662f\u6d4b\u8bd5\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\uff0c\u6211\u4eec\u90fd\u5fc5\u987b\u521d\u59cb\u5316\u5355\u4e2a\u7684\u7f16\u89e3\u7801\u6a21\u578b\u3002\n\u5728\u4e0b\u9762\u7684\u5757\u4e2d\uff0c\u6211\u4eec\u8bbe\u7f6e\u4e86\u6240\u9700\u7684\u914d\u7f6e\uff0c\u9009\u62e9\u4ece\u5934\u5f00\u59cb\u6216\u8bbe\u7f6e\u8981\u52a0\u8f7d\u7684\u68c0\u67e5\u70b9\uff0c\n\u5e76\u6784\u5efa\u548c\u521d\u59cb\u5316\u6a21\u578b\u3002\u53ef\u4ee5\u968f\u610f\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u578b\u914d\u7f6e\u6765\u4f18\u5316\u6027\u80fd\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Configure models\nmodel_name = 'cb_model'\nattn_model = 'dot'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nbatch_size = 64\n\n# Set checkpoint to load from; set to None if starting from scratch\nloadFilename = None\ncheckpoint_iter = 4000\n#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n\n\n# Load model if a loadFilename is provided\nif loadFilename:\n    # If loading on same machine the model was trained on\n    checkpoint = torch.load(loadFilename)\n    # If loading a model trained on GPU to CPU\n    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n    encoder_sd = checkpoint['en']\n    decoder_sd = checkpoint['de']\n    encoder_optimizer_sd = checkpoint['en_opt']\n    decoder_optimizer_sd = checkpoint['de_opt']\n    embedding_sd = checkpoint['embedding']\n    voc.__dict__ = checkpoint['voc_dict']\n\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.num_words, hidden_size)\nif loadFilename:\n    embedding.load_state_dict(embedding_sd)\n# Initialize encoder & decoder models\nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\nif loadFilename:\n    encoder.load_state_dict(encoder_sd)\n    decoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u8bad\u7ec3\u8fc7\u7a0b\n~~~~~~~~~~~~\n\n\u5982\u679c\u8981\u8bad\u7ec3\u6a21\u578b\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u5757\u3002\n\n\u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570\uff0c\u7136\u540e\u521d\u59cb\u5316\u6211\u4eec\u7684\u4f18\u5316\u5668\uff0c\u6700\u540e\u6211\u4eec\u8c03\u7528 ``trainIters`` \u51fd\u6570\u6765\u8fd0\u884c\u6211\u4eec\u7684\u8bad\u7ec3\u8fed\u4ee3\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Configure training/optimization\nclip = 50.0\nteacher_forcing_ratio = 1.0\nlearning_rate = 0.0001\ndecoder_learning_ratio = 5.0\nn_iteration = 4000\nprint_every = 1\nsave_every = 500\n\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\nif loadFilename:\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n\n# Run training iterations\nprint(\"Starting Training!\")\ntrainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u8bc4\u4f30\u8fc7\u7a0b\n~~~~~~~~~~~~~~\n\n\u82e5\u8981\u4e0e\u6a21\u578b\u804a\u5929\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u5757\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set dropout layers to eval mode\nencoder.eval()\ndecoder.eval()\n\n# Initialize search module\nsearcher = GreedySearchDecoder(encoder, decoder)\n\n# Begin chatting (uncomment and run the following line to begin)\n# evaluateInput(encoder, decoder, searcher, voc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u603b\u7ed3\n----------\n\n\u4f19\u8ba1\u4eec\uff0c\u8fd9\u4ef6\u4e8b\u5c31\u5230\u6b64\u4e3a\u6b62\u4e86\u3002\u606d\u559c\u4f60\uff0c\u4f60\u73b0\u5728\u77e5\u9053\u5efa\u7acb\u4e00\u4e2a\u751f\u6210\u5f0f\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u7684\u57fa\u672c\u539f\u7406\u4e86\uff01\n\u5982\u679c\u60a8\u611f\u5174\u8da3\uff0c\u60a8\u53ef\u4ee5\u5c1d\u8bd5\u88c1\u526a(tailoring)\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\uff0c\u65b9\u6cd5\u662f\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u53c2\u6570\uff0c\n\u5e76\u81ea\u5b9a\u4e49\u60a8\u8bad\u7ec3\u6a21\u578b\u7684\u6570\u636e\u3002\n\n\u67e5\u770bPyTorch\u4e2d\u7684\u5176\u4ed6\u6559\u7a0b\uff0c\u4ee5\u83b7\u5f97\u66f4\u9177\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\uff01\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}