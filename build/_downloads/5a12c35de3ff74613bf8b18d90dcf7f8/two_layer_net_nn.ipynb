{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch: nn\n-----------\n\n\u4e00\u4e2a\u5b8c\u5168\u8fde\u63a5\u7684ReLU\u7f51\u7edc\uff0c\u53ea\u6709\u4e00\u4e2a\u9690\u85cf\u5c42\uff0c\u6ca1\u6709\u504f\u7f6e\uff0c\u6700\u5c0f\u5316\u6b27\u6c0f\u8bef\u5dee\u8bad\u7ec3\u4ecex\u9884\u6d4by\u3002\n\n\u8fd9\u4e2a\u5b9e\u73b0\u4f7f\u7528PyTorch\u7684nn\u5305\u6765\u6784\u5efa\u7f51\u7edc\u3002PyTorch Autograd\u4f7f\u5b9a\u4e49\u8ba1\u7b97\u56fe\u548c\u83b7\u53d6\u68af\u5ea6\u53d8\u5f97\u5f88\u5bb9\u6613\uff0c\n\u4f46\u662f\u5bf9\u4e8e\u5b9a\u4e49\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u8bf4\uff0c\u539f\u59cb\u7684\u81ea\u52a8\u68af\u5ea6\u53ef\u80fd\u592a\u4f4e\u7ea7\u4e86\uff1b\u8fd9\u5c31\u662fnn\u5305\u53ef\u4ee5\u63d0\u4f9b\u5e2e\u52a9\u7684\u5730\u65b9\u3002\nnn\u5305\u5b9a\u4e49\u4e86\u4e00\u7ec4Modules\uff0c\u53ef\u4ee5\u628a\u5b83\u770b\u4f5c\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u5b83\u4ea7\u751f\u8f93\u5165\u7684\u8f93\u51fa\uff0c\n\u5e76\u4e14\u53ef\u80fd\u5177\u6709\u4e00\u4e9b\u53ef\u8bad\u7ec3\u7684\u6743\u91cd\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \u4f7f\u7528 nn package \u6765\u628a\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3alayers\u6784\u6210\u7684\u5e8f\u5217\u3002nn.Sequential\n# \u662f\u4e00\u4e2a\u5305\u542b\u4e86\u5176\u4ed6Modules\u7684Module, \u5e76\u628a\u5b83\u4eec\u5e94\u7528\u5728\u5e8f\u5217\u4e2d\u4ea7\u751f\u8f93\u51fa\u3002\n# \u6bcf\u4e2aLinear Module\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u4ece\u8f93\u5165\u8ba1\u7b97\u8f93\u51fa\uff0c\u5e76\u4e14\u6301\u6709\u5185\u90e8\u5f20\u91cf\u7528\u4e8e\u5b58\u50a8\u5b83\u7684\u6743\u91cd\u548c\u504f\u7f6e\u3002\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(D_in, H),\n    torch.nn.ReLU(),\n    torch.nn.Linear(H, D_out),\n)\n\n# nn package \u4e5f\u5305\u542b\u4e86\u5404\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u635f\u5931\u51fd\u6570;\n# \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528 Mean Squared Error (MSE) \u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u3002\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(500):\n    # \u524d\u5411\u4f20\u9012: \u628a x \u4f20\u5165 model \u8ba1\u7b97\u9884\u6d4b\u8f93\u51fa y \u3002\u56e0\u4e3a Module objects \u91cd\u8f7d\u4e86 \n    # __call__ \u8fd9\u4e2a\u9b54\u6cd5\u51fd\u6570\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u50cf\u8c03\u7528\u51fd\u6570\u4e00\u6837\u8c03\u7528 model \u3002\n    # \u5f53\u4f60\u8fd9\u4e48\u505a\u7684\u65f6\u5019\uff0c\u4f60\u8981\u628a\u8f93\u5165\u6570\u636e\u7684Tensor\u4f20\u9012\u5230Module\u91cc\u9762\uff0c\u5e76\u4ea7\u751f\u8f93\u51fa\u6570\u636e\u7684Tensor.\n    y_pred = model(x)\n\n    # \u8ba1\u7b97\u5e76\u8f93\u51fa loss. \u6211\u4eec\u628a\u5305\u542b\u9884\u6d4b\u503c\u7684\u5f20\u91cf y_pred \u548c\u771f\u5b9e\u503c\u7684\u5f20\u91cf y \u90fd\u4f20\u5165\u635f\u5931\u51fd\u6570\uff0c\n    # \u635f\u5931\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u635f\u5931\u7684\u5f20\u91cf\u3002\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \u5728\u8fd0\u884c\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u5148\u5c06\u6a21\u578b\u5185\u90e8\u7684\u68af\u5ea6\u7f13\u5b58\u90fd\u6e05\u96f6\n    model.zero_grad()\n\n    # \u53cd\u5411\u4f20\u9012: \u8ba1\u7b97\u635f\u5931\u76f8\u5bf9\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6\n    # \u5728\u5185\u90e8, \u6bcf\u4e2a Module \u7684\u53c2\u6570\u88ab\u5b58\u50a8\u5728\u72b6\u6001\u4e3a\n    # requires_grad=True \u7684 Tensors \u4e2d, \u6240\u4ee5\u8c03\u7528backward()\u540e\uff0c\n    # \u5c06\u4f1a\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6\u3002\n    loss.backward()\n\n    # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd. \u6bcf\u4e2a\u53c2\u6570\u662f\u4e00\u4e2aTensor, \u56e0\u6b64\n    # \u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u4e00\u6837\u901a\u8fc7 param.grad \u6765\u83b7\u53d6\u68af\u5ea6\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}