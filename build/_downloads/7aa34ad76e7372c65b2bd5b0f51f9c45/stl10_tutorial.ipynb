{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5728STL10\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CNN\n==============================\n\n**\u4f5c\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`__\n\n\u6211\u4eec\u5c06\u91c7\u53d6\u4ee5\u4e0b\u6b65\u9aa4:\n\n1. \u4f7f\u7528 ``torchvision`` \u52a0\u8f7d\u548c\u89c4\u8303\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\n2. \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\n3. \u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoardX\u67e5\u770b\n4. \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n5. \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8bad\u7ec3\u7f51\u7edc\n6. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\n\n# \u5ffd\u7565 warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STL10 \u6570\u636e\u96c6\u7684\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\n-------------------------------------------\n\nSTL-10\u6570\u636e\u96c6\u662f\u4e00\u79cd\u7528\u4e8e\u5f00\u53d1\u65e0\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\n\u81ea\u5b66\u5b66\u4e60\u7b97\u6cd5\u7684\u56fe\u50cf\u8bc6\u522b\u6570\u636e\u96c6.\u5b83\u53d7CIFAR-10\u6570\u636e\u96c6\u7684\u542f\u53d1\uff0c\n\u4f46\u4f5c\u4e86\u4e00\u4e9b\u4fee\u6539\u3002\u7279\u522b\u662f\uff0c\u6bcf\u4e00\u7c7b\u6709\u6807\u8bb0\u7684\u8bad\u7ec3\u6837\u4f8b\u6bd4CIFAR-10\u5c11\uff0c\n\u4f46\u662f\u5728\u76d1\u7763\u8bad\u7ec3\u4e4b\u524d\u63d0\u4f9b\u4e86\u4e00\u7ec4\u975e\u5e38\u5927\u7684\u672a\u6807\u8bb0\u6837\u4f8b\u6765\u5b66\u4e60\u56fe\u50cf\u6a21\u578b\u3002\n\u4e3b\u8981\u7684\u6311\u6218\u662f\u5229\u7528\u672a\u6807\u8bb0\u7684\u6570\u636e(\u6765\u81ea\u4e0e\u6807\u8bb0\u6570\u636e\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684\u5206\u5e03)\u6765\u6784\u5efa\u6709\u7528\u7684\u5148\u9a8c\u6570\u636e\u3002\n\u6211\u4eec\u8fd8\u671f\u671b\u8fd9\u4e2a\u6570\u636e\u96c6(96x96)\u7684\u66f4\u9ad8\u5206\u8fa8\u7387\u5c06\u4f7f\u5b83\u6210\u4e3a\u5f00\u53d1\u66f4\n\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002\n\u8be6\u7ec6\u4fe1\u606f\u770b\u5b98\u7f51\uff1ahttps://cs.stanford.edu/~acoates/stl10/\n\n\u6570\u636e\u96c6\u603b\u4f53\u60c5\u51b5\uff1a\n\n- 1\u300110 \u4e2a\u7c7b: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n- 2\u3001\u56fe\u50cf\u662f\u5f69\u8272\u768496x96\u50cf\u7d20\u7684\n- 3\u3001\u8bad\u7ec3\u56fe\u50cf500\u5f20(10\u5f20\u9884\u5b9a\u4e49)\uff0c\u6bcf\u4e2a\u7c7b800\u5f20\u6d4b\u8bd5\u56fe\u50cf\n- 4\u3001100000\u5f20\u65e0\u6807\u8bb0\u56fe\u50cf\u7528\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u3002\u8fd9\u4e9b\u4f8b\u5b50\u662f\u4ece\u76f8\u4f3c\u4f46\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u5206\u5e03\u4e2d\u63d0\u53d6\u51fa\u6765\u7684\u3002\n  \u4f8b\u5982\uff0c\u9664\u4e86\u6807\u8bb0\u96c6\u4e2d\u7684\u90a3\u4e9b\u52a8\u7269\u4e4b\u5916\uff0c\u5b83\u5305\u542b\u5176\u4ed6\u7c7b\u578b\u7684\u52a8\u7269(\u718a\u3001\u5154\u5b50\u7b49)\uff0c\u53ca\u8f66\u8f86(\u706b\u8f66\u3001\u5df4\u58eb\u7b49)\u3002\n- 5\u3001\u56fe\u50cf\u662f\u4eceImageNet\u4e0a\u6807\u8bb0\u7684\u793a\u4f8b\u4e2d\u83b7\u53d6\u7684\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u53d8\u6362\uff0c\u52a0\u8f7d\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n\n\ntransform_train = transforms.Compose([transforms.Pad(4), transforms.RandomCrop(96), \n                                      transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n\ntransform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.STL10(root='./data/stl10', split='train', download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.STL10(root='./data/stl10', split='test', download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nclasses = ('airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck')\n\n# \u8fd9\u6837\u7b97\u51fa\u6765\u7684\u6837\u672c\u6570\u91cf\u4e0d\u662f\u5f88\u4e25\u683c\nprint(\"\u8bad\u7ec3\u96c6\u5927\u5c0f\uff1a\",len(trainloader)*batch_size)\nprint(\"\u6d4b\u8bd5\u96c6\u5927\u5c0f\uff1a\",len(testloader)*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u7247\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u7528\u4e8e\u663e\u793a\u4e00\u5f20\u56fe\u50cf\u7684\u51fd\u6570\ndef imshow(img, title=None):\n    img = img / 2 + 0.5     # \u53bb\u5f52\u4e00\u5316\n    npimg = img.numpy()\n    plt.figure(figsize=[6.5, 2.5])\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n\n# \u83b7\u53d6\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\uff0c\u4e00\u6b21\u8fed\u4ee3\u53d6\u51fabatch_size\u5f20\u56fe\u7247\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# \u663e\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\nimshow(torchvision.utils.make_grid(images), \"One Batch Train Images\")\n# \u8f93\u51fa \u5bf9\u5e94\u6279\u6b21\u56fe\u50cf\u7684\u6807\u7b7e\nprint(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49CNN\u7f51\u7edc\u6a21\u578b\n---------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom collections import OrderedDict\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel_urls = {\n    'svhn': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/svhn-f564f3d8.pth',\n}\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1), # 96\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 48\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 48\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 24\n            \n            nn.Conv2d(64, 64, kernel_size=3, padding=1), # 24\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 12\n            \n            nn.Conv2d(64, 64, kernel_size=3, padding=1), # 12\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 6\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(64 * 6 * 6, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n    \n    \nmodel_urls = {\n    'stl10': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/stl10-866321e9.pth',\n}\n\nclass STL10(nn.Module):\n    def __init__(self, features, n_channel, num_classes):\n        super(STL10, self).__init__()\n        assert isinstance(features, nn.Sequential), type(features)\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(n_channel, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for i, v in enumerate(cfg):\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            padding = v[1] if isinstance(v, tuple) else 1\n            out_channels = v[0] if isinstance(v, tuple) else v\n            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU()]\n            else:\n                layers += [conv2d, nn.ReLU()]\n            in_channels = out_channels\n    return nn.Sequential(*layers)\n\ndef stl10(n_channel, pretrained=None):\n    cfg = [\n        n_channel, 'M',\n        2*n_channel, 'M',\n        4*n_channel, 'M',\n        4*n_channel, 'M',\n        (8*n_channel, 0), (8*n_channel, 0), 'M'\n    ]\n    layers = make_layers(cfg, batch_norm=True)\n    model = STL10(layers, n_channel=8*n_channel, num_classes=10)\n    if pretrained is not None:\n        m = model_zoo.load_url(model_urls['stl10'])\n        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n        model.load_state_dict(state_dict)\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoardX\u67e5\u770b\n-----------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorboardX import SummaryWriter\n\n# \u865a\u62df\u8f93\u5165\uff0c\u4e0e STL10 \u7684\u4e00\u4e2abatch\u6570\u636e\u7684shape\u76f8\u540c\ndummy_input = torch.autograd.Variable(torch.rand(batch_size, 3, 96, 96))\n\nmodel = stl10(n_channel=32, pretrained=None)\n# model = SimpleNet()\nprint(model)\nwith SummaryWriter(comment='_stl10_net1') as w:\n    w.add_graph(model, (dummy_input, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u628a\u6a21\u578b\u8fc1\u79fb\u5230GPU\u4e0a\n----------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 117\ncuda = torch.cuda.is_available()\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nnet = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n----------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n\nloss = nn.CrossEntropyLoss()\n# loss = F.cross_entropy\n\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n# optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8ba1\u7b97\u521d\u59cb\u7f51\u7edc\u7684\u51c6\u786e\u7387\n--------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u8bad\u7ec3\n----------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nnum_epochs = 30\nnum_batches = len(trainloader)\nlog_interval = 20\n\nbest_acc, old_file = 0, None\nt_begin = time.time()\n\ndecreasing_lr=[10, 20]\nwriter = SummaryWriter(comment='_stl10_logs')\n\nnet.train()\n\nfor epoch in range(num_epochs):\n    \n    if epoch in decreasing_lr:\n            optimizer.param_groups[0]['lr'] *= 0.5\n            \n    running_loss = 0.0        \n    for batch_idx, batch_data in enumerate(trainloader):\n        n_iter = epoch * num_batches + batch_idx\n        images, labels = batch_data[0].to(device), batch_data[1].to(device)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward\n        out = net(images)\n        # \u8ba1\u7b97\u635f\u5931\n        loss_value = loss(out, labels)\n        # backward\n        loss_value.backward()\n        # optimise\n        optimizer.step()\n        # LOGGING\n        writer.add_scalar('loss', loss_value.item(), n_iter)\n        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], n_iter)\n\n        running_loss += loss_value.item()\n        if batch_idx % log_interval == 0 and batch_idx > 0:\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / log_interval))\n            running_loss = 0.0\n\n    elapse_time = time.time() - t_begin\n    speed_epoch = elapse_time / (epoch + 1)\n    speed_batch = speed_epoch / num_batches\n    eta = speed_epoch * num_epochs - elapse_time\n    print(\"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(elapse_time, speed_epoch, speed_batch, eta))\n        \nprint(\"Total Elapse: {:.2f}, Best Result: {:.3f}%\".format(time.time()-t_begin, best_acc))\nwriter.close()\nprint('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u6d4b\u8bd5\n----------------\n\n\u5728\u6574\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7edf\u8ba1\u6bcf\u4e2a\u7c7b\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.eval()\nnum_classes = len(classes)\nclass_correct = list(0. for i in range(num_classes))\nclass_total = list(0. for i in range(num_classes))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(labels.size()[0]):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(num_classes):\n    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u6d4b\u8bd5\u6837\u672c\uff0c\u771f\u5b9e\u6807\u7b7e\u548c\u9884\u6d4b\u6807\u7b7e\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images), \"One Batch Test Images\")\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n\noutputs = net(images.to(device))\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted:   ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}