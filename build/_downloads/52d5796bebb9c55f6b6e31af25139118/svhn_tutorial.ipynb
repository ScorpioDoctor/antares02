{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5728SVHN\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CNN\n==============================\n\n**\u4f5c\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`__\n\n\u6211\u4eec\u5c06\u91c7\u53d6\u4ee5\u4e0b\u6b65\u9aa4:\n\n1. \u4f7f\u7528 ``torchvision`` \u52a0\u8f7d\u548c\u89c4\u8303\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\n2. \u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\n3. \u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoardX\u67e5\u770b\n4. \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n5. \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8bad\u7ec3\u7f51\u7edc\n6. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u6d4b\u8bd5\u7f51\u7edc\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\n\n# \u5ffd\u7565 warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVHN \u6570\u636e\u96c6\u7684\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\n-------------------------------------------\n\nSVHN\u6570\u636e\u96c6\uff1a**The Street View House Numbers (SVHN) Dataset**\n\nCharacter level ground truth in an MNIST-like format. All digits have been resized to a \nfixed resolution of 32-by-32 pixels. The original character bounding boxes are extended \nin the appropriate dimension to become square windows, so that resizing them to 32-by-32 \npixels does not introduce aspect ratio distortions. Nevertheless this preprocessing \nintroduces some distracting digits to the sides of the digit of interest. \nLoading the .mat files creates 2 variables: X which is a 4-D matrix containing the images, \nand y which is a vector of class labels. To access the images, X(:,:,:,i) gives \nthe i-th 32-by-32 RGB image, with class label y(i).\n\n\u8be6\u7ec6\u4fe1\u606f\u770b\u5b98\u7f51\uff1ahttp://ufldl.stanford.edu/housenumbers/\n\n**Note**: The SVHN dataset assigns the label 10 to the digit 0. \nHowever, in this Dataset, we assign the label 0 to the digit 0 to be compatible \nwith PyTorch loss functions which expect the class labels to be in the range [0, C-1]\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u53d8\u6362\uff0c\u52a0\u8f7d\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n\ndef target_transform(target):\n        return target\n\ntransform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntransform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.SVHN(root='./data/svhn', split='train', download=True, \n                                     transform=transform_train, target_transform=target_transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.SVHN(root='./data/svhn', split='test', download=True, \n                                    transform=transform_test, target_transform=target_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nclasses = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n\n# \u8fd9\u6837\u7b97\u51fa\u6765\u7684\u6837\u672c\u6570\u91cf\u4e0d\u662f\u5f88\u4e25\u683c\nprint(\"\u8bad\u7ec3\u96c6\u5927\u5c0f\uff1a\",len(trainloader)*batch_size)\nprint(\"\u6d4b\u8bd5\u96c6\u5927\u5c0f\uff1a\",len(testloader)*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u7247\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u7528\u4e8e\u663e\u793a\u4e00\u5f20\u56fe\u50cf\u7684\u51fd\u6570\ndef imshow(img, title=None):\n    img = img / 2 + 0.5     # \u53bb\u5f52\u4e00\u5316\n    npimg = img.numpy()\n    plt.figure(figsize=[6.5, 2.5])\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n\n\n\n# \u83b7\u53d6\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\uff0c\u4e00\u6b21\u8fed\u4ee3\u53d6\u51fabatch_size\u5f20\u56fe\u7247\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# \u663e\u793a\u4e00\u4e2a\u6279\u6b21\u7684\u56fe\u50cf\nimshow(torchvision.utils.make_grid(images),\"One Batch Train Images\")\n# \u8f93\u51fa \u5bf9\u5e94\u6279\u6b21\u56fe\u50cf\u7684\u6807\u7b7e\nprint(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49CNN\u7f51\u7edc\u6a21\u578b\n---------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom collections import OrderedDict\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nmodel_urls = {\n    'svhn': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/svhn-f564f3d8.pth',\n}\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1), # 32\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2), # 16\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 16\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2) # 8\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(64 * 8 * 8, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nclass SVHN(nn.Module):\n    def __init__(self, features, n_channel, num_classes):\n        super(SVHN, self).__init__()\n        assert isinstance(features, nn.Sequential), type(features)\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(n_channel, num_classes)\n        )\n        # print(self.features)\n        # print(self.classifier)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for i, v in enumerate(cfg):\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            padding = v[1] if isinstance(v, tuple) else 1\n            out_channels = v[0] if isinstance(v, tuple) else v\n            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU(), nn.Dropout(0.3)]\n            else:\n                layers += [conv2d, nn.ReLU(), nn.Dropout(0.3)]\n            in_channels = out_channels\n    return nn.Sequential(*layers)\n\ndef svhn(n_channel, pretrained=None):\n    cfg = [n_channel, n_channel, 'M', 2*n_channel, 2*n_channel, 'M', 4*n_channel, 4*n_channel, 'M', (8*n_channel, 0), 'M']\n    layers = make_layers(cfg, batch_norm=True)\n    model = SVHN(layers, n_channel=8*n_channel, num_classes=10)\n    if pretrained is not None:\n        m = model_zoo.load_url(model_urls['svhn'])\n        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n        model.load_state_dict(state_dict)\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c06\u6a21\u578b\u5199\u5165\u6587\u4ef6\u5e76\u7528TensorBoardX\u67e5\u770b\n-----------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorboardX import SummaryWriter\n\n# \u865a\u62df\u8f93\u5165\uff0c\u4e0e SVHN \u7684\u4e00\u4e2abatch\u6570\u636e\u7684shape\u76f8\u540c\ndummy_input = torch.autograd.Variable(torch.rand(batch_size, 3, 32, 32))\n\nmodel = svhn(n_channel=32, pretrained=None)\n# model = SimpleNet()\nprint(model)\nwith SummaryWriter(comment='_svhn_net1') as w:\n    w.add_graph(model, (dummy_input, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u628a\u6a21\u578b\u8fc1\u79fb\u5230GPU\u4e0a\n----------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 117\ncuda = torch.cuda.is_available()\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nnet = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n----------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n\nloss = nn.CrossEntropyLoss()\n# loss = F.cross_entropy\n\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\noptimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8ba1\u7b97\u521d\u59cb\u7f51\u7edc\u7684\u51c6\u786e\u7387\n--------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u8bad\u7ec3\n----------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nnum_epochs = 30\nnum_batches = len(trainloader)\nlog_interval = 500\n\nbest_acc, old_file = 0, None\nt_begin = time.time()\n\nwriter = SummaryWriter(comment='_svhn_logs')\n    \nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, batch_data in enumerate(trainloader):\n        n_iter = epoch * num_batches + batch_idx\n        images, labels = batch_data[0].to(device), batch_data[1].to(device)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward\n        out = net(images)\n        # \u8ba1\u7b97\u635f\u5931\n        loss_value = loss(out, labels)\n        # backward\n        loss_value.backward()\n        # optimise\n        optimizer.step()\n        # LOGGING\n        writer.add_scalar('loss', loss_value.item(), n_iter)\n\n        running_loss += loss_value.item()\n        if batch_idx % log_interval == 0 and batch_idx > 0:\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / log_interval))\n            running_loss = 0.0\n\n    elapse_time = time.time() - t_begin\n    speed_epoch = elapse_time / (epoch + 1)\n    speed_batch = speed_epoch / num_batches\n    eta = speed_epoch * num_epochs - elapse_time\n    print(\"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(elapse_time, speed_epoch, speed_batch, eta))\n        \nprint(\"Total Elapse: {:.2f}, Best Result: {:.3f}%\".format(time.time()-t_begin, best_acc))\nwriter.close()\nprint('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5f00\u59cb\u6d4b\u8bd5\n----------------\n\n\u5728\u6574\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7edf\u8ba1\u6bcf\u4e2a\u7c7b\u4e0a\u7684\u51c6\u786e\u7387\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.eval()\nnum_classes = len(classes)\nclass_correct = list(0. for i in range(num_classes))\nclass_total = list(0. for i in range(num_classes))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(labels.size()[0]):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(num_classes):\n    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5c55\u793a\u6d4b\u8bd5\u6837\u672c\uff0c\u771f\u5b9e\u6807\u7b7e\u548c\u9884\u6d4b\u6807\u7b7e\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images), \"One Batch Test Images\")\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n\noutputs = net(images.to(device))\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted:   ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}