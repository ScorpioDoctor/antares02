{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc(STN)\u6559\u7a0b\n=====================================\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`__\n\n.. figure:: /_static/img/stn/FSeq.png\n\n\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u4e00\u79cd\u79f0\u4e3a\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u7684\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u6765\u589e\u5f3a\u60a8\u7684\u7f51\u7edc\u3002\n\u60a8\u53ef\u4ee5\u5728 `DeepMind paper <https://arxiv.org/abs/1506.02025>`__ \u4e2d\u66f4\u591a\u5730\u9605\u8bfb\u6709\u5173\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u7684\u5185\u5bb9\u3002\n\n\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc(Spatial transformer networks, STN)\u662f\u5bf9\u4efb\u4f55\u7a7a\u95f4\u53d8\u6362\u7684\u53ef\u5fae\u5173\u6ce8(differentiable attention)\u7684\u63a8\u5e7f\u3002\nSTN\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5982\u4f55\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u7a7a\u95f4\u53d8\u6362\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u51e0\u4f55\u4e0d\u53d8\u6027\u3002\n\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u88c1\u526a\u611f\u5174\u8da3\u7684\u533a\u57df\u3001\u7f29\u653e\u548c\u7ea0\u6b63\u56fe\u50cf\u7684\u65b9\u5411\u3002\n\u8fd9\u662f\u4e00\u79cd\u6709\u7528\u7684\u673a\u5236\uff0c\u56e0\u4e3aCNN\u5bf9\u56fe\u50cf\u65cb\u8f6c\u3001\u5c3a\u5ea6\u548c\u66f4\u4e00\u822c\u7684\u4eff\u5c04\u53d8\u6362\u4e0d\u5177\u6709\u4e0d\u53d8\u6027\u3002\n\n\u5173\u4e8eSTN\u6700\u597d\u7684\u4e8b\u60c5\u4e4b\u4e00\u662f\u80fd\u591f\u7b80\u5355\u5730\u5c06\u5b83\u63d2\u5165\u5230\u4efb\u4f55\u73b0\u6709\u7684CNN\u4e2d\uff0c\u800c\u5f88\u5c11\u505a\u4efb\u4f55\u4fee\u6539\u3002\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD\n# Author: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.ion()   # \u4ea4\u4e92\u5f0f\u6a21\u5f0f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d\u6570\u636e\n----------------\n\n\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u5b9e\u9a8c\u4e86\u7ecf\u5178\u7684MNIST\u6570\u636e\u96c6\u3002\u4f7f\u7528\u6807\u51c6\u5377\u79ef\u7f51\u7edc\u548cSTN\u7f51\u7edc\u8fdb\u884c\u589e\u5e7f\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# \u8bad\u7ec3\u6570\u636e\u96c6\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='./data/mnist', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# \u6d4b\u8bd5\u6570\u636e\u96c6\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='./data/mnist', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u63cf\u8ff0 STN \u7f51\u7edc\n--------------------------------------\n\n\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u53ef\u5f52\u7ed3\u4e3a\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206:\n\n-  \u5b9a\u4f4d\u7f51\u7edc(localization network)\u662f\u4e00\u4e2a\u89c4\u5219\u7684CNN\u7f51\u7edc\uff0c\u5b83\u5bf9\u53d8\u6362\u53c2\u6570\u8fdb\u884c\u56de\u5f52\u3002\n   \u8be5\u53d8\u6362\u4ece\u4e0d\u4ece\u6b64\u6570\u636e\u96c6\u4e2d\u663e\u5f0f\u5b66\u4e60\uff0c\u800c\u662f\u7531\u7f51\u7edc\u81ea\u52a8\u5b66\u4e60\u63d0\u9ad8\u5168\u5c40\u7cbe\u5ea6\u7684\u7a7a\u95f4\u8f6c\u6362\u3002\n-  \u7f51\u683c\u751f\u6210\u5668\u4ece\u8f93\u51fa\u56fe\u50cf\u4e2d\u751f\u6210\u4e0e\u6bcf\u4e2a\u50cf\u7d20\u5bf9\u5e94\u7684\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u5750\u6807\u7f51\u683c\u3002\n-  \u91c7\u6837\u5668\u4f7f\u7528\u53d8\u6362\u5668\u7684\u53c2\u6570\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u3002\n\n.. figure:: /_static/img/stn/stn-arch.png\n\n.. Note::\n   \u6211\u4eec\u9700\u8981\u5305\u542b affine_grid \u548c grid_sample modules \u7684 PyTorch.\n   (PyToch 1.0 \u53ca\u4ee5\u4e0a)\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # \u7a7a\u95f4\u53d8\u6362\u5b9a\u4f4d\u7f51\u7edc(localization-network)\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # \u7528\u4e8e\u4f30\u8ba1 3 * 2 \u4eff\u5c04\u77e9\u9635\u7684\u56de\u5f52\u7f51\u7edc\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # \u4f7f\u7528\u6052\u7b49\u53d8\u6362(identity transformation)\u521d\u59cb\u5316 weights/bias\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # STN\u7f51\u7edc\u7684\u524d\u5411\u51fd\u6570\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # \u5bf9\u8f93\u5165\u8fdb\u884c\u53d8\u6362\n        x = self.stn(x)\n\n        # \u6267\u884c\u901a\u5e38\u7684\u524d\u5411\u4f20\u9012\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nmodel = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bad\u7ec3\u6a21\u578b\n------------------\n\n\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u4f7f\u7528SGD\u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u7f51\u7edc\u4ee5\u76d1\u7763\u7684\u65b9\u5f0f\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1\u3002\u540c\u65f6\uff0c\n\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u81ea\u52a8\u5b66\u4e60STN\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# \u5728MNIST\u4e0a\u6d4b\u8bd5STN\u6027\u80fd\u7684\u4e00\u79cd\u7b80\u5355\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002 \n#\n\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u53ef\u89c6\u5316 STN \u7684\u7ed3\u679c\n---------------------------\n\n\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u68c0\u67e5\u6211\u4eec\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u7684\u7ed3\u679c\u3002\n\n\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5c0f\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u65f6\u53ef\u89c6\u5316\u53d8\u6362\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# \u5728\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u6211\u4eec\u8981\u53ef\u89c6\u5316STN\u5c42\u7684\u8f93\u51fa\u3002\n# \u53ef\u89c6\u5316\u4e00\u6279\u8f93\u5165\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u4f7f\u7528STN\u53d8\u6362\u4ea7\u751f\u7684\u6279\u6b21\u3002\n\n\ndef visualize_stn():\n    with torch.no_grad():\n        # Get a batch of training data\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}