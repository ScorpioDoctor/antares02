{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5fae\u8c03 Torchvision \u6a21\u578b\n=============================\n\n**\u7ffb\u8bd1\u8005**: `Antares\u535a\u58eb <http://www.studyai.com/antares>`__\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5982\u4f55 \u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u6a21\u578b \u548c \u628a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u3002\n\u6211\u4eec\u8981\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u662f `torchvision models <https://pytorch.org/docs/stable/torchvision/models.html>`__,\n\u6240\u6709\u8fd9\u4e9bmodels\u90fd\u5df2\u57281000\u7c7bImagenet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\n\u672c\u6559\u7a0b\u5c06\u7ed9\u51fa\u4e00\u4e2a\u6df1\u5165\u7684\u7814\u7a76\u5982\u4f55\u4f7f\u7528\u51e0\u79cd\u73b0\u4ee3CNN\u67b6\u6784\uff0c\u5e76\u5e2e\u4f60\u5efa\u7acb\u4e00\u4e2a\u5fae\u8c03\u4efb\u4f55PyTorch\u6a21\u578b\u7684\u76f4\u89c9\u3002\n\u56e0\u4e3a\u6bcf\u4e2a\u6a21\u578b\u67b6\u6784\u662f\u4e0d\u540c\u7684\uff0c\u6240\u4ee5\u6ca1\u6709\u5728\u6240\u6709\u573a\u666f\u4e2d\u90fd\u80fd\u5de5\u4f5c\u7684\u6837\u677f\u5316\u4ee3\u7801\u3002\n\u76f8\u53cd\uff0c\u7814\u7a76\u4eba\u5458\u5fc5\u987b\u67e5\u770b\u73b0\u6709\u7684\u4f53\u7cfb\u7ed3\u6784\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u81ea\u5b9a\u4e49\u8c03\u6574\u3002\n\n\u5728\u672c\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u5c06\u6267\u884c\u4e24\u79cd\u7c7b\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff1a\u5fae\u8c03(finetuning)\u548c\u7279\u5f81\u63d0\u53d6(feature extraction)\u3002\n\u5728 \u5fae\u8c03 \u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u4ece\u4e00\u4e2a\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u5f00\u59cb\uff0c\u5e76\u4e3a\u6211\u4eec\u7684\u65b0\u4efb\u52a1 **\u66f4\u65b0\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570** \uff0c\n\u672c\u8d28\u4e0a\u662f\u5bf9\u6574\u4e2a\u6a21\u578b\u8fdb\u884c\u518d\u8bad\u7ec3\u3002\u5728 \u7279\u5f81\u63d0\u53d6 \u4e2d\uff0c\u6211\u4eec\u4ece\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u5f00\u59cb\uff0c **\u53ea\u66f4\u65b0\u6700\u540e\u4e00\u5c42\u6743\u503c** \uff0c\u4ece\u800c\u5f97\u5230\u9884\u6d4b\u3002\n\u5b83\u88ab\u79f0\u4e3a\u7279\u5f81\u63d0\u53d6\uff0c\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684CNN\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u4e14\u53ea\u6539\u53d8\u8f93\u51fa\u5c42\u3002\n\u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u66f4\u591a\u6280\u672f\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 `\u8fd9\u91cc <http://cs231n.github.io/transfer-learning/>`__ \u548c \n`\u8fd9\u91cc <http://ruder.io/transfer-learning/>`__ \u3002\n\n\u4e00\u822c\u6765\u8bf4\uff0c\u8fd9\u4e24\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u9075\u5faa\u76f8\u540c\u7684\u51e0\u4e2a\u6b65\u9aa4:\n\n-  \u521d\u59cb\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u3002\n-  \u91cd\u65b0\u5851\u9020(Reshape)\u6700\u7ec8\u5c42\uff0c\u4f7f\u5176\u5177\u6709\u4e0e\u65b0\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u6570\u76f8\u540c\u7684\u8f93\u51fa\u6570\u3002\n-  \u4e3a\u4f18\u5316\u7b97\u6cd5\u6307\u5b9a\u5728\u8bad\u7ec3\u671f\u95f4\u8981\u66f4\u65b0\u54ea\u4e9b\u53c2\u6570\u3002\n-  \u8fd0\u884c\u8bad\u7ec3\u6b65\u9aa4\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function \nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8f93\u5165\n------\n\n\u4ee5\u4e0b\u662f\u8fd0\u884c\u65f6\u8981\u66f4\u6539\u7684\u6240\u6709\u53c2\u6570\u3002\u6211\u4eec\u5c06\u4f7f\u7528 *hymenoptera_data* \uff0c\u53ef\u4ee5\u5728 \n`\u8fd9\u91cc <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`__ \u4e0b\u8f7d\u3002\n\u6b64\u6570\u636e\u96c6\u5305\u542b\u4e24\u4e2a\u7c7b(\u871c\u8702\u548c\u8682\u8681)\uff0c\u5176\u7ed3\u6784\u4f7f\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 \n`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`__ \u6570\u636e\u96c6\uff0c\n\u800c\u4e0d\u662f\u7f16\u5199\u6211\u4eec\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002\u4e0b\u8f7d\u6570\u636e\u5e76\u5c06 ``data_dir`` \u8f93\u5165\u8bbe\u7f6e\u4e3adataset\u7684\u6839\u76ee\u5f55\u3002 \n``model_name`` \u8f93\u5165\u662f\u60a8\u5e0c\u671b\u4f7f\u7528\u7684\u6a21\u578b\u7684\u540d\u79f0\uff0c\u5fc5\u987b\u4ece\u6b64\u5217\u8868\u4e2d\u9009\u62e9:\n\n::\n\n   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n\n\u5176\u4ed6\u8f93\u5165\u5982\u4e0b\uff1a``num_classes`` \u662f\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u6570\uff0c ``batch_size`` \u662f\u7528\u4e8e\u8bad\u7ec3\u7684\u6279\u5927\u5c0f\uff0c\n\u53ef\u4ee5\u6839\u636e\u673a\u5668\u7684\u80fd\u529b\u8fdb\u884c\u8c03\u6574\uff0c ``num_epochs`` \u662f\u6211\u4eec\u60f3\u8981\u8fd0\u884c\u7684\u8bad\u7ec3\u671f\u7684\u6570\u76ee\uff0c\n\u800c ``feature_extract`` \u662f\u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u5b83\u5b9a\u4e49\u4e86\u6211\u4eec\u662f\u5728 \u5fae\u8c03 \u8fd8\u662f \u63d0\u53d6\u7279\u5f81\u3002\n\u5982\u679c ``feature_extract = False`` \uff0c\u5219\u5bf9\u6a21\u578b\u8fdb\u884c \u5fae\u8c03\uff0c\u5e76\u66f4\u65b0\u6240\u6709\u6a21\u578b\u53c2\u6570\u3002\n\u5982\u679c ``feature_extract = True`` \uff0c\u5219\u53ea\u66f4\u65b0\u6700\u540e\u4e00\u4e2a\u5c42\u53c2\u6570\uff0c\u5176\u4ed6\u53c2\u6570\u5219\u4fdd\u6301\u4e0d\u53d8\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u9876\u5c42\u6570\u636e\u76ee\u5f55\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5047\u8bbe\u76ee\u5f55\u7684\u683c\u5f0f\u7b26\u5408 ImageFolder \u7ed3\u6784 \ndata_dir = \"./data/hymenoptera_data\"\n\n# \u53ef\u4f9b\u9009\u62e9\u7684\u6a21\u578b [resnet, alexnet, vgg, squeezenet, densenet, inception]\nmodel_name = \"resnet\"\n\n# \u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u6570\nnum_classes = 2\n\n# \u7528\u4e8e\u8bad\u7ec3\u7684\u6279\u5904\u7406\u5927\u5c0f(\u6839\u636e\u60a8\u7684\u5185\u5b58\u5927\u5c0f\u66f4\u6539) \nbatch_size = 8\n\n# \u8981\u8bad\u7ec3\u7684\u56de\u5408(epochs)\u6570\nnum_epochs = 15\n\n# \u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u7684\u6807\u5fd7\u3002\u5f53\u5176\u53d6\u503c\u4e3aFalse\u65f6\u6211\u4eec\u5fae\u8c03\u6574\u4e2a\u6a21\u578b\uff0c\u5f53\u5176\u53d6\u503c\u4e3aTrue\u65f6\u53ea\u66f4\u65b0\u88ab\u91cd\u5851\u7684\u5c42\u53c2\u6570\nfeature_extract = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8f85\u52a9\u51fd\u6570\n----------------\n\n\u5728\u7f16\u5199\u8c03\u6574\u6a21\u578b\u7684\u4ee3\u7801\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u5b9a\u4e49\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\u3002\n\n\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u4ee3\u7801\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``train_model`` \u51fd\u6570\u5904\u7406\u7ed9\u5b9a\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u4f5c\u4e3a\u8f93\u5165\uff0c\u5b83\u9700\u8981\u4e00\u4e2aPyTorch\u6a21\u578b\u3001\n\u4e00\u4e2adataloader\u7684\u5b57\u5178\u3001\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u3001\u4e00\u4e2a\u4f18\u5316\u5668\u3001\u9700\u8981\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u6307\u5b9a\u6570\u91cf\u7684\u56de\u5408(epoches)\uff0c\n\u4ee5\u53ca\u4e00\u4e2a\u5e03\u5c14\u6807\u5fd7\u6307\u793a\u662f\u5426\u5c06 Inception model \u4f5c\u4e3a\u521d\u59cb\u6a21\u578b\u3002 *is_inception* \n\u6807\u5fd7\u7528\u4e8e\u5bb9\u7eb3 *Inception v3* \u6a21\u578b\uff0c\u56e0\u4e3a\u8be5\u4f53\u7cfb\u7ed3\u6784\u4f7f\u7528\u8f85\u52a9\u8f93\u51fa\uff0c\u800c\u603b\u4f53\u6a21\u578b\u635f\u5931\u65e2\u5305\u62ec\u8f85\u52a9\u8f93\u51fa\u7684\u635f\u5931\uff0c\n\u4e5f\u5305\u62ec\u6700\u7ec8\u8f93\u51fa\u7684\u635f\u5931\uff0c\u5982\u4e0b\u6240\u8ff0\u3002\u8be5\u51fd\u6570\u5bf9\u6307\u5b9a\u6570\u91cf\u7684\u56de\u5408\u6570(epoches)\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u6bcf\u4e2aepoch\u4e4b\u540e\n\u8fd0\u884c\u4e00\u4e2a\u5b8c\u6574\u7684\u9a8c\u8bc1\u6b65\u9aa4\u3002\u5b83\u8fd8\u8ddf\u8e2a\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b(\u5728\u9a8c\u8bc1\u7cbe\u5ea6\u65b9\u9762)\uff0c\u5e76\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u8fd4\u56de\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\u3002\n\u6bcf\u4e2aepoch\u4e4b\u540e\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u88ab\u6253\u5370\u51fa\u6765\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # \u6bcf\u4e2a\u56de\u5408(epoch)\u90fd\u6709\u4e00\u4e2a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u9636\u6bb5\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # \u5728\u6570\u636e\u4e0a\u8fed\u4ee3.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # \u628a \u53c2\u6570\u68af\u5ea6 \u7f6e\u96f6\n                optimizer.zero_grad()\n\n                # \u524d\u5411\u8fc7\u7a0b\n                # \u53ea\u6709\u5728 train \u6a21\u5f0f\u4e0b\u624d\u4f1a\u8ddf\u8e2a\u5386\u53f2\n                with torch.set_grad_enabled(phase == 'train'):\n                    # \u83b7\u5f97\u6a21\u578b\u8f93\u51fa\u5e76\u8ba1\u7b97\u635f\u5931\n                    # inception \u6a21\u578b\u6bd4\u8f83\u7279\u522b\uff0c\u56e0\u4e3a\u5b83\u5728\u8bad\u7ec3\u9636\u6bb5\u5b83\u8fd8\u6709\u4e00\u4e2a\u8f85\u52a9\u8f93\u51fa\u3002\n                    #   \u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u6700\u7ec8\u8f93\u51fa\u548c\u8f85\u52a9\u8f93\u51fa\u6c42\u548c\u8ba1\u7b97\u635f\u5931\n                    #   \u4f46\u662f\u5728\u6700\u7ec8\u6a21\u5f0f\u4e0b\uff0c\u6211\u4eec\u53ea\u8003\u8651\u6700\u7ec8\u8f93\u51fa \n                    if is_inception and phase == 'train':\n                        # \u6765\u81ea\u4e8e https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # \u53ea\u6709\u5728\u8bad\u7ec3\u9636\u6bb5\u624d\u6709 backward + optimize\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # \u7edf\u8ba1\u51c6\u786e\u7387\u4fe1\u606f\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # \u6df1\u5ea6\u62f7\u8d1d\u6a21\u578b\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8bbe\u7f6e\u6a21\u578b\u53c2\u6570\u7684 .requires_grad \u5c5e\u6027\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u5f53\u6211\u4eec\u63d0\u53d6\u7279\u5f81\u65f6\uff0c\u8fd9\u4e2a\u8f85\u52a9\u51fd\u6570\u5c06\u6a21\u578b\u4e2d\u53c2\u6570\u7684 ``.requires_grad`` \u5c5e\u6027\u8bbe\u7f6e\u4e3afalse\u3002\n\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u52a0\u8f7d\u4e00\u4e2a\u9884\u5148\u8bad\u7ec3\u8fc7\u7684\u6a21\u578b\u65f6\uff0c\u6240\u6709\u53c2\u6570\u90fd\u6709 ``.requires_grad=True`` \uff0c\n\u5982\u679c\u6211\u4eec\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u7684\u8bdd\uff0c\u8fd9\u662f\u5f88\u597d\u7684\u3002\u4f46\u662f\uff0c\u5982\u679c\u6211\u4eec\u8981\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\n\u5e76\u4e14\u53ea\u60f3\u4e3a\u65b0\u521d\u59cb\u5316\u7684\u5c42\u8ba1\u7b97\u68af\u5ea6\uff0c\u90a3\u4e48\u6211\u4eec\u5e0c\u671b\u6240\u6709\u5176\u4ed6\u53c2\u6570\u90fd\u4e0d\u9700\u8981\u68af\u5ea6\u3002\n\u7a0d\u540e\u8fd9\u5c31\u66f4\u6709\u610f\u4e49\u4e86\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521d\u59cb\u5316\u548c\u91cd\u5851\u7f51\u7edc\n-----------------------------------\n\n\u73b0\u5728\u662f\u6700\u6709\u8da3\u7684\u90e8\u5206\u3002\u8fd9\u91cc\u662f\u6211\u4eec\u628a\u6bcf\u4e2a\u7f51\u7edc\u8fdb\u884c\u91cd\u5851\u7684\u5730\u65b9\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e0d\u662f\u4e00\u4e2a\u81ea\u52a8\u7684\u8fc7\u7a0b\uff0c\n\u5bf9\u6bcf\u4e2a\u6a21\u578b\u6765\u8bf4\u5176\u91cd\u5851(reshaping)\u662f\u72ec\u7279\u7684\uff0c\u6a21\u578b\u76f8\u5173\u7684\u3002\n\u56de\u60f3\u4e00\u4e0b\uff0cCNN\u6a21\u578b\u7684\u6700\u540e\u4e00\u5c42(\u901a\u5e38\u662fFC\u5c42)\u7684\u8282\u70b9\u6570\u4e0e\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u51fa\u7c7b\u6570\u76f8\u540c\u3002\u7531\u4e8e\u6240\u6709\u6a21\u578b\u90fd\u5728Imagenet\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\n\u5b83\u4eec\u90fd\u6709\u5927\u5c0f\u4e3a1000\u7684\u8f93\u51fa\u5c42\uff0c\u6bcf\u4e2a\u7c7b\u90fd\u6709\u4e00\u4e2a\u8282\u70b9\u3002\u8fd9\u91cc\u7684\u76ee\u6807\u662f\u91cd\u5851(reshape)\u6700\u540e\u4e00\u4e2a\u5c42\uff0c\u4f7f\u5176\u5177\u6709\u4e0e\u4ee5\u524d\u76f8\u540c\u7684\u8f93\u5165\u6570\uff0c\n\u5e76\u5177\u6709\u4e0edataset\u4e2d\u7684\u7c7b\u6570\u76f8\u540c\u7684\u8f93\u51fa\u6570\u3002\u5728\u4e0b\u9762\u7684\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u5982\u4f55\u5355\u72ec\u66f4\u6539\u6bcf\u4e2a\u6a21\u578b\u7684\u4f53\u7cfb\u7ed3\u6784\u3002\n\u4f46\u662f\u9996\u5148\uff0c\u5bf9\u4e8e\u5fae\u8c03(finetuning)\u548c\u7279\u5f81\u63d0\u53d6(feature-extraction)\u4e4b\u95f4\u7684\u533a\u522b\u6709\u4e00\u4e2a\u91cd\u8981\u7684\u7ec6\u8282.\n\n\u5728\u7279\u5f81\u63d0\u53d6\u65f6\uff0c\u6211\u4eec\u53ea\u60f3\u66f4\u65b0\u6700\u540e\u4e00\u5c42\u7684\u53c2\u6570\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u53ea\u60f3\u66f4\u65b0\u6211\u4eec\u6b63\u5728\u91cd\u5851\u7684\u5c42\u7684\u53c2\u6570\u3002\n\u56e0\u6b64\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u6211\u4eec\u4e0d\u6539\u53d8\u7684\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u56e0\u6b64\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u5c06 ``.requires_grad``` \u5c5e\u6027\u8bbe\u7f6e\u4e3afalse\u3002\n\u8fd9\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6b64\u5c5e\u6027\u8bbe\u7f6e\u4e3aTrue\u3002\u7136\u540e\uff0c\u5f53\u6211\u4eec\u521d\u59cb\u5316\u65b0\u5c42\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u65b0\u53c2\u6570\u6709 ``.requires_grad=True`` \uff0c\n\u56e0\u6b64\u53ea\u6709\u65b0\u5c42\u7684\u53c2\u6570\u624d\u4f1a\u88ab\u66f4\u65b0\u3002\u5f53\u6211\u4eec\u5fae\u8c03\u6a21\u578b\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6240\u6709 ``.required_grad``` \u7684\u8bbe\u7f6e\u4fdd\u7559\u4e3aTrue\u7684\u9ed8\u8ba4\u503c\u3002\n\n\u6700\u540e, \u8bf7\u6ce8\u610f inception_v3 \u9700\u8981\u7684\u8f93\u5165\u5c3a\u5bf8\u4e3a (299,299), \u800c\u5176\u4ed6\u6a21\u578b\u7684\u671f\u671b\u8f93\u5165\u5c3a\u5bf8\u4e3a (224,224)\u3002\n\nResnet\n~~~~~~\n\nResnet \u51fa\u81ea\u8fd9\u7bc7\u6587\u7ae0 `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__. \n\u6709\u5404\u79cd\u4e0d\u540c\u5927\u5c0f\u7684\u53d8\u4f53\u7248\u672c, \u5305\u62ec Resnet18, Resnet34, Resnet50, Resnet101, \u548c Resnet152,\n\u6240\u6709\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u4ece torchvision models \u4e2d\u83b7\u5f97\u3002 \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 Resnet18, \u56e0\u4e3a\u6211\u4eec\u7684\u6570\u636e\u96c6\u662f\u53ea\u6709\u4e24\u4e2a\u7c7b\u7684\u5c0f\u6570\u636e\u96c6\u3002\n\u5f53\u6211\u4eec\u8f93\u51fa\u6a21\u578b\u7684\u65f6\u5019\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6700\u540e\u4e00\u5c42\u662f\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u5982\u4e0b\u6240\u793a:\n\n::\n\n   (fc): Linear(in_features=512, out_features=1000, bias=True) \n\n\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u5c06 ``model.fc`` \u91cd\u65b0\u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5177\u6709512\u4e2a\u8f93\u5165\u7279\u5f81\u548c2\u4e2a\u8f93\u51fa\u7279\u5f81\u7684\u7ebf\u6027\u5c42\uff0c\u5982\u4e0b\u6240\u793a:\n\n::\n\n   model.fc = nn.Linear(512, num_classes)\n\nAlexnet\n~~~~~~~\n\nAlexnet \u51fa\u81ea\u8fd9\u7bc7\u6587\u7ae0 `ImageNet Classification with Deep Convolutional Neural Networks \n<https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf>`__\n\u800c\u4e14\u662f\u7b2c\u4e00\u4e2a\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u975e\u5e38\u6210\u529f\u7684CNN\u3002 \u5f53\u6211\u4eec\u8f93\u51fa\u6a21\u578b\u67b6\u6784\u7684\u65f6\u5019\uff0c\u6211\u4eec\u770b\u5230\u6765\u81ea\u5206\u7c7b\u5668\u7b2c6\u5c42\u7684\u6a21\u578b\u8f93\u51fa\uff0c\u5982\u4e0b\uff1a\n\n::\n\n   (classifier): Sequential(\n       ...\n       (6): Linear(in_features=4096, out_features=1000, bias=True)\n    ) \n\n\u4e3a\u4e86\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u628a\u6a21\u578b\u8f93\u51fa\u5c42\u91cd\u65b0\u521d\u59cb\u5316\u4e3a:\n\n::\n\n   model.classifier[6] = nn.Linear(4096,num_classes)\n\nVGG\n~~~\n\nVGG \u51fa\u81ea\u6587\u7ae0 `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/pdf/1409.1556.pdf>`__.\nTorchvision \u63d0\u4f9b\u4e86 VGG \u76848\u4e2a\u4e0d\u540c\u7248\u672c\uff0c\u6709\u4e9b\u5177\u6709\u4e0d\u540c\u7684\u957f\u5ea6\uff0c\u6709\u4e9b\u5177\u6709 batch normalizations layers\u3002\n\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5e26\u6709batch normalization \u7684 VGG-11 \u3002 VGG\u7684\u8f93\u51fa\u5c42\u7c7b\u4f3c\u4e8e Alexnet, i.e.\n\n::\n\n   (classifier): Sequential(\n       ...\n       (6): Linear(in_features=4096, out_features=1000, bias=True)\n    )\n\n\u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u529e\u6cd5\u4fee\u6539\u6700\u540e\u4e00\u5c42\uff1a\n\n::\n\n   model.classifier[6] = nn.Linear(4096,num_classes)\n\nSqueezenet\n~~~~~~~~~~\n\nSqueeznet \u51fa\u81ea\u8fd9\u7bc7\u6587\u7ae0 `SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model\nsize <https://arxiv.org/abs/1602.07360>`__  \u3002  \u5b83\u4f7f\u7528\u7684\u8f93\u51fa\u7ed3\u6784\u4e0e\u6211\u4eec\u4e0a\u9762\u4ecb\u7ecd\u7684\u90a3\u4e9b\u6a21\u578b\u90fd\u4e0d\u4e00\u6837\u3002\nTorchvision \u91cc\u9762\u6709\u4e24\u4e2a\u7248\u672c\u7684 Squeezenet\uff0c\u6211\u4eec\u4f7f\u7528 1.0 \u7248\u3002 \u5b83\u7684\u8f93\u51fa\u6765\u81ea\u4e00\u4e2a 1x1 \u5377\u79ef\u5c42(\u5206\u7c7b\u5668\u7684\u7b2c\u4e00\u4e2a\u5c42):\n\n::\n\n   (classifier): Sequential(\n       (0): Dropout(p=0.5)\n       (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n       (2): ReLU(inplace)\n       (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n    ) \n\n\u4e3a\u4e86\u4fee\u6539\u8fd9\u4e2a\u7f51\u7edc, \u6211\u4eec\u91cd\u65b0\u521d\u59cb\u5316 Conv2d layer \u6765\u83b7\u5f97\u6df1\u5ea6\u4e3a2\u7684\u8f93\u51fa\u7279\u5f81\u56fe\uff1a\n\n::\n\n   model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n\nDensenet\n~~~~~~~~\n\nDensenet \u51fa\u81ea\u8fd9\u7bc7\u6587\u7ae0 `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`__ \u3002 \nTorchvision \u6709\u56db\u79cd Densenet \u7684\u7248\u672c\uff0c\u4f46\u6211\u4eec\u53ea\u4f7f\u7528 Densenet-121 \u3002 \u8f93\u51fa\u5c42\u662f\u4e00\u4e2a\u5177\u67091024\u4e2a\u8f93\u5165\u7279\u5f81\u7684\u7ebf\u6027\u5c42:\n\n::\n\n   (classifier): Linear(in_features=1024, out_features=1000, bias=True) \n\n\u4e3a\u4e86\u91cd\u5851\u8fd9\u4e2a\u7f51\u7edc, \u6211\u4eec\u91cd\u65b0\u521d\u59cb\u5316\u5206\u7c7b\u5668\u7684\u7ebf\u6027\u5c42:\n\n::\n\n   model.classifier = nn.Linear(1024, num_classes)\n\nInception v3\n~~~~~~~~~~~~\n\nInception v3 \u51fa\u81ea\u8fd9\u7bc7\u6587\u7ae0 `Rethinking the Inception Architecture for Computer\nVision <https://arxiv.org/pdf/1512.00567v1.pdf>`__ \u3002 \n\u8fd9\u4e2a\u7f51\u7edc\u662f\u72ec\u7279\u7684\uff0c\u56e0\u4e3a\u5b83\u5728\u8bad\u7ec3\u65f6\u6709\u4e24\u4e2a\u8f93\u51fa\u5c42\u3002\u7b2c\u4e8c\u4e2a\u8f93\u51fa\u79f0\u4e3a\u8f85\u52a9\u8f93\u51fa\uff0c\u5305\u542b\u5728\u7f51\u7edc\u7684 AuxLogits \u90e8\u5206\u3002\n\u4e3b\u8981\u8f93\u51fa\u662f\u7f51\u7edc\u672b\u7aef\u7684\u7ebf\u6027\u5c42\u3002\u6ce8\u610f\uff0c\u5728\u6d4b\u8bd5\u65f6\uff0c\u6211\u4eec\u53ea\u8003\u8651\u4e3b\u8981\u8f93\u51fa\u3002\u8be5\u6a21\u578b\u7684\u8f85\u52a9\u8f93\u51fa\u548c\u4e3b\u8f93\u51fa\u6253\u5370\u5982\u4e0b\uff1a\n\n::\n\n   (AuxLogits): InceptionAux(\n       ...\n       (fc): Linear(in_features=768, out_features=1000, bias=True)\n    )\n    ...\n   (fc): Linear(in_features=2048, out_features=1000, bias=True)\n\n\u4e3a\u4e86\u5fae\u8c03\u8fd9\u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u5fc5\u987b\u91cd\u65b0\u5851\u9020\u4e24\u4e2alayers\u3002\u8fd9\u662f\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u5b8c\u6210\u7684:\n\n::\n\n   model.AuxLogits.fc = nn.Linear(768, num_classes)\n   model.fc = nn.Linear(2048, num_classes)\n\n\u8bf7\u6ce8\u610f\uff0c\u8bb8\u591a\u6a21\u578b\u5177\u6709\u76f8\u4f3c\u7684\u8f93\u51fa\u7ed3\u6784\uff0c\u4f46\u6bcf\u4e2a\u6a21\u578b\u7684\u5904\u7406\u65b9\u5f0f\u5fc5\u987b\u7565\u6709\u4e0d\u540c\u3002\n\u6b64\u5916\uff0c\u8bf7\u68c0\u67e5\u5df2\u91cd\u5851\u7684\u7f51\u7edc\u7684\u6253\u5370\u6a21\u578b\u4f53\u7cfb\u7ed3\u6784\uff0c\u5e76\u786e\u4fdd\u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u4e0e\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u6570\u76f8\u540c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n        \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3 \n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n    \n    return model_ft, input_size\n\n# \u8fd0\u884c\u4e0b\u9762\u7684\u4ee3\u7801\u521d\u59cb\u5316\u6a21\u578b\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# \u6253\u5370\u51fa\u6211\u4eec\u521a\u521a\u521d\u59cb\u5316\u7684\u6a21\u578b\nprint(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u52a0\u8f7d\u6570\u636e\n---------\n\n\u73b0\u5728\u6211\u4eec\u77e5\u9053\u4e86\u8f93\u5165\u5927\u5c0f\u5fc5\u987b\u662f\u4ec0\u4e48\uff0c\u6211\u4eec\u53ef\u4ee5\u521d\u59cb\u5316\u6570\u636e\u8f6c\u6362\u5668\u3001\u56fe\u50cf\u6570\u636e\u96c6\u7c7b\u548c\u6570\u636e\u52a0\u8f7d\u5668\u3002\n\u6ce8\u610f\uff0c\u6a21\u578b\u662f\u7528\u786c\u7f16\u7801\u7684\u5f52\u4e00\u5316\u503c\u9884\u5148\u8bad\u7ec3\u7684\uff0c\u8bf7\u770b `\u8fd9\u91cc <https://pytorch.org/docs/master/torchvision/models.html>`__ \u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nprint(\"Initializing Datasets and Dataloaders...\")\n\n# \u521b\u5efa\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u96c6\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n# \u521b\u5efa\u8bad\u7ec3\u548c\u9a8c\u8bc1\u52a0\u8f7d\u5668\ndataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n\n# \u68c0\u6d4b\u662f\u5426\u6709 GPU \u53ef\u4ee5\u7528\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u521b\u5efa\u4f18\u5316\u5668(Optimizer) \n-----------------------\n\n\u65e2\u7136\u6a21\u578b\u7ed3\u6784\u662f\u6b63\u786e\u7684\uff0c\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u7684\u6700\u540e\u4e00\u6b65\u5c31\u662f\u521b\u5efa\u4e00\u4e2a **\u53ea\u66f4\u65b0\u6240\u9700\u53c2\u6570** \u7684\u4f18\u5316\u5668\u3002\n\u56de\u60f3\u4e00\u4e0b\uff0c\u5728\u52a0\u8f7d\u7ecf\u8fc7\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u4e4b\u540e\uff0c\u4f46\u662f\u5728\u91cd\u65b0\u6784\u5efa\u4e4b\u524d\uff0c\u5982\u679c ``feature_extract=True`` \uff0c\n\u6211\u4eec\u4f1a\u624b\u52a8\u5c06\u6240\u6709\u53c2\u6570\u7684 ``.requires_grad`` \u5c5e\u6027\u8bbe\u7f6e\u4e3afalse\u3002\u7136\u540e\uff0c\n\u91cd\u65b0\u521d\u59cb\u5316\u5c42\u7684\u53c2\u6570\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u6709 ``.requires_grad=True`` \u3002\n\u56e0\u6b64\uff0c\u73b0\u5728\u6211\u4eec\u77e5\u9053\uff0c *\u6240\u6709\u5177\u6709 ``.requires_grad=True`` \u7684\u53c2\u6570\u90fd\u5e94\u8be5\u8fdb\u884c\u4f18\u5316* \u3002\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5217\u51fa\u4e86\u8fd9\u4e9b\u53c2\u6570\uff0c \u5e76\u5c06\u8fd9\u4e2a\u5217\u8868\u8f93\u5165\u5230SGD\u7b97\u6cd5\u6784\u9020\u51fd\u6570\u4e2d\u3002\n\n\u8981\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u8bf7\u67e5\u770b\u6253\u5370\u51fa\u7684\u53c2\u6570\u6765\u5b66\u4e60\u3002\u5f53finetuning\u65f6\uff0c\u8fd9\u4e2a\u5217\u8868\u5e94\u8be5\u5f88\u957f\uff0c\u5305\u62ec\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\u3002\n\u7136\u800c\uff0c\u5f53\u7279\u5f81\u63d0\u53d6\u65f6\uff0c\u8fd9\u4e2a\u5217\u8868\u5e94\u8be5\u662f\u77ed\u7684\uff0c\u5e76\u4e14\u53ea\u5305\u542b\u88ab\u91cd\u5851\u5c42\u7684\u6743\u91cd\u548c\u504f\u7f6e\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u628a\u6a21\u578b\u53d1\u9001\u5230 GPU\nmodel_ft = model_ft.to(device)\n\n# \u5728\u6b64\u8fd0\u884c\u4e2d\u6536\u96c6\u8981\u4f18\u5316/\u66f4\u65b0\u7684\u53c2\u6570\u3002\u5982\u679c\u6211\u4eec\u6b63\u5728\u8fdb\u884c finetuning\uff0c\u6211\u4eec\u5c06\u66f4\u65b0\u6240\u6709\u53c2\u6570\u3002\n# \u4f46\u662f\uff0c\u5982\u679c\u6211\u4eec\u662f\u5728\u505a\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u6211\u4eec\u5c06\u53ea\u66f4\u65b0\u6211\u4eec\u521a\u521a\u521d\u59cb\u5316\u7684\u53c2\u6570 \uff0c\n# \u4e5f\u5c31\u662f requires_grad == True \u7684\u90a3\u4e9b\u53c2\u6570\u3002\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# \u89c2\u5bdf\u5230 all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u8fd0\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6b65\u9aa4\n--------------------------------\n\n\u6700\u540e\uff0c\u6700\u540e\u4e00\u6b65\u662f\u5efa\u7acb\u6a21\u578b\u7684\u635f\u5931\uff0c\u7136\u540e\u8fd0\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51fd\u6570\u4e3a\u8bbe\u5b9a\u7684\u56de\u5408(epochs)\u6570\u3002\n\u6ce8\u610f\uff0c\u8fd9\u4e00\u6b65\u5728CPU\u4e0a\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\uff0c\u53d6\u51b3\u4e8e\u65f6\u95f4\u7684\u957f\u77ed\u3002\n\u53e6\u5916\uff0c\u5bf9\u4e8e\u6240\u6709\u7684\u6a21\u578b\uff0c\u9ed8\u8ba4\u7684\u5b66\u4e60\u7387\u5e76\u4e0d\u662f\u6700\u4f18\u7684\uff0c\n\u56e0\u6b64\u4e3a\u4e86\u8fbe\u5230\u6700\u5927\u7684\u7cbe\u5ea6\uff0c\u9700\u8981\u5bf9\u6bcf\u4e2a\u6a21\u578b\u5206\u522b\u8fdb\u884c\u8c03\u6574\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u8bbe\u7f6e\u635f\u5931\u51fd\u6570\ncriterion = nn.CrossEntropyLoss()\n\n# \u8bad\u7ec3 \u548c \u8bc4\u4f30\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4e0e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\n------------------------------------------\n\n\u53ea\u662f\u4e3a\u4e86\u597d\u73a9\uff0c\u8ba9\u6211\u4eec\u770b\u770b\u6a21\u578b\u5982\u4f55\u5b66\u4e60\uff0c\u5982\u679c\u6211\u4eec\u4e0d\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u3002finetuning vs.\u00a0feature extracting \n\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\uff0c\u4f46\u4e0e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u8fd9\u4e24\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u603b\u4f53\u7cbe\u5ea6\u65b9\u9762\n\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \u521d\u59cb\u5316\u6ca1\u6709\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u73a9\u73a9\nscratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\nscratch_model = scratch_model.to(device)\nscratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\nscratch_criterion = nn.CrossEntropyLoss()\n_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n\n# \u4e3a\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u7ed8\u5236\u9a8c\u8bc1\u51c6\u786e\u7387\u4e0e\u8bad\u7ec3\u65f6\u95f4(epoch)\u7684\u8bad\u7ec3\u66f2\u7ebf\uff0c\u5e76\u4e0e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u5f97\u5230\u7684\u66f2\u7ebf\u6bd4\u8f83\u3002\nohist = []\nshist = []\n\nohist = [h.cpu().numpy() for h in hist]\nshist = [h.cpu().numpy() for h in scratch_hist]\n\nplt.title(\"Validation Accuracy vs. Number of Training Epochs\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Validation Accuracy\")\nplt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\nplt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\nplt.ylim((0,1.))\nplt.xticks(np.arange(1, num_epochs+1, 1.0))\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6700\u540e\u7684\u60f3\u6cd5\u548c\u4e0b\u4e00\u6b65\u8be5\u53bb\u54ea\u91cc\n-----------------------------------\n\n\u8bd5\u7740\u8fd0\u884c\u4e00\u4e9b\u5176\u4ed6\u7684\u6a21\u578b\uff0c\u770b\u770b\u6709\u591a\u597d\u7684\u51c6\u786e\u6027\u3002\u53e6\u5916\uff0c\u8bf7\u6ce8\u610f\uff0c\u7279\u5f81\u63d0\u53d6\u6240\u82b1\u8d39\u7684\u65f6\u95f4\u8f83\u5c11\uff0c\n\u56e0\u4e3a\u5728\u5411\u540e\u4f20\u9012\u4e2d\uff0c\u6211\u4eec\u4e0d\u5fc5\u8ba1\u7b97\u5927\u90e8\u5206\u68af\u5ea6\u3002\u4ece\u8fd9\u91cc\u6709\u5f88\u591a\u5730\u65b9\u53ef\u53bb\u3002\u4f60\u53ef\u4ee5\uff1a\n\n-  \u4f7f\u7528\u66f4\u96be\u7684\u6570\u636e\u96c6\u8fd0\u884c\u6b64\u4ee3\u7801\uff0c\u5e76\u770b\u5230\u8fc1\u79fb\u5b66\u4e60\u7684\u66f4\u591a\u597d\u5904\u3002\n-  \u4f7f\u7528\u8fd9\u91cc\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528 transfer learning  \u6765\u66f4\u65b0\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u53ef\u80fd\u662f\u5728\u4e00\u4e2a\u65b0\u7684\u9886\u57df(\u4f8b\u5982NLP\u3001\u97f3\u9891\u7b49)\u3002\n-  \u4e00\u65e6\u60a8\u5bf9\u6a21\u578b\u611f\u5230\u6ee1\u610f\uff0c\u5c31\u53ef\u4ee5\u5c06\u5176\u5bfc\u51fa\u4e3aONNX\u6a21\u578b\uff0c\u6216\u8005\u4f7f\u7528\u6df7\u5408\u524d\u7aef\u8ddf\u8e2a\u5b83\uff0c\u4ee5\u83b7\u5f97\u66f4\u591a\u7684\u901f\u5ea6\u548c\u4f18\u5316\u673a\u4f1a\u3002\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}