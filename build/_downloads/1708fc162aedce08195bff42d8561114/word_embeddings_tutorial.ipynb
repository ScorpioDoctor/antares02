{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u5355\u8bcd\u5d4c\u5165: \u7f16\u7801\u8bcd\u6c47\u8bed\u4e49\n===========================================\n\n\u5355\u8bcd\u5d4c\u5165(Word embeddings)\u662f\u5b9e\u6570\u7684\u5bc6\u96c6\u5411\u91cf\uff0c\u5728\u4f60\u7684\u8bcd\u6c47\u8868\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u90fd\u4f1a\u6709\u4e00\u4e2a\u6574\u6570\u5bf9\u5e94\u3002\n\u5728NLP\u4e2d\uff0c\u7279\u5f81\u51e0\u4e4e\u603b\u662f\u5355\u8bcd(word)\uff01\u4f46\u662f\uff0c\u4f60\u5e94\u8be5\u5982\u4f55\u5728\u8ba1\u7b97\u673a\u4e2d\u8868\u793a\u4e00\u4e2a\u5355\u8bcd\u5462\uff1f\n\u60a8\u53ef\u4ee5\u5b58\u50a8\u5b83\u7684ascii\u5b57\u7b26\u8868\u793a\uff0c\u4f46\u8fd9\u53ea\u544a\u8bc9\u60a8\u5355\u8bcd\u662f\u4ec0\u4e48\uff0c\u5b83\u5e76\u4e0d\u80fd\u8bf4\u660e\u5b83\u7684\u542b\u4e49\u3002\n\u66f4\u91cd\u8981\u7684\u662f\uff0c\u4f60\u53ef\u4ee5\u5728\u4ec0\u4e48\u610f\u4e49\u4e0a\u7ec4\u5408\u8fd9\u4e9b\u8868\u793a\uff1f\u6211\u4eec\u7ecf\u5e38\u5e0c\u671b\u4ece\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f97\u5230\u5bc6\u96c6\u7684\u8f93\u51fa\uff0c\n\u5176\u4e2d\u8f93\u5165\u662f $|V|$ \u7ef4\u7684\uff0c\u5176\u4e2d $V$ \u662f\u6211\u4eec\u7684\u8bcd\u6c47\u8868\uff0c\u4f46\u662f\u901a\u5e38\u8f93\u51fa\u53ea\u6709\u51e0\u7ef4\n(\u4f8b\u5982\uff0c\u5982\u679c\u6211\u4eec\u53ea\u662f\u9884\u6d4b\u51e0\u4e2a\u6807\u7b7e\u7684\u8bdd)\u3002\u6211\u4eec\u5982\u4f55\u4ece\u4e00\u4e2a\u5927\u7684\u7a7a\u95f4\u5230\u4e00\u4e2a\u8f83\u5c0f\u7684\u7a7a\u95f4\uff1f\n\n\u5982\u679c\u6211\u4eec\u4e0d\u4f7f\u7528 ascii representations, \u800c\u662f\u4f7f\u7528 one-hot encoding \u5462?\n\u5c31\u662f\u8bf4, \u6211\u4eec\u8868\u8fbe\u5355\u8bcd $w$ \uff0c \u901a\u8fc7\n\n\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n\n\u5176\u4e2d 1 \u662f\u4e00\u4e2a\u60df\u4e00\u5bf9\u5e94\u4e8e $w$ \u7684\u4f4d\u7f6e\u3002 \u4efb\u4f55\u5176\u4ed6\u7684word\u5c06\u4f1a\u5728\u5176\u4ed6\u7684\u67d0\u4e2a\u4f4d\u7f6e\u6709 1 \u6216\u8005 0 \u3002\n\n\u8fd9\u4e2a\u8868\u793a\u6cd5\u9664\u4e86\u975e\u5e38\u5de8\u5927\u8fd9\u4e2a\u660e\u663e\u7684\u7f3a\u70b9\u4e4b\u5916\u8fd8\u6709\u4e00\u4e2a\u5de8\u5927\u7684\u7f3a\u70b9\u3002\u5b83\u57fa\u672c\u4e0a\u628a\u6240\u6709\u7684\u8bcd\u5f53\u4f5c\u72ec\u7acb\u7684\u5b9e\u4f53\uff0c\u6ca1\u6709\u4efb\u4f55\u5173\u7cfb\u3002\n\u6211\u4eec\u771f\u6b63\u60f3\u8981\u7684\u662f\u8bcd\u8bed\u4e4b\u95f4\u76f8\u4f3c(*similarity*)\u7684\u6982\u5ff5\u3002\u4e3a\u4ec0\u4e48\uff1f\u8ba9\u6211\u4eec\u770b\u770b\u4e00\u4e2a\u4f8b\u5b50\u3002\n\n\u5047\u8bbe\u6211\u4eec\u6b63\u5728\u6784\u5efa\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b(language model)\u3002\u5047\u8bbe\u6211\u4eec\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u770b\u8fc7\u8fd9\u4e9b\u53e5\u5b50\uff1a\n\n* The mathematician ran to the store.\n* The physicist ran to the store.\n* The mathematician solved the open problem.\n\n\u73b0\u5728\uff0c\u5047\u8bbe\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u53e5\u5b50\uff0c\u8fd9\u5728\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u662f\u524d\u6240\u672a\u89c1\u7684\uff1a\n\n* The physicist solved the open problem.\n\n\u6211\u4eec\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u53e5\u8bdd\u4e0a\u53ef\u80fd\u505a\u5f97\u4e0d\u9519\uff0c\u4f46\u5982\u679c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4e24\u4e2a\u4e8b\u5b9e\uff0c\u96be\u9053\u4e0d\u662f\u66f4\u597d\u5417\uff1a\n\n* \u6211\u4eec\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u53e5\u5b50\u4e2d\u626e\u6f14\u7740\u540c\u6837\u7684\u89d2\u8272\u3002\u6216\u591a\u6216\u5c11\u7684\uff0c\u4ed6\u4eec\u6709\u4e00\u4e2a\u8bed\u4e49\u5173\u7cfb( semantic relation)\u3002\n* \u6211\u4eec\u770b\u5230\u6570\u5b66\u5bb6\u5728\u8fd9\u4e2a\u65b0\u7684\u6ca1\u89c1\u8fc7\u7684\u53e5\u5b50\u4e2d\u626e\u6f14\u7740\u548c\u6211\u4eec\u73b0\u5728\u770b\u5230\u7684\u7269\u7406\u5b66\u5bb6\u76f8\u540c\u7684\u89d2\u8272\u3002\n\n\u7136\u540e\u63a8\u65ad\u7269\u7406\u5b66\u5bb6\u5728\u8fd9\u4e2a\u65b0\u7684\u770b\u4e0d\u89c1\u7684\u53e5\u5b50\u4e2d\u662f\u4e2a\u5f88\u597d\u7684\u4eba\u9009\uff1f\u8fd9\u5c31\u662f\u6211\u4eec\u6240\u8bf4\u7684\u76f8\u4f3c\u7684\u6982\u5ff5\uff1a\n\u6211\u4eec\u6307\u7684\u662f\u8bed\u4e49\u76f8\u4f3c\u6027(*semantic similarity*)\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5177\u6709\u76f8\u4f3c\u7684\u8868\u793a\u6cd5\u3002\u8fd9\u662f\u4e00\u79cd\u4e0e\u8bed\u8a00\u6570\u636e\u7684\u7a00\u758f\u6027\u4f5c\u6597\u4e89\u7684\u6280\u672f\uff0c\n\u5b83\u5c06\u6211\u4eec\u6240\u770b\u5230\u7684\u548c\u6211\u4eec\u6240\u6ca1\u6709\u7684\u4e1c\u897f\u8054\u7cfb\u8d77\u6765\u3002\u8fd9\u4e2a\u4f8b\u5b50\u5f53\u7136\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u672c\u7684\u8bed\u8a00\u5b66\u5047\u8bbe\uff1a\n\u5728\u76f8\u4f3c\u7684\u8bed\u5883\u4e2d\u51fa\u73b0\u7684\u8bcd\u5728\u8bed\u4e49\u4e0a\u662f\u76f8\u4e92\u5173\u8054\u7684\u3002\u8fd9\u5c31\u662f\u6240\u8c13\u7684\u5206\u5e03\u5047\u8bf4\n( `distributional hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__)\u3002\n\n\n\u83b7\u5f97\u5bc6\u96c6\u8bcd\u5d4c\u5165\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u6211\u4eec\u5982\u4f55\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u600e\u6837\u624d\u80fd\u5728\u8bcd\u4e2d\u7f16\u7801\u8bed\u4e49\u76f8\u4f3c\u6027\u5462\uff1f\u4e5f\u8bb8\u6211\u4eec\u60f3\u51fa\u4e86\u4e00\u4e9b\u8bed\u4e49\u5c5e\u6027\u3002\n\u4f8b\u5982\uff0c\u6211\u4eec\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u90fd\u80fd\u8fd0\u884c\uff0c\u6240\u4ee5\u4e5f\u8bb8\u6211\u4eec\u7ed9\u8fd9\u4e9b\u8bcd\u4e00\u4e2a\u9ad8\u5206\uff0c\u56e0\u4e3a\u201cis able to run\u201d\u8bed\u4e49\u5c5e\u6027\u3002\n\u60f3\u60f3\u5176\u4ed6\u4e00\u4e9b\u5c5e\u6027\uff0c\u60f3\u8c61\u4e00\u4e0b\u4f60\u53ef\u80fd\u4f1a\u5728\u8fd9\u4e9b\u5c5e\u6027\u4e0a\u5f97\u5230\u4e00\u4e9b\u5171\u540c\u7684\u8bcd\u6c47\u3002\n\n\u5982\u679c\u6bcf\u4e2a\u5c5e\u6027\u90fd\u662f\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u7ed9\u6bcf\u4e2a\u5355\u8bcd\u4e00\u4e2a\u5411\u91cf\uff0c\u5982\u4e0b\u6240\u793a\uff1a\n\n\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n\n\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n\n\u7136\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8fd9\u6837\u505a\u6765\u8861\u91cf\u8fd9\u4e9b\u8bcd\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff1a\n\n\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n\n\u867d\u7136\u7528\u957f\u5ea6\u6765\u89c4\u8303\u662f\u6bd4\u8f83\u5e38\u89c1\u7684:\n\n\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n\n\u5176\u4e2d $\\phi$ \u662f\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u7684\u89d2\u5ea6\u3002\u8fd9\u6837\uff0c\u975e\u5e38\u76f8\u4f3c\u7684\u5355\u8bcd(\u5d4c\u5165\u6307\u5411\u76f8\u540c\u65b9\u5411\u7684\u5355\u8bcd)\u5c06\u5177\u6709\u76f8\u4f3c\u6027 1\u3002\u975e\u5e38\u4e0d\u540c\u7684\u8bcd\u5e94\u8be5\u6709\u76f8\u4f3c\u5ea6 -1 \u3002\n\n\u4f60\u53ef\u4ee5\u628a\u672c\u8282\u4e00\u5f00\u59cb\u8bb2\u5230\u7684\u7a00\u758f\u7684one-hot vectors\u770b\u4f5c\u662f\u6211\u4eec\u5b9a\u4e49\u7684\u8fd9\u4e9b\u65b0\u5411\u91cf\u7684\u4e00\u4e2a\u7279\u4f8b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5355\u8bcd\u57fa\u672c\u4e0a\u90fd\u6709\u76f8\u4f3c\u5ea60\uff0c\n\u6211\u4eec\u7ed9\u6bcf\u4e2a\u5355\u8bcd\u4e00\u4e9b\u72ec\u7279\u7684\u8bed\u4e49\u5c5e\u6027\u3002\u8fd9\u4e9b\u65b0\u7684\u5411\u91cf\u662f\u5bc6\u96c6\u7684(*dense*)\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4eec\u7684\u6761\u76ee\u901a\u5e38\u662f\u975e\u96f6\u7684.\n\n\u4f46\u662f\uff0c\u8fd9\u4e9b\u65b0\u7684\u5411\u91cf\u662f\u4e00\u4e2a\u5f88\u5927\u7684\u75db\u82e6\uff1a\u4f60\u53ef\u4ee5\u60f3\u51fa\u6210\u5343\u4e0a\u4e07\u79cd\u4e0d\u540c\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u53ef\u80fd\u4e0e\u786e\u5b9a\u76f8\u4f3c\u6027\u6709\u5173\uff0c\n\u4f60\u5230\u5e95\u4f1a\u5982\u4f55\u8bbe\u7f6e\u4e0d\u540c\u5c5e\u6027\u7684\u503c\u5462\uff1f\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff0c\u7531\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7279\u5f81\u7684\u8868\u793a\uff0c\u800c\u4e0d\u662f\u8981\u6c42\u7a0b\u5e8f\u5458\u81ea\u5df1\u8bbe\u8ba1\u5b83\u4eec\u3002\n\u90a3\u4e48\uff0c\u4e3a\u4ec0\u4e48\u4e0d\u8ba9\u5355\u8bcd\u5d4c\u5165(word embedding)\u6210\u4e3a\u6211\u4eec\u6a21\u578b\u4e2d\u7684\u53c2\u6570\uff0c\u7136\u540e\u5728\u8bad\u7ec3\u671f\u95f4\u8fdb\u884c\u66f4\u65b0\u5462\uff1f\u8fd9\u6b63\u662f\u6211\u4eec\u8981\u505a\u7684\u3002\n\u6211\u4eec\u5c06\u6709\u4e00\u4e9b\u6f5c\u5728\u7684\u8bed\u4e49\u5c5e\u6027(*latent semantic attributes*)\uff0c\u539f\u5219\u4e0a\u7f51\u7edc\u53ef\u4ee5\u5b66\u4e60\u3002\u6ce8\u610f\uff0c\u5d4c\u5165(embeddings)\u8fd9\u4e2a\u8bcd\u53ef\u80fd\u662f\u4e0d\u53ef\u89e3\u91ca\u7684\u3002\n\u4e5f\u5c31\u662f\u8bf4\uff0c\u5c3d\u7ba1\u4e0a\u9762\u6211\u4eec\u624b\u5de5\u5236\u4f5c\u7684\u5411\u91cf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u7684\u76f8\u4f3c\u4e4b\u5904\u5728\u4e8e\u4ed6\u4eec\u90fd\u559c\u6b22\u5496\u5561\uff0c\n\u5982\u679c\u6211\u4eec\u5141\u8bb8\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u5d4c\u5165\uff0c\u5e76\u4e14\u770b\u5230\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u7b2c\u4e8c\u7ef4\u5ea6\u4e2d\u90fd\u6709\u5f88\u5927\u7684\u503c\uff0c\n\u6211\u4eec\u8fd8\u4e0d\u6e05\u695a\u8fd9\u610f\u5473\u7740\u4ec0\u4e48\u3002\u5b83\u4eec\u5728\u67d0\u4e9b\u6f5c\u5728\u7684\u8bed\u4e49\u7ef4\u5ea6\u4e0a\u662f\u76f8\u4f3c\u7684\uff0c\u4f46\u8fd9\u5bf9\u6211\u4eec\u53ef\u80fd\u6ca1\u6709\u89e3\u91ca\u6027\u3002\n\n\u603b\u7ed3\u4e00\u4e0b, **\u8bcd\u5d4c\u5165(word embeddings)\u662f\u4e00\u4e2a\u5355\u8bcd\u7684\u8bed\u4e49\u7684\u4e00\u79cd\u8868\u793a\uff0c\n\u9ad8\u6548\u7684\u7f16\u7801\u4e86\u53ef\u80fd\u4e0e\u624b\u5934\u7684\u4efb\u52a1\u76f8\u5173\u7684\u8bed\u4e49\u4fe1\u606f**\u3002 \u4f60\u4e5f\u53ef\u4ee5\u5d4c\u5165\u5176\u4ed6\u4e00\u4e9b\u4e1c\u897f\uff1a\n\u90e8\u5206\u8bed\u97f3\u6807\u8bb0, \u89e3\u6790\u6811, or \u4efb\u4f55\u4e1c\u897f! \u7279\u5f81\u5d4c\u5165\u7684\u601d\u60f3\u662f\u4ee5\u5177\u4f53\u9886\u57df\u4e3a\u4e2d\u5fc3\u7684\u3002\n\n\nPytorch \u4e2d\u7684\u8bcd\u5d4c\u5165\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u5728\u6211\u4eec\u5f00\u59cb\u4e00\u4e2a\u6709\u6548\u7684\u793a\u4f8b\u548c\u7ec3\u4e60\u4e4b\u524d\uff0c\u5148\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b\u5982\u4f55\u5728Pytorch\u548c\u4e00\u822c\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u4e2d\u4f7f\u7528\u5d4c\u5165(embeddings)\u3002\n\u5c31\u50cf\u6211\u4eec\u5728\u751f\u6210one-hot\u5411\u91cf\u65f6\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u5b9a\u4e49\u552f\u4e00\u7d22\u5f15\u4e00\u6837\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u5728\u4f7f\u7528\u5d4c\u5165(embeddings)\u65f6\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7d22\u5f15\u3002\n\u8fd9\u4e9b\u5c06\u662f\u67e5\u627e\u8868\u7684\u952e(keys into a lookup table)\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5d4c\u5165\u88ab\u5b58\u50a8\u4e3a\u4e00\u4e2a $|V| \\times D$ \u77e9\u9635\uff0c\n\u5176\u4e2d $D$ \u662f\u5d4c\u5165\u7684\u7ef4\u6570\uff0c\u4f7f\u5f97\u5177\u6709\u7d22\u5f15 $i$ \u7684\u5355\u8bcd\u7684\u5d4c\u5165\u88ab\u5b58\u50a8\u5728\u77e9\u9635\u7684\u7b2c $i$ \u884c\u4e2d\u3002\n\u5728\u6211\u7684\u6240\u6709\u4ee3\u7801\u4e2d\uff0c\u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u662f\u4e00\u4e2a\u540d\u4e3a word\\_to\\_ix \u7684\u5b57\u5178\u3002\n\n\u5141\u8bb8\u60a8\u4f7f\u7528\u5d4c\u5165\u7684module\u662f torch.nn.Embedding \uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u53c2\u6570\uff1a\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u548c\u5d4c\u5165\u7684\u7ef4\u5ea6\u3002\n\n\u8981\u60f3\u5728\u8fd9\u4e2a\u8868\u4e2d\u8fdb\u884c\u7d22\u5f15, \u4f60\u5fc5\u987b\u9002\u5e94 torch.LongTensor (\u56e0\u4e3a\u7d22\u5f15\u662f integers, \u800c\u4e0d\u662f floats).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Robert Guthrie\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\nembeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\nlookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\nhello_embed = embeds(lookup_tensor)\nprint(hello_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4e00\u4e2a\u793a\u4f8b: N-Gram \u8bed\u8a00\u6a21\u578b\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u56de\u60f3\u4e00\u4e0b\uff0c\u5728n-gram\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u7ed9\u5b9a\u4e00\u5355\u8bcd\u5e8f\u5217 $w$ \uff0c\u6211\u4eec\u60f3\u8981\u8ba1\u7b97\n\n\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n\n\u5176\u4e2d $w_i$ \u662f\u5e8f\u5217\u7684\u7b2c i \u4e2a\u5355\u8bcd\u3002\n\n\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d, \u6211\u4eec\u5c06\u5728\u4e00\u4e9b\u8bad\u7ec3\u6837\u672c\u4e0a\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4e14\u4f7f\u7528\u53cd\u5411\u4f20\u64ad(backpropagation)\u66f4\u65b0\u53c2\u6570.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CONTEXT_SIZE = 2\nEMBEDDING_DIM = 10\n# We will use Shakespeare Sonnet 2\ntest_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".split()\n# we should tokenize the input, but we will ignore that for now\n# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\ntrigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\n# print the first 3, just so you can see what they look like\nprint(trigrams[:3])\n\nvocab = set(test_sentence)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n\n\nclass NGramLanguageModeler(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs).view((1, -1))\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        log_probs = F.log_softmax(out, dim=1)\n        return log_probs\n\n\nlosses = []\nloss_function = nn.NLLLoss()\nmodel = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(10):\n    total_loss = 0\n    for context, target in trigrams:\n\n        # Step 1. \u51c6\u5907\u8981\u4f20\u9012\u7ed9\u6a21\u578b\u7684\u8f93\u5165 (i.e, \u628a\u8fd9\u4e9b\u5355\u8bcd\u8f6c\u5316\u4e3a\u6574\u6570\u7d22\u5f15\u5e76\u4e14\u5c01\u88c5\u5728\u5f20\u91cf\u4e2d)\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n\n        # Step 2. \u56de\u60f3\u4e00\u4e0btorch\u4f1a *\u7d2f\u79ef* \u68af\u5ea6\u3002 \u5728\u4f20\u5165\u4e00\u4e2a\u65b0\u7684\u6837\u4f8b\u4e4b\u524d\uff0c\n        # \u5e94\u8be5\u628a\u4e4b\u524d\u90a3\u4e2a\u6837\u4f8b\u4ea7\u751f\u7684\u68af\u5ea6\u6e05\u96f6\n        model.zero_grad()\n\n        # Step 3. \u8fd0\u884c\u524d\u5411\u4f20\u9012\u8fc7\u7a0b, \u5f97\u5230\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u5bf9\u6570\u6982\u7387\n        log_probs = model(context_idxs)\n\n        # Step 4. \u8ba1\u7b97\u635f\u5931\u51fd\u6570. (\u8fdb\u4e00\u6b65, Torch \u60f3\u8981 \u76ee\u6807\u5355\u8bcd\u5c01\u88c5\u5728\u4e00\u4e2atensor\u4e2d)\n        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n\n        # Step 5. \u6267\u884c\u53cd\u5411\u4f20\u9012\u5e76\u66f4\u65b0\u68af\u5ea6\n        loss.backward()\n        optimizer.step()\n\n        # \u901a\u8fc7\u8c03\u7528 tensor.item() \u5f97\u5230\u5355\u5143\u7d20\u5f20\u91cf\u4e2d\u7684Python\u6570\u5b57\n        total_loss += loss.item()\n    losses.append(total_loss)\nprint(losses)  # \u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u6bcf\u4e00\u6b21\u8fed\u4ee3\u635f\u5931\u90fd\u4f1a\u4e0b\u964d!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u7ec3\u4e60: \u8ba1\u7b97\u8bcd\u5d4c\u5165: \u8fde\u7eed\u8bcd\u888b\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\u8fde\u7eed\u8bcd\u888b\u6a21\u578b(CBOW)\u662fNLP\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u7528\u7684\u4e00\u79cd\u6a21\u5f0f.\u5b83\u662f\u4e00\u79cd\u6a21\u578b\uff0c\n\u5b83\u8bd5\u56fe\u9884\u6d4b\u7ed9\u5b9a\u76ee\u6807\u5355\u8bcd\u4e4b\u524d\u548c\u4e4b\u540e\u7684\u51e0\u4e2a\u5355\u8bcd\u7684\u4e0a\u4e0b\u6587\u4e2d\u7684\u76ee\u6807\u5355\u8bcd\u3002\n\u8fd9\u4e0e\u8bed\u8a00\u5efa\u6a21\u4e0d\u540c\uff0c\u56e0\u4e3aCBOW\u4e0d\u662f\u5e8f\u5217\u5316\u7684\uff0c\u4e5f\u4e0d\u4e00\u5b9a\u662f\u6982\u7387\u7684\u3002\u901a\u5e38\uff0cCBOW\u7528\u4e8e\u5feb\u901f\u8bad\u7ec3\u5355\u8bcd\u5d4c\u5165\uff0c\n\u800c\u8fd9\u4e9b\u5d4c\u5165\u7528\u4e8e\u521d\u59cb\u5316\u4e00\u4e9b\u66f4\u590d\u6742\u7684\u6a21\u578b\u7684\u5d4c\u5165\u3002\u901a\u5e38\uff0c\u8fd9\u88ab\u79f0\u4e3a\u9884\u8bad\u7ec3\u5d4c\u5165(*pretraining embeddings*)\u3002\n\u5b83\u51e0\u4e4e\u603b\u662f\u80fd\u5e2e\u52a9\u6027\u80fd\u63d0\u5347\u51e0\u4e2a\u767e\u5206\u70b9\u3002\n\nCBOW \u6a21\u578b\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u5982\u4e0b\u6240\u793a\u3002 \u7ed9\u5b9a\u4e00\u4e2a\u76ee\u6807\u5355\u8bcd $w_i$ \u548c \u4e00\u4e2a\u4e24\u8fb9\u957f\u5ea6\u4e3a $N$ \u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\n$w_{i-1}, \\dots, w_{i-N}$ \u548c $w_{i+1}, \\dots, w_{i+N}$, $C$ \u6307\u5411\u6240\u6709\u4e0a\u4e0b\u6587\u5355\u8bcd\u96c6\u4f53\uff0c\nCBOW \u8bd5\u56fe\u53bb\u6700\u5c0f\u5316\n\n\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n\n\u5176\u4e2d $q_w$ \u662f\u5355\u8bcd $w$ \u7684\u5d4c\u5165\u3002\n\n\u901a\u8fc7\u5b8c\u5584\u4e0b\u9762\u8fd9\u4e2a\u7c7b\u5728Pytorch\u4e2d\u5b9e\u73b0\u8fd9\u4e2a\u6a21\u578b. \n\n\u4e00\u4e9b\u5c0f\u5efa\u8bae:\n\n* \u8003\u8651\u4e00\u4e0b\u4f60\u9700\u8981\u5b9a\u4e49\u4ec0\u4e48\u6837\u7684\u53c2\u6570.\n* \u786e\u4fdd\u4f60\u77e5\u9053\u6bcf\u4e2a\u64cd\u4f5c\u7684\u671f\u671b\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5f20\u91cf\u7684shape\u3002\u5982\u679c\u4f60\u9700\u8981reshape\u7684\u8bdd\uff0c\u4f7f\u7528 .view() \u3002\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\nraw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\".split()\n\n# By deriving a set from `raw_text`, we deduplicate the array\nvocab = set(raw_text)\nvocab_size = len(vocab)\n\nword_to_ix = {word: i for i, word in enumerate(vocab)}\ndata = []\nfor i in range(2, len(raw_text) - 2):\n    context = [raw_text[i - 2], raw_text[i - 1],\n               raw_text[i + 1], raw_text[i + 2]]\n    target = raw_text[i]\n    data.append((context, target))\nprint(data[:5])\n\n\nclass CBOW(nn.Module):\n\n    def __init__(self):\n        pass\n\n    def forward(self, inputs):\n        pass\n\n# \u521b\u5efa\u4f60\u7684\u6a21\u578b\u5e76\u8bad\u7ec3\u3002\u8fd9\u91cc\u7684\u4e00\u4e9b\u51fd\u6570\u53ef\u4ee5\u5e2e\u4f60\u51c6\u5907\u597d\u4f60\u7684module\u6240\u9700\u8981\u7684\u6570\u636e \n\n\ndef make_context_vector(context, word_to_ix):\n    idxs = [word_to_ix[w] for w in context]\n    return torch.tensor(idxs, dtype=torch.long)\n\n\nmake_context_vector(data[0][0], word_to_ix)  # example"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}